{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f483c55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "abe84e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"pdf_parsing/output/article.csv\"\n",
    "df = pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8acada04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7061_Towards_Understanding_Fac.pdf</td>\n",
       "      <td>Published as a conference paper at ICLR 2024TO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4784_Making_Pre_trained_Langua.pdf</td>\n",
       "      <td>Published as a conference paper at ICLR 2024MA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3958_Evaluating_the_Zero_shot_.pdf</td>\n",
       "      <td>Under review as a conference paper at ICLR 202...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1154_ToolLLM_Facilitating_Larg.pdf</td>\n",
       "      <td>Published as a conference paper at ICLR 2024TO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4503_Instructive_Decoding_Inst.pdf</td>\n",
       "      <td>Published as a conference paper at ICLR 2024IN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8024_In_Context_Pretraining_La.pdf</td>\n",
       "      <td>7.3Figure 5: Ablation study of our method desi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>215_Large_Language_Models_Are_.pdf</td>\n",
       "      <td>Published as a conference paper at ICLR 2024LA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>9451_Beyond_Memorization_Viola.pdf</td>\n",
       "      <td>Published as a conference paper at ICLR 2024BE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1478_Retroformer_Retrospective.pdf</td>\n",
       "      <td>Published as a conference paper at ICLR 2024RE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3818_DP_OPT_Make_Large_Languag.pdf</td>\n",
       "      <td>SST-20.8680.895Trec0.3110.653Mpqa0.6900.807Dis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3099_FLASK_Fine_grained_Langua.pdf</td>\n",
       "      <td>CommonsenseUnderstandingIs the model accuratel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              filename  \\\n",
       "0   7061_Towards_Understanding_Fac.pdf   \n",
       "1   4784_Making_Pre_trained_Langua.pdf   \n",
       "2   3958_Evaluating_the_Zero_shot_.pdf   \n",
       "3   1154_ToolLLM_Facilitating_Larg.pdf   \n",
       "4   4503_Instructive_Decoding_Inst.pdf   \n",
       "5   8024_In_Context_Pretraining_La.pdf   \n",
       "6   215_Large_Language_Models_Are_.pdf   \n",
       "7   9451_Beyond_Memorization_Viola.pdf   \n",
       "8   1478_Retroformer_Retrospective.pdf   \n",
       "9   3818_DP_OPT_Make_Large_Languag.pdf   \n",
       "10  3099_FLASK_Fine_grained_Langua.pdf   \n",
       "\n",
       "                                              article  \n",
       "0   Published as a conference paper at ICLR 2024TO...  \n",
       "1   Published as a conference paper at ICLR 2024MA...  \n",
       "2   Under review as a conference paper at ICLR 202...  \n",
       "3   Published as a conference paper at ICLR 2024TO...  \n",
       "4   Published as a conference paper at ICLR 2024IN...  \n",
       "5   7.3Figure 5: Ablation study of our method desi...  \n",
       "6   Published as a conference paper at ICLR 2024LA...  \n",
       "7   Published as a conference paper at ICLR 2024BE...  \n",
       "8   Published as a conference paper at ICLR 2024RE...  \n",
       "9   SST-20.8680.895Trec0.3110.653Mpqa0.6900.807Dis...  \n",
       "10  CommonsenseUnderstandingIs the model accuratel...  "
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b1d1bb06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Published as a conference paper at ICLR 2024TOWARDS UNDERSTANDING FACTUAL KNOWLEDGE OFLARGE LANGUAGE MODELSXuming Hu1,2*, Junzhe Chen1*, Xiaochuan Li1*, Yufei Guo1, Lijie Wen1†,Philip S. Yu3, Zhijiang Guo4†1 Tsinghua University2 The Hong Kong University of Science and Technology (Guangzhou)3 University of Illinois at Chicago4 University of Cambridgexuminghu@hkust-gz.edu.cn, wenlj@tsinghua.edu.cn, zg283@cam.ac.ukABSTRACTLarge language models (LLMs) have recently driven striking performance im-provements across a range of natural language processing tasks. The factualknowledge acquired during pretraining and instruction tuning can be useful invarious downstream tasks, such as question answering, and language generation.Unlike conventional Knowledge Bases (KBs) that explicitly store factual knowl-edge, LLMs implicitly store facts in their parameters. Content generated by theLLMs can often exhibit inaccuracies or deviations from the truth, due to facts thatcan be incorrectly induced or become obsolete over time. To this end, we aim toexplore the extent and scope of factual knowledge within LLMs by designing thebenchmark Pinocchio. Pinocchio contains 20K diverse factual questions that spandifferent sources, timelines, domains, regions, and languages. Furthermore, weinvestigate whether LLMs can compose multiple facts, update factual knowledgetemporally, reason over multiple pieces of facts, identify subtle factual differences,and resist adversarial examples. Extensive experiments on different sizes and typesof LLMs show that existing LLMs still lack factual knowledge and suffer fromvarious spurious correlations. We believe this is a critical bottleneck for realizingtrustworthy artificial intelligence. The dataset Pinocchio and our codes are publiclyavailable at: https://github.com/THU-BPM/Pinocchio.1INTRODUCTIONLarge language models (LLMs) have revolutionized natural language processing (NLP) in recentyears since they have significantly improved performance on various downstream tasks (Brown et al.,2020; Chowdhery et al., 2022; Ouyang et al., 2022; Touvron et al., 2023a;b; OpenAI, 2022; 2023).Prior efforts have shown that language models can store factual knowledge and act as knowledgebases (Petroni et al., 2019; Jiang et al., 2020c). Factual knowledge in language models acquiredduring pretraining can benefit knowledge-intensive downstream tasks such as question answering andfact checking (Roberts et al., 2020; Yu et al., 2023a; Pan et al., 2023).Despite advancements in LLMs, they still struggle with generating content that exhibits inaccuraciesor deviations from the facts and making reasoning errors (Lin et al., 2022; Bubeck et al., 2023).These factual errors can be difficult to identify since LLMs implicitly memorize facts through theirparameters rather than explicitly store factual knowledge as traditional Knowledge Bases. Accessingand interpreting the computations and memories of these models can be challenging (Ribeiro et al.,2016; Belinkov & Glass, 2019), especially when APIs are the only means of interaction and manyinterpretation methods rely on weights and representations (Cao et al., 2021b). The presence of errorsin stored factual knowledge or the incorrect induction and obsolescence of certain facts over timemay be contributing factors to this limitation, which in turn affects the performance of LLMs (Elazaret al., 2021; Cao et al., 2021a). This limitation restricts the application of LLMs in some high-stakesareas, such as healthcare, finance, and law (Dong et al., 2022). Hence, exploring the degree to whichLLMs hold factual information and their ability to reason with such knowledge is vital.∗Equal Contribution.† Corresponding authors.1Published as a conference paper at ICLR 2024Figure 1: Pinocchio is a comprehensive dataset that tackles 7 distinct tasks related to factual knowl-edge and reasoning. It consists of 20,713 multiple-choice questions that have been sourced fromvarious reliable and diverse channels.To this end, we propose the Pinocchio, a testbed aimed at understanding factuality and reasoningfor LLMs. It contains 20K diverse factual questions that span different sources, timelines, domains,regions, and languages. Furthermore, we investigate whether LLMs are able to recognize thecombination of multiple facts, reason over structured and unstructured evidence, realize facts changeover time, identify subtle factual differences, and resist adversarial examples based on the dataset.We control for problem difficulty in each distinct reasoning task to enable fine-grained analysis.With the Pinocchio benchmark, we explore whether various LLMs (Scao et al., 2022b; Zhang et al.,2022; Ouyang et al., 2022; Chung et al., 2022; Touvron et al., 2023a; Chiang et al., 2023) couldstore factual knowledge and perform reasoning based on it. We envision Pinocchio as a suite ofbenchmarks, subsets of which could be separately utilized to assess certain model abilities of interestand analyze important strengths and limitations of LLMs. For instance, in temporal tasks, we find thatLLMs lack factual knowledge for up-to-date questions; in complex factual tasks that require multi-hopreasoning, LLMs still have limitations, even when various prompting strategies are employed. Wehope Pinocchio can serve as the initial step towards understanding the abilities of LLMs from multipledimensions and facilitate the development of LLMs.2DATASET CONSTRUCTION2.1TASKSAiming to systematically evaluate the factual knowledge and related reasoning abilities of LLMs,we raise seven research questions, then carefully select factual statements from different sourcessummarized in Table 1.• Task 1: Multifaceted Previous research (Petroni et al., 2019) has shown that small languagemodels like BERT have the ability to retain relational knowledge from training data and answer“fill-in-the-blank” cloze statements. This raises the question of whether LLMs can also storeand reason over multiple pieces of facts obtained during pretraining. It is not just importantfor LLMs to memorize individual facts accurately, but to also recognize and generate newcombinations of facts from different sources. To investigate this issue, we have selected claimsfrom the FEVER dataset (Thorne et al., 2018), which were written by human annotators basedon information from Wikipedia articles. These claims are either supported or refuted by multiplefacts from (the same or several) Wikipedia articles, or there is insufficient information availableto verify them. To assess the performance of language models in handling various combinationsof facts, we have sampled statements that require different numbers of evidence, ranging fromone to many, enabling fine-grained analysis.• Task 2: Structural In addition to unstructured text, factual knowledge is also commonly storedin a structured format, such as tables, lists, or databases (Bhagavatula et al., 2013). However,2Published as a conference paper at ICLR 2024Table 1: Pinocchio Dataset Sources, Descriptions, and Data Distribution.DomainDescriptionSourcesDistributionFact.Non-Fact.NEIALLMultifacetedContain multiple factsFEVER1,1111,1111,1103,332StructuralContain structured and unstructured factsFEVEROUS1,7411,9532503,944AdversarialContain facts edited by adversarial methodsSymmetric, FM2815921-1,736TemporalContain facts that change over timeVitaminC1,8981,0433553,296Real-WorldContain factual statements spread onlinePolitiFact9861,9876093,582Domain-SpecificContain facts from health and science domainsPubHealth, SciFact1,1567157372,608Multi-LingualContain facts in different languagesXFact, CHEF8208485472,215current LLMs are primarily trained on unstructured text using next word prediction loss (Brownet al., 2020; Touvron et al., 2023a). In order to process structured data, it is often convertedinto text strings using various methods, such as linearizing tables. This raises the question ofwhether LLMs are capable of effectively memorizing and reasoning over facts from structuredsources, similar to their performance with unstructured text. To investigate this question, wesample factual statements from the FEVEROUS dataset (Aly et al., 2021), which is constructedin a similar manner to FEVER but includes evidence in the form of tables, sentences, or both.• Task 3: Adversarial Language models are known to be vulnerable to adversarial examples thatare strategically modified to deceive even advanced models with hardly noticeable changes (Shenet al., 2023). Given this knowledge, it is important to examine whether LLMs can withstand ad-versarial examples in the context of factuality. To investigate this, we utilize two datasets, namelySymmetric (Schuster et al., 2019) and FM2 (Eisenschlos et al., 2021). These datasets consistof adversarial examples that have been crafted using various strategies, including temporalinference and diverting to unrelated facts.• Task 4: Temporal Facts are not static but rather possess a dynamic nature. With the vast amountof new information constantly emerging, facts often undergo changes, additions, or alterations.It raises the question of whether LLMs are able to adapt to these factual changes over time. Inparticular, we wonder if LLMs are capable of discerning factual knowledge from different timeperiods, since the pretraining corpus may not be processed and organized chronologically. Toexplore this, we utilize the VitaminC (Schuster et al., 2021) dataset, which consists of claimsbased on modifications made to factual content in Wikipedia articles. Claims can be eitherrefuted by outdated facts or supported by updated facts.• Task 5: Real-World In contrast to other tasks that assume Wikipedia has all the essential factualinformation, verifying viral claims on the internet often requires not only factual knowledgefrom various sources but also common sense and worldly knowledge. An important query wehave is whether LLMs can effectively integrate diverse types and sources of knowledge acquiredduring training. To address this, we select claims from the FactCheck (Misra, 2022) dataset,which consists of claims spread over the Internet and subsequently verified by journalists.• Task 6: Domain-Specific In addition to the tasks mentioned earlier, which primarily focus onfactual knowledge in general domains, we are also interested in exploring how LLMs possess thecapability to access domain-specific factual knowledge. The domain-specific setting presentsunique challenges. Take the science domain as an example, LLMs need to acquire backgroundknowledge, handle quantitative reasoning, and comprehend specialized statistical language. Toinvestigate this further, we sample claims from PubHealth (Kotonya & Toni, 2020) in the publichealth domain and SciFact (Wadden et al., 2022) in the science domain.• Task 7: Multi-Lingual Existing LLMs are mainly trained on English corpus because of theirabundance and quality (Chowdhery et al., 2022; Touvron et al., 2023a). However, the scarcity oftraining data in other languages raises the question of whether LLMs can transfer the factualknowledge acquired in English to other languages. To investigate this, we collected claims fromvarious languages including French, Chinese, and more, using the XFACT dataset (Gupta &Srikumar, 2021) and the CHEF dataset (Hu et al., 2022b) in a total of 27 different languages.2.2ANNOTATION AND QUALITY CONTROLMultiple-choice questions offer a practical approach to assess the complex capabilities of LLMs, ofwhich GPT-4 is a prime example (OpenAI, 2023). Key benchmarks such as MMLU (Hendryckset al., 2021b), HellaSwag (Zellers et al., 2019), ARC (Clark et al., 2018a), and TruthfulQA (Lin et al.,2022), all of which utilize multi-choice formats, serve distinct purposes in evaluating various aspectsof GPT-4’s proficiency. Specifically, the MMLU gauges an LLM’s knowledge breadth and depth.3Published as a conference paper at ICLR 2024Zero-shot\\xa0 \\xa0\\xa0You will be presented with a question.\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 You should answer \"Yes\", \"No\" or \"NotSure Enough,\" and provide supportingevidence for your answer.\\xa0 \\xa0 Q:\\xa0Has gas prices gone up 99 percentsince Obama became president, making itthe highest gas price increase since Carter?\\xa0 \\xa0 A:Few-shot with CoT\\xa0 \\xa0You will be presented with a question.\\xa0 \\xa0\\xa0You should answer \"Yes\", \"No\" or \"NotSure Enough,\" and provide supportingevidence for your answer.Here are some examples:\\xa0 \\xa0 Q:\\xa0\\xa0Is there a capital called Mogadishu?\\xa0 \\xa0 A:\\xa0Mogadishu is a city in East Africa,specifically in Somalia. Furthermore, thecapital of Somalia is exactly Mogadishu.Therefore, the answer is \"Yes\".\\xa0 \\xa0 Q :Has gas prices gone up 99 percentsince Obama became president, making itthe highest gas price increase since Carter?\\xa0 \\xa0 A:Few-shot\\xa0 \\xa0 You will be presented with a\\xa0question.\\xa0\\xa0 \\xa0 You should answer \"Yes\", \"No\" or\\xa0\"Not\\xa0Sure Enough\".Here are some examples:\\xa0 \\xa0 Q:Is it true that sixty two year oldWelsh journalist Jan Moir worked\\xa0for a\\xa0couple other papers before working\\xa0at\\xa0Daily Mail as an opinion columnist\\xa0and\\xa0has won several awards for her writing?\\xa0 \\xa0 A:\\xa0Yes.\\xa0 \\xa0 Q:\\xa0Has gas prices gone up 99 percent since\\xa0Obama became president, making it the\\xa0highest gas price\\xa0increase since Carter?\\xa0 \\xa0 A:Zero-shot with CoT\\xa0 \\xa0You will be presented with a question.\\xa0\\xa0 \\xa0You should answer \"Yes\", \"No\" or \"NotSure Enough,\" and provide supportingevidence for your answer.\\xa0 \\xa0 Q:\\xa0Has gas prices gone up 99 percentsince Obama became president, making itthe highest gas price increase since Carter?Let\\'s think step by step.\\xa0 \\xa0 A:Figure 2: Illustration of prompts using different settings.HellaSwag tests commonsense reasoning, and ARC focuses on challenging questions. TruthfulQAmeasures how LLMs mimic human falsehoods. Furthermore, the evaluation of language generationbrings its own set of challenges, as a universal metric for measurement is currently lacking (Sai et al.,2023), which multiple-choice questions help to mitigate by offering straightforward classificationaccuracy for assessment (Hendrycks et al., 2021b). Also, prior studies (Kadavath et al., 2022)underscore that LLMs demonstrate reliable calibration on multiple-choice scenarios. Therefore, wealso used the multi-choice questions as a simple but good proxy to evaluate the abilities of LLMs.For data annotation, we hired 10 undergraduate students, all with good English proficiency. We askedthe students to rewrite the original claims into questions without distorting factuality while providingfactuality labels for the questions. By transforming declarative statements into questions, using aQuestion-Answering approach can more effectively elicit factual knowledge from LLMs (Kadavathet al., 2022; Lin et al., 2022), and we also illustrate through experiments in Sec. 4.2. Note that claimsin the original datasets are usually labeled based on given evidence, e.g. evidence supports or refutesthe claim, but in Pinocchio, we only need to judge the factuality of the question. So we use unifiedlabels: Yes, No, Not Sure Enough. The three labels correspond respectively to Factual, Non-Factual,and Not Enough Information for factual questions. Considering that all fact-checking datasets use athree-label system (Guo et al., 2022), we did not modify the number of labels to maintain consistencyin labeling. When dealing with factuality questions in low-resource languages, for Chinese, the 5undergraduate students we hired are native Chinese speakers. For other low-resource languages,we first use Google Translate to translate them into English and generate factuality questions, thentranslate the English questions back to the corresponding languages. The label distribution is shownin Table 1. We paid the annotators accordingly based on the quantity and quality of the annotations.We ensure the quality of the annotated factuality questions in two ways. The two authors of thispaper served as meta-reviewers, sampling 10 questions from each of the three categories across theseven domains in Pinocchio. The meta-reviewers judged if the factuality labels were correct. Forthe 210 factuality questions, the average label accuracy was 92.4%. We divided the 10 students intotwo groups and had each group re-annotate a random 200 questions annotated by the other group,then calculated inter-annotator agreement (IAA). The final IAA was 85.6%. Based on meta-reviewerresults and IAA, the factuality labels in Pinocchio are of good quality.3METHODOLOGY3.1MODELSTo give a comprehensive view of the status of LLMs in a factual context, we evaluate 10 accessibleLLMs, undergone different training stages including pretraining, instruction tuning, and reinforcementlearning from human feedback (Ouyang et al., 2022), covering diverse organizations and varying insize. A detailed description can be found in Appendix A.2.3.2PROMPT STRATEGYAs illustrated in Figure 2, we employ 4 types of prompts to elicit desired responses from LLMs,namely: Zero-shot, Zero-shot with CoT (Kojima et al., 2022), Few-shot, and Few-shot with CoT (Weiet al., 2022). Specifically, we begin by providing the model with task instruction, denoted as Z: “You4Published as a conference paper at ICLR 2024Table 2: Results obtained using different forms of prompts on 10 accessible LLMs.MethodsZero-shot w/o CoTZero-shot w/ CoTFew-shot w/o CoTFew-shot w/ CoTOverall PerformanceAccuracyF1AccuracyF1AccuracyF1AccuracyF1AccuracyF1OPT-6.7B————36.927.937.928.518.814.3BLOOM-7B29.726.214.818.129.728.16.612.220.221.2LLaMA-7B31.829.622.324.936.828.635.331.431.628.6Alpaca-7B40.223.733.724.437.924.939.426.237.824.8Vicuna-7B33.233.634.232.935.534.848.540.637.934.9Vicuna-13B42.635.644.036.947.038.647.042.545.238.4ChatGLM-6B37.431.036.531.741.637.942.937.539.634.5Flan-T5-11B24.621.529.929.325.923.738.438.429.726.9Text-Davinci-00245.236.245.737.346.640.446.242.545.939.1Text-Davinci-00342.841.443.142.148.843.246.943.445.542.5GPT-3.5-Turbo46.944.346.844.447.244.747.145.747.044.8will be given a question. You should answer whether it is Yes, No, or Not Sure Enough and show yourevidence”. This instruction informs the LLMs about the expected input and output. Subsequently, forany given input Q, we anticipate obtaining an output label Y from the LLMs f: Y = f(Q, Z).Zero-Shot PromptIn the zero-shot setting, the LLMs are expected to provide answers based on theQuestion Q and the task instruction Z. We anticipate that the LLMs can directly generate the factualanswer “No” when presented with Q: “Has gas prices gone up 99 percent since Obama becamepresident, making it the highest gas price increase since Carter?” The zero-shot with CoT settingextends the question Q by adding a two-stage prompt (Kojima et al., 2022): “Let’s think step bystep”, designed to encourage the LLMs to contemplate the process of determining the factual label Y .Few-Shot PromptIn the few-shot setting, we employ three shots for model input (Q). Detailedexamples of the prompts in Figure 2 are presented in Appendix A.4. In the few-shot with CoTsetting, we provide potential reasoning instructions to the LLMs before presenting the factual label(Y ). As shown in Figure 2, for the Q: “Is there a capital called Mogadish?” Our reasoningapproach entails first explaining the noun phrase in the Q (the subject and object), and subsequentlyelaborating on modifying phrases such as predicates or adjectives. Regarding the subject “Mogadish”,we begin by furnishing a detailed definition: “Mogadishu is a city in East Africa, specifically inSomalia.” Following this, we proceed to reason about the relation between “Mogadish” and “capital”:“Furthermore, the capital of Somalia is indeed Mogadishu.” Consequently, we arrive at the ultimatefactual label: “Therefore, the answer is Yes.”4EXPERIMENTSIn an effort to take the initial step in understanding the capabilities of LLMs, we undertake acomprehensive analysis of various LLMs on Pinocchio, under different conditions and tasks.4.1MAIN RESULTSIn Table 2, we present the average results of 10 accessible LLMs operating under varying settings onPinocchio, run three times each. From Table 2, we draw the following conclusions:• Regarding overall performance, we observe that, on average, LLMs without instruction tuningunderperform those with instruction tuning by 16.0%. GPT family LLMs undergoing RLHFexhibit superior results, indicating that instruction tuning and RLHF optimize alignment withhuman knowledge, thereby improving factual question response accuracy.• Results obtained using the Few-shot setting significantly outperform those obtained when simplyasking factual questions to LLMs in the Zero-shot setting, especially for models without RLHF,exhibiting an average improvement of 7.3%. This highlights the capability of some sampleprompts to better extract the inherent factual knowledge of LLMs.• Using the CoT method, we observed a relative boost in performance in LLMs subjected toinstruction tuning and RLHF, improving by an average of 2.1%. Notably, the factual accuracy ofLLMs like OPT, BLOOM, and LLaMA was mostly stable or even decreased. A review of outputsfrom these untuned LLMs revealed that, post-CoT application, LLMs tend to produce related5Published as a conference paper at ICLR 2024Table 3: Results of different LLMs using Few-shot w/ CoT prompts across different tasks.TaskMultifacetedStructuralAdversarialTemporalReal-WorldDomain SpecificMulti-lingualAcc.F1Acc.F1Acc.F1Acc.F1Acc.F1Acc.F1Acc.F1OPT-6.7B34.524.145.530.951.851.730.018.053.727.528.228.316.217.7BLOOM-7B10.713.50.83.52.03.73.77.75.48.511.815.69.815.9LLaMA-7B38.333.944.132.143.246.141.630.026.426.323.625.027.827.7Alpaca-7B38.628.848.023.646.435.149.626.124.519.942.926.824.217.7Vicuna-7B44.236.049.736.359.059.250.137.649.041.844.338.646.743.1Vicuna-13B49.945.348.137.958.960.045.437.847.742.743.540.437.837.9ChatGLM-6B41.036.046.835.751.548.639.432.448.934.835.235.037.135.3Flan-T5-11B49.249.443.533.754.756.631.630.631.129.435.634.625.314.4Text-Davinci-00247.747.750.838.464.264.333.931.151.741.436.436.143.139.5Text-Davinci-00351.147.844.333.764.163.741.435.148.042.840.441.443.743.6GPT-3.5-Turbo53.653.144.837.867.467.437.433.950.443.138.740.341.341.1content considerations, and extensive considerations often overshadow factual discernment tasks,causing incorrect factual label outputs. In contrast, for instruction-tuned LLMs, the CoT methodfacilitates enhanced exploration of factual entity relations in questions, resulting in accuratefactual labels. See Appendix A.5 for detailed case analyses.• The OPT model, without being tuned to instructions, struggles significantly to output correctfactual labels under the settings of Zero-shot and Zero-shot CoT, often resulting in either arepetition of the original question or a refusal to output any content at all. This issue is somewhatalleviated under the settings of Few-shot and Few-shot CoT.• Additionally, we studied the hyperparameters of LLMs. Due to limited computing resources,we only explored Vicuna-7B and Vicuna-13B. We found that as model parameters increase,performance on factual questions improves correspondingly, with an average increase of 5.4%.This indicates that LLMs with more parameters can store more world knowledge and havestronger factual knowledge recognition capabilities.In Table 3, we present the factual performance of LLMs in various tasks under the Few-shot CoTsetting. This reveals the relative difficulty LLMs have in understanding and responding to factualquestions in different tasks, providing insights for future training of factual knowledge in LLMs.From Table 3, it is observed that LLMs exhibit relatively poorer performance on factual questionsrelated to the real-world, domain-specific knowledge, and multilingualism, being on average 6.4%lower compared to the other four tasks. This is attributed to the fact that the training data for LLMstypically come from general domains and are not up-to-date, which indirectly inspires the explorationof retrieval-augmented LLMs (Ram et al., 2023). We analyze the LLMs in different tasks in Sec. 4.2.4.2ANALYSISIn this section, we explore LLMs’ capabilities focusing on key areas like handling of multi-hop factualquestions, proficiency in diverse prompt strategies, and tackling challenges like numerical reasoningand entity ambiguity. We also examine their performance on time-sensitive factual questions, againstadversarial attacks, with fine-grained labels and prompts in multiple languages.1 Hop2 Hops3 HopsMultiHops 1 Hop  2 Hops 3 Hops 020040060080010001200Class Counts100020090200120025040202530354045505551.348.447.045.240.639.430.2MultifactedStructrualF1 of MultifactedF1 of Structrual(a) Multi-hop Reasoning AnalysisOnly SentencesOnly TablesCombine BothSentences and Tables800900100011001200130014001500160015951135121432343638404240.034.834.3Class CountsClass Performance(b) Structural Knowledge AnalysisEntityDisambiguationOtherMulti-hopReasoningCombiningTablesand Text Searchterms notin claimNumericalReasoning05001000150020009521806215377343820253035404550Macro F140.939.533.432.729.027.9Class CountsClass Performance(c) Challenges of Different QuestionsFigure 3: GPT-3.5-Turbo’s outcomes across three distinct tasks under Few-shot CoT setting.Multi-hop Factual Question AnalysisTo analyze the performance of LLMs when faced withfactual questions based on multiple pieces of facts that require complex logical reasoning, wecategorize multifaced and structural factual questions into distinct subsets, depending on the numberof “hops” necessary to validate each factual question. To maintain fairness, we randomly sampled1,490 data pieces from each of the two datasets for verification. Figure 3(a) illustrates the data6Published as a conference paper at ICLR 2024counts and Macro F1 scores of GPT-3.5-Turbo for each respective subset. The figure reveals a clearpattern: as the number of “hops” increases, the reasoning chain for deriving conclusions from existingfactual knowledge extends, necessitating heightened logical reasoning capabilities from the LLMs.Consequently, the performance of the LLMs exhibits diminishing trends.Structural Knowledge Analysis in LLMsTo investigate whether LLMs can effectively memorizefactual knowledge from structured data, we divided the structural task questions into three subsetsaccording to evidence distribution: evidence in unstructured data (Only text), structured data (Onlytables), or both (Combine text and tables). Figure 3(b) shows a notable decline (Avg. -5.5%) inGPT-3.5-Turbo’s performance when evidence involves structured data, indicating LLMs’ limitedability in extracting knowledge from structured tables. The LLMs also perform less effectively whenhandling questions requiring the combination of both evidence types, reflecting their incapacity tointegrate diverse structured evidence effectively.Analysis of Different Factual Questions Poses ChallengesTo assess the capabilities of LLMs inaddressing various challenges, we partitioned each factual question within the structural task into sixdistinct challenges: 1) Entity disambiguation, 2) Other, 3) Multi-hop reasoning, 4) Combining tablesand text, 5) Search terms not in claim, 6) Numerical reasoning, each centered around the most criticaldifficulty encountered during verification. Figure 3(c) illustrates GPT-3.5-Turbo’s performance anddata distribution across challenges. The extensive training and large-scale parameters enhance LLMs’performance in handling entity ambiguity. Longer reasoning chains and various forms of evidencechallenge LLMs’ factual abilities. When correct inference involves unmentioned entities, LLMsmay lack necessary hints from factual questions, posing significant challenges. LLMs also exhibitdeficiencies in precise numerical calculations due to the inherent hallucination phenomenon, resultingin subpar performance when numerical reasoning is needed for verification. Factual  Non-Factual  Not EnoughInformation FactualNon-FactualNot EnoughInformation10152025303540455055Macro F135.6033.9349.021.036.845.938.217.7Avg. F1 of Outdated DataAvg. F1 of Updated DataOutdated DataUpdated Data(a) Temporal Questions VerificationFactualNon-Factual Factual  Non-Factual  Factual  Non-Factual 50556065707580859081.886.183.775.263.563.681.886.183.775.283.9579.4563.55Avg. F1 of Synthetic dataAvg. F1 of Modified dataAvg. F1 of Man-made dataSynthetic dataModified dataMan-made data(b) Adversarial Attacks Resilience Factual MostlyFactual NEI MostlyFalse Non-Factual PantsFireFactualNon-FactualNEI010203040506024.812.72.816.043.016.538.166.225.119.3043.13Avg. F1 of Fine-grainedAvg. F1 of Coarse-grainedFine-grainedCoarse-grained(c) Label Granularity VariationsFigure 4: Results of GPT-3.5-Turbo in three different tasks under Few-shot CoT setting.Temporal AnalysisAs time progresses, the factuality of questions may undergo changes. This taskencompasses such data, and we leverage this task to explore the ability of LLMs to adapt to factualchanges. Figure 4(a) illustrates that GPT-3.5-Turbo exhibits a modest yet noticeable performancedifference when dealing with outdated data as compared to updated data. This discrepancy arises fromthe fact'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0][\"article\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal-systems",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
