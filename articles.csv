"filename","article"
"215_Large_Language_Models_Are_.pdf","Published as a conference paper at ICLR 2024 [NEWLINE] LARGE LANGUAGE MODELS ARE NOT ROBUST [NEWLINE] MULTIPLE CHOICE SELECTORS [NEWLINE] Chujie Zheng† [NEWLINE] Hao Zhou‡ [NEWLINE] Fandong Meng‡ [NEWLINE] Jie Zhou‡ [NEWLINE] Minlie Huang†∗ [NEWLINE] †The CoAI Group, DCST, BNRist, Tsinghua University, Beijing 100084, China [NEWLINE] ‡Pattern Recognition Center, WeChat AI, Tencent Inc., China [NEWLINE] chujiezhengchn@gmail.com [NEWLINE] aihuang@tsinghua.edu.cn [NEWLINE] ABSTRACT [NEWLINE] Multiple choice questions (MCQs) serve as a common yet important task format [NEWLINE] in the evaluation of large language models (LLMs). This work shows that modern [NEWLINE] LLMs are vulnerable to option position changes in MCQs due to their inherent [NEWLINE] “selection bias”, namely, they prefer to select speciﬁc option IDs as answers (like [NEWLINE] “Option A”). Through extensive empirical analyses with 20 LLMs on three bench- [NEWLINE] marks, we pinpoint that this behavioral bias primarily stems from LLMs’ token [NEWLINE] bias, where the model a priori assigns more probabilistic mass to speciﬁc option [NEWLINE] ID tokens (e.g., A/B/C/D) when predicting answers from the option IDs. To mit- [NEWLINE] igate selection bias, we propose a label-free, inference-time debiasing method, [NEWLINE] called PriDe, which separates the model’s prior bias for option IDs from the over- [NEWLINE] all prediction distribution. PriDe ﬁrst estimates the prior by permutating option [NEWLINE] contents on a small number of test samples, and then applies the estimated prior [NEWLINE] to debias the remaining samples. We demonstrate that it achieves interpretable [NEWLINE] and transferable debiasing with high computational efﬁciency. We hope this work [NEWLINE] can draw broader research attention to the bias and robustness of modern LLMs.1 [NEWLINE] 1 [NEWLINE] INTRODUCTION [NEWLINE] Question: [NEWLINE] In an SR latch [NEWLINE] built from NOR gates, which [NEWLINE] condition is not allowed [NEWLINE] Options: [NEWLINE] A. S=0, R=0 [NEWLINE] B. S=0, R=1 [NEWLINE] C. S=1, R=0 [NEWLINE] D. S=1, R=1 [NEWLINE] Answer: [NEWLINE] D [NEWLINE] Figure 1: A multiple choice ques- [NEWLINE] tion (MCQ) example from MMLU. [NEWLINE] Multiple choice question (MCQ) is a prevalent input format [NEWLINE] of large language models (LLMs). An MCQ typically encom- [NEWLINE] passes a question accompanied by multiple candidate options, [NEWLINE] from which the model is tasked to select the most suitable an- [NEWLINE] swer, as exempliﬁed in Figure 1. Current LLM-centric scenar- [NEWLINE] ios have widely utilized the task format of MCQ, for instance, [NEWLINE] within benchmarks targeted at assessing LLMs (Hendrycks [NEWLINE] et al., 2020; Zhong et al., 2023; Huang et al., 2023) and in [NEWLINE] LLM-based automatic evaluation frameworks (Chiang et al., 2023; Zheng et al., 2023b). In any [NEWLINE] scenario, we always expect LLMs to robustly select reliable answers in MCQs. [NEWLINE] Unfortunately, we observe that modern LLMs are vulnerable to option position changes in MCQs [NEWLINE] (Robinson & Wingate, 2022). We show in Table 1 that, in the 0-shot MMLU evaluation (Hendrycks [NEWLINE] et al., 2020), a simple “answer-moving attack” by always moving the golden answers to a speciﬁc [NEWLINE] position causes LLMs’ dramatic performance ﬂuctuations. For instance, moving the golden answers [NEWLINE] to position D degrades the accuracy of gpt-3.5-turbo by 6.3 (from 67.2 to 60.9). When moving [NEWLINE] to A, llama-30b is boosted by 15.2 and surpasses gpt-3.5-turbo (68.2 vs. 65.3), which starkly [NEWLINE] contrasts with their original performance (53.1 vs. 67.2). [NEWLINE] LLMs’ poor robustness to option position changes results from their biased behavior: they prefer [NEWLINE] to select speciﬁc option IDs as answers (like “Option A”), which we term as selection bias. As [NEWLINE] a simple veriﬁcation, we randomly sampled 1,000 MMLU test samples, where we controlled the [NEWLINE] number of correct answers being A/B/C/D as 250 each. Among these samples, llama-30B selects [NEWLINE] A/B/C/D 34.6% / 27.3% / 22.3% / 15.8% of the time, while gpt-3.5-turbo for 22.5% / 25.6% / [NEWLINE] ∗Corresponding author. [NEWLINE] 1Project repository: https://github.com/chujiezheng/LLM-MCQ-Bias. [NEWLINE] 1 [NEWLINE]  [NEWLINE] Published as a conference paper at ICLR 2024 [NEWLINE] Table 1: Simply moving the golden answers of MCQs to [NEWLINE] a speciﬁc option position can cause dramatic performance [NEWLINE] ﬂuctuations (0-shot MMLU), suggesting LLMs’ poor ro- [NEWLINE] bustness to option position changes in MCQs. [NEWLINE] Move Golden to [NEWLINE] Orig [NEWLINE] A [NEWLINE] B [NEWLINE] C [NEWLINE] D [NEWLINE] llama-30B [NEWLINE] 53.1 [NEWLINE] 68.2 [NEWLINE] (+15.2) [NEWLINE] 54.1 [NEWLINE] (+1.1) [NEWLINE] 50.1 [NEWLINE] (-2.9) [NEWLINE] 41.2 [NEWLINE] (-11.9) [NEWLINE] vicuna-v1.3-33B [NEWLINE] 57.0 [NEWLINE] 59.5 [NEWLINE] (+2.5) [NEWLINE] 58.6 [NEWLINE] (+1.5) [NEWLINE] 65.8 [NEWLINE] (+8.8) [NEWLINE] 44.8 [NEWLINE] (-12.3) [NEWLINE] falcon-40B [NEWLINE] 51.8 [NEWLINE] 46.3 [NEWLINE] (-5.5) [NEWLINE] 45.2 [NEWLINE] (-6.7) [NEWLINE] 64.8 [NEWLINE] (+13.0) [NEWLINE] 47.9 [NEWLINE] (-3.9) [NEWLINE] falcon-inst-40B [NEWLINE] 51.5 [NEWLINE] 38.3 [NEWLINE] (-13.3) [NEWLINE] 38.9 [NEWLINE] (-12.7) [NEWLINE] 55.7 [NEWLINE] (+4.1) [NEWLINE] 69.1 [NEWLINE] (+17.6) [NEWLINE] llama-2-70B [NEWLINE] 64.0 [NEWLINE] 61.5 [NEWLINE] (-2.6) [NEWLINE] 68.6 [NEWLINE] (+4.5) [NEWLINE] 64.1 [NEWLINE] (+0.1) [NEWLINE] 62.0 [NEWLINE] (-2.1) [NEWLINE] gpt-3.5-turbo [NEWLINE] 67.2 [NEWLINE] 65.3 [NEWLINE] (-1.9) [NEWLINE] 68.5 [NEWLINE] (+1.3) [NEWLINE] 74.2 [NEWLINE] (+6.9) [NEWLINE] 60.9 [NEWLINE] (-6.3) [NEWLINE] 10 [NEWLINE] 0 [NEWLINE] 10 [NEWLINE]  Acc (%) [NEWLINE] llama-30B [NEWLINE] vicuna-v1.3-33B [NEWLINE] falcon-40B [NEWLINE] falcon-inst-40B [NEWLINE] llama-2-70B [NEWLINE] gpt-3.5-turbo [NEWLINE] 25 [NEWLINE] 50 [NEWLINE] 75 [NEWLINE] Recall (%) [NEWLINE] A [NEWLINE] B [NEWLINE] C [NEWLINE] D [NEWLINE] Figure 2: The performance ﬂuctua- [NEWLINE] tion (left) correlates with the balance [NEWLINE] of different option IDs’ recalls in the [NEWLINE] original inputs, as the original recall [NEWLINE] of option ID X is actually calculated [NEWLINE] on a subset of samples where all the [NEWLINE] golden answers are moved to X. [NEWLINE] 32.3% / 19.6%, respectively (averaged over 10 runs). These proportions are statistically nonuniform [NEWLINE] (χ2 Test, p-value ≪10−4) and align well with the performance ﬂuctuations in Table 1. [NEWLINE] Through extensive empirical evaluation (§2.3), with 20 LLMs, on three benchmarks, and with vary- [NEWLINE] ing option numbers (from two to ﬁve), we show that selection bias is prevalent across various LLMs [NEWLINE] and cannot be well mitigated by simple prompting strategies (§2.6), like Chain-of-Thought prompt- [NEWLINE] ing (Wei et al., 2022; Kojima et al., 2022). It varies with models but manifests a cross-domain [NEWLINE] similarity within the same model (§2.3). With careful ablation analyses (§2.4), we ﬁnd that, con- [NEWLINE] trary to the common view in previous work (Wang et al., 2023a; Pezeshkpour & Hruschka, 2023), [NEWLINE] selection bias arises less from LLMs’ position bias, where they are deemed to favor options pre- [NEWLINE] sented at speciﬁc ordering positions (like ﬁrst or last). In contrast, we pinpoint one more salient [NEWLINE] intrinsic cause of selection bias as the model’s token bias when predicting answers from the option [NEWLINE] IDs given the standard MCQ prompt, where the model a priori assigns more probabilistic mass to [NEWLINE] speciﬁc ID tokens (e.g., A/B/C/D). [NEWLINE] To efﬁciently mitigate selection bias, we propose a method called PriDe (§3), referring to Debiasing [NEWLINE] with Prior estimation. In PriDe, we assume the model’s prior bias for option IDs can be separated [NEWLINE] from the overall prediction distribution. PriDe ﬁrst estimates the prior by permutating option con- [NEWLINE] tents on a small number of test samples (e.g., 5%), and then applies it to debias the remaining [NEWLINE] samples. The whole debiasing procedure needs no sample labels, takes place during the inference [NEWLINE] time, and requires only negligible extra computational costs, which is especially suitable for modern [NEWLINE] LLMs. We demonstrate that PriDe achieves superior debiasing effectiveness to strong baselines, es- [NEWLINE] pecially in the low-cost scenario (§4.1). Furthermore, the prior estimated by PriDe provides a good [NEWLINE] interpretation for selection bias (§4.1) and can transfer well across different domains (§4.2), which [NEWLINE] highlights its practical potential in broader scenarios. [NEWLINE] Summary of Contributions [NEWLINE] (1) We identify the ubiquitous selection bias in LLMs and provide [NEWLINE] extensive empirical analyses (with 20 LLMs, on three MCQ benchmarks) and valuable insights on [NEWLINE] this problem. (2) We pinpoint LLMs’ token bias as one primary intrinsic cause of selection bias. [NEWLINE] (3) We propose a label-free, inference-time debiasing method PriDe, which demonstrates notable [NEWLINE] effectiveness and efﬁciency, interpretability, and cross-domain transferability. We hope this work [NEWLINE] can inspire future research on the bias and robustness of LLMs. [NEWLINE] 2 [NEWLINE] INVESTIGATION ON SELECTION BIAS [NEWLINE] 2.1 [NEWLINE] EXPERIMENTAL SETUP [NEWLINE] Models [NEWLINE] Our study focuses on the causal, decoder-only LLMs since this architecture has become [NEWLINE] the dominant choice for modern LLMs. We experiment with 20 LLMs from popular LLM families [NEWLINE] across various sizes: llama-7/13/30/65B (Touvron et al., 2023a), llama-2(-chat)-7/13/70B [NEWLINE] (Touvron et al., 2023b), vicuna-v1.3-7/13/33B, vicuna-v1.5-7/13B (Chiang et al., 2023), [NEWLINE] falcon(-inst)-7/40B (Almazrouei et al., 2023), and gpt-3.5-turbo-0613 (OpenAI, 2022). [NEWLINE] The models except gpt-3.5-turbo are all open source on the HuggingFace website, and we can [NEWLINE] 2 [NEWLINE]  [NEWLINE] Published as a conference paper at ICLR 2024 [NEWLINE] access their output probabilities. gpt-3.5-turbo is the commercial API of ChatGPT. It accepts [NEWLINE] textual prompts and returns generated texts without providing access to output probabilities. [NEWLINE] Benchmarks We conduct experiments on MMLU (Hendrycks et al., 2020), ARC-Challenge (Clark [NEWLINE] et al., 2018), and CommonsenseQA (CSQA) (Talmor et al., 2019), which are all MCQ benchmarks [NEWLINE] widely used for LLM evaluation. Our selection of benchmarks takes into account the diversity [NEWLINE] of tasks and domains. Speciﬁcally, MMLU and ARC consist of 4-option MCQs, while CSQA [NEWLINE] consists of 5-option ones, and MMLU covers tests from 4 domain categories spanning 57 subjects. [NEWLINE] The diverse domains facilitate us to derive general observations and enable cross-domain transfer [NEWLINE] exploration. See Appendix E for detailed data statistics. [NEWLINE] Evaluation [NEWLINE] Our evaluation protocol follows the mainstream LLM evaluation frameworks, such [NEWLINE] as HuggingFace LLM Leaderboard, EleutherAI lm-harness, the original MMLU implementation, [NEWLINE] and OpenAI Evals (see Appendix F). Speciﬁcally, for open-source models, we access the output [NEWLINE] probabilities of option ID tokens A/B/C/D/E and use the maximal one as the model prediction. For [NEWLINE] gpt-3.5-turbo, we compare the golden answer with the ﬁrst generated token, with the decoding [NEWLINE] temperature set to 0. See Figure 7 and 6 in Appendix A for the input formats. [NEWLINE] Our evaluation mainly considers the 0-shot setting, which excludes biases introduced by in-context [NEWLINE] examples, but we also conduct 5-shot experiments. The in-context examples come from the devel- [NEWLINE] opment sets and are shared across all the test samples within the same task. [NEWLINE] 2.2 [NEWLINE] MEASUREMENT OF SELECTION BIAS [NEWLINE] In our study, selection bias is deﬁned as the model’s behavioral bias to select speciﬁc option IDs as [NEWLINE] answers. To measure selection bias, one naive way is based on the counting for model predictions, [NEWLINE] which, however, is susceptible to label imbalance. We instead propose to measure selection bias [NEWLINE] based on the balance of recalls of different option IDs and use the standard deviation of recalls [NEWLINE] (RStd) as a quantitative metric. This measurement is intuitive that greater recall imbalance indicates [NEWLINE] more pronounced selection bias and is not as susceptible to label imbalance as the counting-based [NEWLINE] measurement. More importantly, recall balance well reﬂects the model’s robustness to option posi- [NEWLINE] tion changes, as illustrated in Figure 2. Hence, we reasonably expect that reducing selection bias [NEWLINE] (measured by recall balance) will improve LLMs’ robustness to option position changes in MCQs. [NEWLINE] Note that measuring selection bias with recall balance implies the premise that the golden answers [NEWLINE] should be randomly placed. We show in Figure 18 in Appendix C that randomly shufﬂing the options [NEWLINE] does not obviously change selection bias, validating the above premise. [NEWLINE] 2.3 [NEWLINE] KEY OBSERVATIONS [NEWLINE] We ﬁrst conduct an extensive evaluation of LLMs on various benchmarks to gain a preliminary [NEWLINE] understanding of selection bias. We show partial results in Figure 3 for a brief presentation and put [NEWLINE] the full results in Appendix B. We draw the following main observations and insights: [NEWLINE] Selection bias is prevalent across various LLMs and varies with model families and sizes. [NEWLINE] In- [NEWLINE] tuitively, selection bias is likely to originate from LLMs’ training data, where some answers (e.g., C) [NEWLINE] may occur more frequently than others. However, we do not observe consistent patterns of selection [NEWLINE] bias within the same model family where the models are trained with the same training data (e.g., [NEWLINE] llama-7/13/30/65B, llama-2-7/13/70B). We speculate that selection bias arises as a product of [NEWLINE] complex interactions between training data composition and ordering, model capacity (number of [NEWLINE] parameters), and other factors like hyperparameters. [NEWLINE] Selection bias within the same model displays a moderate similarity across different domains. [NEWLINE] For instance, under the 0-shot setting, llama-30B consistently prefers A/B on various benchmarks, [NEWLINE] while gpt-3.5-turbo favors C/B more. While the preference ranking may not strictly persist [NEWLINE] across tasks or domains, there is an overarching tendency for each model to lean towards certain [NEWLINE] option IDs (e.g., A and B) and away from others (e.g., C and D). It suggests that selection bias is an [NEWLINE] inherent behavioral bias of LLMs that is less impacted by tasks or domains. [NEWLINE] In-context examples can reduce but may meanwhile alter selection bias. [NEWLINE] As exempliﬁed in [NEWLINE] Figure 3, llama-30B disfavors C under the 0-shot setting but becomes biased towards it under the [NEWLINE] 5-shot setting. We ﬁnd that this alteration still does not display noticeable patterns within the same [NEWLINE] 3 [NEWLINE]  [NEWLINE] Published as a conference paper at ICLR 2024 [NEWLINE] 10 [NEWLINE] 0 [NEWLINE] 10 [NEWLINE] 20 [NEWLINE] llama-30B [NEWLINE] 0-shot [NEWLINE] 5-shot [NEWLINE] 0-shot, Removing IDs [NEWLINE] STEM [NEWLINE] Social Science [NEWLINE] Humanities [NEWLINE] Others [NEWLINE] MMLU [NEWLINE] ARC [NEWLINE] CSQA [NEWLINE] 10 [NEWLINE] 5 [NEWLINE] 0 [NEWLINE] 5 [NEWLINE] 10 [NEWLINE] gpt-3.5-turbo [NEWLINE] STEM [NEWLINE] Social Science [NEWLINE] Humanities [NEWLINE] Others [NEWLINE] MMLU [NEWLINE] ARC [NEWLINE] CSQA [NEWLINE] STEM [NEWLINE] Social Science [NEWLINE] Humanities [NEWLINE] Others [NEWLINE] MMLU [NEWLINE] ARC [NEWLINE] CSQA [NEWLINE] A [NEWLINE] B [NEWLINE] C [NEWLINE] D [NEWLINE] E [NEWLINE] Figure 3: Selection bias of llama-30B and gpt-3.5-turbo on various benchmarks, see Appendix B for all [NEWLINE] the 20 LLMs’ results. Y-axis: the recall score (%) normalized by subtracting the overall accuracy. MMLU is [NEWLINE] additionally split into four domains (STEM, Social Science, Humanities, Others) based on its subject categories. [NEWLINE] model family. It indicates that in-context examples can introduce new biases that will be intertwined [NEWLINE] with the inherent selection bias, making the latter complex and less regular. [NEWLINE] 2.4 [NEWLINE] WHAT CAUSES SELECTION BIAS? [NEWLINE] Given the ubiquity of selection bias in various LLMs, we now seek to ﬁgure out the intrinsic causes [NEWLINE] resulting in this behavioral bias. We propose two hypotheses: (1) Token bias. In the standard MCQ [NEWLINE] prompt (Figure 1), when selecting answers from the option IDs, the model may a priori assign [NEWLINE] more probabilistic mass to speciﬁc ID tokens (such as A or C). (2) Position bias. The model may [NEWLINE] favor options presented at speciﬁc ordering positions (such as the ﬁrst or last one). Note that in a [NEWLINE] recent work, Wang et al. (2023a) similarly found that GPT-4 exhibits the bias towards the responses [NEWLINE] from “Assistant 1”. However, it is still unclear whether it is because the preferred responses are [NEWLINE] “selected via the ID token 1” or because they are “presented ﬁrst”. [NEWLINE] Table 2: Preliminary debiasing results (0-shot, [NEWLINE] gpt-3.5-turbo), measured by the standard [NEWLINE] deviation of recalls (RStd) and accuracy (Acc). [NEWLINE] Methods [NEWLINE] MMLU [NEWLINE] ARC [NEWLINE] RStd [NEWLINE] Acc [NEWLINE] RStd [NEWLINE] Acc [NEWLINE] Default [NEWLINE] 5.5 [NEWLINE] 67.2 [NEWLINE] 3.3 [NEWLINE] 84.3 [NEWLINE] a/b/c/d [NEWLINE] 6.8 [NEWLINE] 67.0 [NEWLINE] 2.1 [NEWLINE] 83.1 [NEWLINE] 1/2/3/4 [NEWLINE] 3.8 [NEWLINE] 65.8 [NEWLINE] 2.1 [NEWLINE] 82.3 [NEWLINE] (A)/(B)/(C)/(D) [NEWLINE] 8.1 [NEWLINE] 66.5 [NEWLINE] 4.0 [NEWLINE] 82.4 [NEWLINE] Debiasing Instruct [NEWLINE] 6.1 [NEWLINE] 66.3 [NEWLINE] 3.9 [NEWLINE] 84.2 [NEWLINE] Chain-of-Thought [NEWLINE] 4.5 [NEWLINE] 66.8 [NEWLINE] 3.4 [NEWLINE] 84.5 [NEWLINE] Shufﬂing IDs [NEWLINE] 5.1 [NEWLINE] 63.9 [NEWLINE] 3.7 [NEWLINE] 80.3 [NEWLINE] Removing IDs [NEWLINE] 1.0 [NEWLINE] 66.7 [NEWLINE] 0.6 [NEWLINE] 84.9 [NEWLINE] One challenge here is that option IDs are bound [NEWLINE] with options’ ordering positions, e.g., the ID B [NEWLINE] is naturally tied with the second-presented option. [NEWLINE] To distinguish the impacts of the two hypothesized [NEWLINE] causes, we conduct two ablation experiments. (1) [NEWLINE] Shufﬂing option IDs. We randomly shufﬂe the [NEWLINE] default ID ordering A/B/C/D, for instance, into [NEWLINE] B/D/A/C or C/A/D/B, etc. In this way, B can de- [NEWLINE] note the option presented at any ordering position, [NEWLINE] thus eliminating the impact of position bias and [NEWLINE] leaving only token bias. Note that this ablation [NEWLINE] will obviously impair the naturalness and qual- [NEWLINE] ity of the MCQ prompt, and may consequently de- [NEWLINE] grade model performance (as shown in Table 2). [NEWLINE] (2) Removing option IDs and asking the model to directly select option contents. In this way, the [NEWLINE] change of selection bias would indicate the impact of token bias, while the remaining part corre- [NEWLINE] sponds to position bias. When evaluating LLMs without option IDs, we require gpt-3.5-turbo to [NEWLINE] generate the whole selected option, which is then compared with the golden answer. For open-source [NEWLINE] models, we compute the likelihoods of options, normalized by their lengths, and use the maximum [NEWLINE] one as the model prediction. See Figure 9 and 8 in Appendix A for the input formats. [NEWLINE] As shown in Figure 3 and Table 2, the removal of option IDs notably reduces selection bias (RStd [NEWLINE] decreases), while RStd is little changed by shufﬂing option IDs. The former observation, in most [NEWLINE] cases, holds for various LLMs, on different benchmarks, and with varying option numbers (from two [NEWLINE] to ﬁve, where 2/3-option settings are constructed from the original data), see Appendix B and Table [NEWLINE] 3 in Appendix C for detailed results. We also try to replace the default ID symbols A/B/C/D with [NEWLINE] several reasonable alternatives, including a/b/c/d, 1/2/3/4, and (A)/(B)/(C)/(D), but observe [NEWLINE] no remarkable reduction in RStd from the default one, as shown in Table 2. These results conﬁrm [NEWLINE] that the model’s token bias is one primary intrinsic cause of selection bias. [NEWLINE] 4 [NEWLINE]  [NEWLINE] Published as a conference paper at ICLR 2024 [NEWLINE] However, with option IDs removed, the remaining selection bias (corresponding to the impact of [NEWLINE] position bias) varies with models and tasks, see Appendix B and Table 3 in Appendix C. For in- [NEWLINE] stance, the remaining selection bias of llama-13B, vicuna-v1.3-7B, and gpt-3.5-turbo is only [NEWLINE] marginal, while that of llama-30B, vicuna-v1.3-33B, and falcon-40B is still pronounced (al- [NEWLINE] though having been reduced much). The selection bias of llama-2-13/70B even slightly increases [NEWLINE] in MMLU and ARC after option IDs being removed while still decreasing in CSQA, implying the [NEWLINE] potential counteraction between token bias and position bias. These results suggest that the model’s [NEWLINE] position bias is somewhat present but quite irregular, largely depending on models and tasks. [NEWLINE] 2.5 [NEWLINE] CAN WE DEBIAS LLMS BY REMOVING OPTION IDS? [NEWLINE] Despite the notably reduced selection bias, we ﬁnd that removing option IDs usually degrades model [NEWLINE] performance (except in a few cases under the 5-shot setting), see Table 3 and 4 in Appendix C. This [NEWLINE] performance degradation results from the way we leverage LLMs to answer MCQs without option [NEWLINE] IDs, i.e., calculating and comparing the likelihoods of options, which is referred to as the “cloze [NEWLINE] prompt” format in Robinson & Wingate (2022). Their study demonstrates that asking LLMs to [NEWLINE] predict option IDs forms a better MCQ prompt than the “cloze prompt”, which is consistent with [NEWLINE] our observation. Besides, selecting answers by calculating and comparing the likelihoods of options [NEWLINE] is not as convenient and straightforward to implement as directly predicting option IDs. We thus [NEWLINE] suggest that removing option IDs is not a practical method to mitigate selection bias. [NEWLINE] 2.6 [NEWLINE] CAN SIMPLE PROMPTING STRATEGIES MITIGATE SELECTION BIAS? [NEWLINE] As a preliminary debiasing attempt, we apply two simple prompting strategies to gpt-3.5-turbo: [NEWLINE] (1) Explicit debiasing instruction: We append an explicit debiasing instruction in the system mes- [NEWLINE] sage of gpt-3.5-turbo (“Please note that the provided options have been randomly shufﬂed, so [NEWLINE] it is essential to consider them fairly and without bias.”). (2) Chain-of-Thought prompting (Wei [NEWLINE] et al., 2022; Kojima et al., 2022): gpt-3.5-turbo is ﬁrst prompted with “Let’s think step by step:” [NEWLINE] to generate its thought process and then produces the ﬁnal answer. We follow the implementation [NEWLINE] in OpenAI Evals, see Figure 10 in Appendix A for details. As shown in Table 2, the two prompting [NEWLINE] strategies cannot mitigate selection bias well. It suggests that selection bias is an inherent behavioral [NEWLINE] bias of LLMs that cannot be addressed by simple prompt engineering. [NEWLINE] 3 [NEWLINE] METHODOLOGY [NEWLINE] 3.1 [NEWLINE] PERMUTATION-BASED DEBIASING BASELINE AND FORMULATION [NEWLINE] Before proposing our debiasing method, we ﬁrst introduce a strong permutation-based debiasing [NEWLINE] baseline that our method builds upon. It averages the model’s prediction distributions under various [NEWLINE] option permutations (Wang et al., 2023a; Zheng et al., 2023b), which intuitively cancels out both the [NEWLINE] model’s token bias and position bias. [NEWLINE] Formally, we use q to denote the MCQ question. Suppose the n default-ordered option IDs (e.g., [NEWLINE] A/B/C/D) are di and the default-ordered option contents are oi, i ∈{1, 2, . . . , n}. We use I to denote [NEWLINE] a permutation of {1, 2, . . . , n}, I to a set of possible Is. We use gI(i) to denote the index of i in I, [NEWLINE] and xI to the concatenation of the default-ordered option IDs and the I-permuted option contents, [NEWLINE] so that oi is tied with dgI(i) in xI. The permutation-based debiasing baseline can be formulated as: [NEWLINE] ePdebiased(oi|q, x) = 1 [NEWLINE] |I| [NEWLINE] X [NEWLINE] I∈I [NEWLINE] Pobserved(dgI(i)|q, xI), i ∈{1, 2, . . . , n}, [NEWLINE] (1) [NEWLINE] where x is the default input of option IDs and option contents, Pobserved(dgI(i)|q, xI) is the observed [NEWLINE] prediction probability for the option ID dgI(i) (meaning oi being correct) under the option permu- [NEWLINE] tation I, and ePdebiased(oi|q, x) denotes the debiased prediction probability for the option content [NEWLINE] oi. Since computing full permutations is prohibitively expensive (×n! costs), we adopt a practical [NEWLINE] alternative, called Cyclic Permutation, where I = {(i, i + 1, . . . , n, 1, . . . , i −1)}n [NEWLINE] i=1. It reduces [NEWLINE] the computational cost (e.g., LLM forward times) from ×n! to ×n and ensures one pairing between [NEWLINE] each option ID di and option content oj. In Figure 16 in Appendix C, we show that selecting other [NEWLINE] permutations I in Cyclic Permutation, where we still ensure one pairing between each di and oj, [NEWLINE] leads to similar debiasing results. However, the overhead of Cyclic Permutation is still somewhat [NEWLINE] high (×n inference costs), which stimulates us to design more efﬁcient debiasing methods. [NEWLINE] 5 [NEWLINE]  [NEWLINE] Published as a conference paper at ICLR 2024 [NEWLINE] 3.2 [NEWLINE] PREDICTION PROBABILITY DECOMPOSITION [NEWLINE] The core idea of our method PriDe is to obtain a debiased prediction distribution by separating the [NEWLINE] model’s prior bias for option IDs from the overall prediction distribution. Equivalently, it assumes [NEWLINE] that the observed prediction distribution Pobserved over di can be decomposed as a prior distribution [NEWLINE] Pprior over di and a debiased distribution Pdebiased over oi: [NEWLINE] Pobserved(di|q, xI) = Z−1 [NEWLINE] q,xIPprior(di|q, xI)Pdebiased(ofI(i)|q, xI), ∀I ∈I, i ∈{1, 2, ..., n}, (2) [NEWLINE] where fI(i) denotes i-th element in I. Note that we can rewrite the form of Pobserved as a joint proba- [NEWLINE] bility P(di, oj|q, xI) for di and oj, which equals to Pobserved(di|q, xI) if j = fI(i) and 0 otherwise. [NEWLINE] Therefore, the above prediction probability decomposition can be interpreted as the conditional in- [NEWLINE] dependent assumption (ignore the normalization item Zq,xI), where the model holds independent [NEWLINE] beliefs about di and oj. Speciﬁcally, Pdebiased(oj|q, xI) reﬂects the model’s true belief about the [NEWLINE] option content oj, which is not inﬂuenced by the option ID di. In contrast, Pprior(di|q, xI) indi- [NEWLINE] cates the model’s prior bias for the option ID di, which actually involves not only the model’s [NEWLINE] token bias but also position bias (§2.4), due to the natural binding between option IDs and options’ [NEWLINE] ordering positions. Hence, under this formulation, we do not need to strictly distinguish the two [NEWLINE] biases laboriously, but can instead address them together by eliminating Pprior. [NEWLINE] For tractable derivation, we reasonably assume that Pdebiased is not affected by how the options are [NEWLINE] ordered, which implies its invariance to option permutation I so we can replace xI with the default [NEWLINE] input x. We also assume Pprior to be independent of xI, which means that the prior for option IDs [NEWLINE] depends on only the question q. Equation 2 is then simpliﬁed to: [NEWLINE] Pobserved(di|q, xI) = Z−1 [NEWLINE] q,xIPprior(di|q)Pdebiased(ofI(i)|q, x), ∀I ∈I, i ∈{1, 2, ..., n}. [NEWLINE] (3) [NEWLINE] 3.3 [NEWLINE] DEBIASING WITH PRIOR ESTIMATION [NEWLINE] Taking the logarithm of both sides of Equation 3 and summing over all I ∈I, we can obtain: [NEWLINE] X [NEWLINE] I∈I [NEWLINE] log Pobserved(di|q, xI) = |I| log Pprior(di|q) + [NEWLINE]  X [NEWLINE] I∈I [NEWLINE] log Pdebiased(ofI(i)|q, x) [NEWLINE]  [NEWLINE] + C [NEWLINE] (4) [NEWLINE] = |I| log Pprior(di|q) + [NEWLINE] |I| [NEWLINE] n [NEWLINE] n [NEWLINE] X [NEWLINE] j=1 [NEWLINE] log Pdebiased(oj|q, x) [NEWLINE]  [NEWLINE] + C [NEWLINE] (5) [NEWLINE] = |I| log Pprior(di|q) + C′, i ∈{1, 2, ..., n}. [NEWLINE] (6) [NEWLINE] We derive Equation 5 because P [NEWLINE] I∈I log Pdebiased(ofI(i)|q, x) actually involves |I|/n iterations [NEWLINE] over each Pdebiased(oj|q, x) (given I contains either full or cyclic permutations), whose aggregation [NEWLINE] over j ∈{1, 2, . . . , n} is a constant. Therefore, without any sample labels, we can obtain: [NEWLINE] Pprior(di|q) = softmax [NEWLINE]  1 [NEWLINE] |I| [NEWLINE] X [NEWLINE] I∈I [NEWLINE] log Pobserved(di|q, xI) [NEWLINE]  [NEWLINE] , i ∈{1, 2, ..., n}. [NEWLINE] (7) [NEWLINE] Recall our observation in §2.3 that selection bias within the same model displays a moderate cross- [NEWLINE] domain similarity. This implies that the prior for option IDs is likely to transfer across different [NEWLINE] samples and domains, which motivates us to compute the priors of partial test samples and use them [NEWLINE] as an approximation for the remaining samples. It can largely improve debiasing efﬁciency since no [NEWLINE] more computational overhead is needed for the remaining samples once the prior is estimated. [NEWLINE] Drawing the above inspiration, PriDe ﬁrst takes K test samples De from the test set D, where K can [NEWLINE] be adjusted based on the estimation budget. Each sample in De undergoes the standard permutation- [NEWLINE] based debiasing in Equation 1, during which we estimate each sample-speciﬁc prior Pprior(di|q) [NEWLINE] using Equation 7. For the remaining samples Dr = D\De, we compute the “global prior” ePprior(di) [NEWLINE] by averaging the previously computed priors as an approximation to the new sample’s Pprior(di|q) [NEWLINE] in Equation 3. We can thus compute the approximated ePdebiased(oi|q, x) and obtain the debiased [NEWLINE] prediction (omit the superscript I as we only use the default input here): [NEWLINE] ePdebiased(oi|q, x) ∝Pobserved(di|q, x)/ ePprior(di), i ∈{1, 2, ..., n}. [NEWLINE] (8) [NEWLINE] When K ≪|D|, the overhead for prior estimation will become negligible compared to the whole [NEWLINE] inference cost. The overall procedure of PriDe is summarized as Algorithm 1. [NEWLINE] 6 [NEWLINE]  [NEWLINE] Published as a conference paper at ICLR 2024 [NEWLINE] Algorithm 1 PriDe: Debiasing with Prior Estimation [NEWLINE] Require: Language model, test samples D = {(qi, xi)}i, option number n, estimation budget K [NEWLINE] Ensure: Model predictions Y [NEWLINE] 1: Initialize the model prediction set Y = ∅and the prior set P = ∅ [NEWLINE] ▷Initialization [NEWLINE] 2: Sample the estimation samples De under K and the remaining samples Dr = D\De [NEWLINE] 3: for (q, x) ∈De do [NEWLINE] 4: [NEWLINE] Debias the model prediction using Equation 1, add the predicted answer to Y [NEWLINE] 5: [NEWLINE] Estimate the sample-speciﬁc prior Pprior(di|q) using Equation 7, add it into P [NEWLINE] 6: end for [NEWLINE] 7: Estimate the global prior ePprior(di) by averaging P [NEWLINE] ▷Prior Estimation [NEWLINE] 8: for (q, x) ∈Dr do [NEWLINE] 9: [NEWLINE] Debias the model prediction using Equation 8 [NEWLINE] ▷Efﬁcient Debiasing [NEWLINE] 10: [NEWLINE] Add the predicted answer to Y [NEWLINE] 11: end for [NEWLINE] 12: return Y [NEWLINE] -9 [NEWLINE] -6 [NEWLINE] -3 [NEWLINE] 0 [NEWLINE]  Recall Std (%) [NEWLINE] MMLU [NEWLINE] Full Perm [NEWLINE] Cyclic Perm [NEWLINE] PriDe [NEWLINE] -9 [NEWLINE] -6 [NEWLINE] -3 [NEWLINE] 0 [NEWLINE] ARC [NEWLINE] Full Perm [NEWLINE] Cyclic Perm [NEWLINE] PriDe [NEWLINE] -12 [NEWLINE] -9 [NEWLINE] -6 [NEWLINE] -3 [NEWLINE] 0 [NEWLINE] CSQA [NEWLINE] Cyclic Perm [NEWLINE] PriDe [NEWLINE] x1 [NEWLINE] x2 [NEWLINE] x4 [NEWLINE] x8 [NEWLINE] x24 [NEWLINE] 0 [NEWLINE] 2 [NEWLINE] 4 [NEWLINE] 6 [NEWLINE]  Accuracy (%) [NEWLINE] Full Perm [NEWLINE] Cyclic Perm [NEWLINE] PriDe [NEWLINE] x1 [NEWLINE] x2 [NEWLINE] x4 [NEWLINE] x8 [NEWLINE] x24 [NEWLINE] 0 [NEWLINE] 2 [NEWLINE] 4 [NEWLINE] 6 [NEWLINE] Full Perm [NEWLINE] Cyclic Perm [NEWLINE] PriDe [NEWLINE] x1 [NEWLINE] x2 [NEWLINE] x3 [NEWLINE] x4 [NEWLINE] x5 [NEWLINE] 0 [NEWLINE] 2 [NEWLINE] 4 [NEWLINE] 6 [NEWLINE] 8 [NEWLINE] Cyclic Perm [NEWLINE] PriDe [NEWLINE] Figure 4: Debiasing results (0-shot, averaged over all the 20 LLMs) under varying computational costs (X- [NEWLINE] axis), see Table 3 and 4 in Appendix C for detailed breakdowns. We control the costs of Cyclic/Full Perm via [NEWLINE] the ratio β of the debiased test samples, where we take β ∈{0%, 10%, 20%, 40%, 60%, 80%, 100%}. For [NEWLINE] PriDe, we control the costs via the ratio α of test samples for prior estimation (these samples are meanwhile [NEWLINE] directly debiased via Cyclic Perm), where we take α ∈{2%, 5%, 10%, 20%, 40%, 60%, 80%}. Note that [NEWLINE] when α = 100%, PriDe degenerates to Cyclic Perm, so we do not plot them. [NEWLINE] 4 [NEWLINE] EXPERIMENTS [NEWLINE] 4.1 [NEWLINE] MAIN RESULTS [NEWLINE] We compare PriDe with two strong permutation-based debiasing baselines: Cyclic Permutation [NEWLINE] and Full Permutation. The latter is not experimented on the 5-option CSQA benchmark due to the [NEWLINE] extremely high cost. Since gpt-3.5-turbo does not return the output probability, we sample 100 [NEWLINE] generated answers as an approximation to Pobserved. For PriDe, we randomly sample K = α|D| [NEWLINE] test samples as De and report the average results over 5 runs. Here, we use α ∈(0, 1) as the ratio of [NEWLINE] test samples for prior estimation to control the estimation overhead. [NEWLINE] Figure 4 presents the debiasing results (averaged over all the models) versus the computational costs [NEWLINE] under the 0-shot setting, see detailed breakdowns in Table 3 and 4 in Appendix C. PriDe achieves [NEWLINE] superior debiasing effectiveness and performance improvement to Full/Cyclic Perm, especially in [NEWLINE] the low-cost scenario. This also holds under the 5-shot setting, see Figure 19 in Appendix C. In [NEWLINE] Figure 17 in Appendix C, we show that the estimated prior manifests a clear correlation with the [NEWLINE] empirical selection bias (i.e., the recalls of different option positions before debiasing). It suggests [NEWLINE] that PriDe can provide a good interpretation for the model’s selection bias. Furthermore, we observe [NEWLINE] that the priors are stable when estimated with different sizes of test samples (from 2% to 20%). It [NEWLINE] suggests that we are able to obtain a reliable estimate of prior even with a limited computational [NEWLINE] 7 [NEWLINE]  [NEWLINE] Published as a conference paper at ICLR 2024 [NEWLINE] Figure 5: Cross-domain debiasing results of PriDe (0-shot, averaged over all the 20 LLMs), with priors esti- [NEWLINE] mated using α = 5% test samples from the source domains. [NEWLINE] budget. It also implies that the model’s prior bias for option IDs exhibits a similar pattern across [NEWLINE] different samples, conﬁrming our design motivation of PriDe in §3.3. [NEWLINE] Note that one may propose to debias with fewer permutations (such as 2 or 3 random permutations, [NEWLINE] with ×2 or ×3 costs) as a low-cost alternative to Cyclic (×n) or Full (×n!) Perm. In Figure 21 and [NEWLINE] 22 in Appendix C, we show that PriDe can still be combined with these methods and notably boost [NEWLINE] debiasing effectiveness and efﬁciency. [NEWLINE] 4.2 [NEWLINE] TRANSFERABILITY ANALYSIS [NEWLINE] In practical scenarios, we may not guarantee that the test samples always come from the same or [NEWLINE] similar domains. We hope the prior estimated by PriDe to be transferable: Once the prior is estimated [NEWLINE] using a small number of samples from domain X, it can be used to debias not only other samples [NEWLINE] from domain X but also samples from domain Y. To this end, we evaluate the cross-domain transfer [NEWLINE] performance of estimated priors on the four category domains of MMLU and ARC (5 domains in [NEWLINE] total, all are 4-option MCQs). We ﬁrst use PriDe to estimate the prior with α test samples from a [NEWLINE] source domain X, and then apply it to debiasing the test samples from a target domain Y. As shown [NEWLINE] in Figure 5, the estimated priors exhibit reasonable transferability acr"
