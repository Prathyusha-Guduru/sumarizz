{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "98542915427e4efaa6e6c0e61f3f2e62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_62c98d824ba146b2b270032e96596c54",
              "IPY_MODEL_c990b546f3484b648cef1d185a404705",
              "IPY_MODEL_bb23f3ec55cd4b14acf186ed7d4ecb83"
            ],
            "layout": "IPY_MODEL_be68e69d4fd34d91b1210a5fed1fd25f"
          }
        },
        "62c98d824ba146b2b270032e96596c54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8d65cb066b524dda97d3af618145ecbb",
            "placeholder": "​",
            "style": "IPY_MODEL_92d8bba193c14624821f311d7f60cf2d",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "c990b546f3484b648cef1d185a404705": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c7ee1c46ef9b43519782d18c27bd9691",
            "max": 27,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2d80ca536f3e4f269c83a49eb19fe29c",
            "value": 27
          }
        },
        "bb23f3ec55cd4b14acf186ed7d4ecb83": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a3e31e0f570d4e8b8ed1dc34fab6a341",
            "placeholder": "​",
            "style": "IPY_MODEL_5cf103f4ef034185b9ec05fc858a7625",
            "value": " 27.0/27.0 [00:00&lt;00:00, 3.13kB/s]"
          }
        },
        "be68e69d4fd34d91b1210a5fed1fd25f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d65cb066b524dda97d3af618145ecbb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "92d8bba193c14624821f311d7f60cf2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7ee1c46ef9b43519782d18c27bd9691": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2d80ca536f3e4f269c83a49eb19fe29c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a3e31e0f570d4e8b8ed1dc34fab6a341": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5cf103f4ef034185b9ec05fc858a7625": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6d769bd148b44e2da1d75d4c4de3e413": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e1f672dcc64a4b69b3b769d57a567fb5",
              "IPY_MODEL_ed4b3fa957aa4b3fa7c69e8197bf6fbe",
              "IPY_MODEL_f2cfb52d1bce47fd86802ceb62d8287c"
            ],
            "layout": "IPY_MODEL_ecf663c582f847d9958e8ab84246a27f"
          }
        },
        "e1f672dcc64a4b69b3b769d57a567fb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_04e21a9da8e347b78985cfc58c2491b4",
            "placeholder": "​",
            "style": "IPY_MODEL_dfdb487c8f7045ebbdf0085eb43ff5c7",
            "value": "vocab.json: 100%"
          }
        },
        "ed4b3fa957aa4b3fa7c69e8197bf6fbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_893fe308b67a4bbdb0ab987e5fa7200f",
            "max": 898822,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b26e9198d61149ddb063ab4b0f512de3",
            "value": 898822
          }
        },
        "f2cfb52d1bce47fd86802ceb62d8287c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc5ec9ea200b4b91a41bf9919f720475",
            "placeholder": "​",
            "style": "IPY_MODEL_028a37929c8a4287aa46903921517652",
            "value": " 899k/899k [00:00&lt;00:00, 2.08MB/s]"
          }
        },
        "ecf663c582f847d9958e8ab84246a27f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04e21a9da8e347b78985cfc58c2491b4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dfdb487c8f7045ebbdf0085eb43ff5c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "893fe308b67a4bbdb0ab987e5fa7200f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b26e9198d61149ddb063ab4b0f512de3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cc5ec9ea200b4b91a41bf9919f720475": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "028a37929c8a4287aa46903921517652": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "539ba924d65a4a25bb6e74314cc28474": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_14e7bde25cd44812a51458578a3181cc",
              "IPY_MODEL_57b115c7f34a4a58af028a3d4dee54c3",
              "IPY_MODEL_72b72585995042bf89b18f3bec0604a1"
            ],
            "layout": "IPY_MODEL_75932bf297a742e38963c52850d1c692"
          }
        },
        "14e7bde25cd44812a51458578a3181cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_50e0e79d05924fdcab1c9a74361ce2c4",
            "placeholder": "​",
            "style": "IPY_MODEL_4a13b366be1342859c001e6d5ea7caf5",
            "value": "merges.txt: 100%"
          }
        },
        "57b115c7f34a4a58af028a3d4dee54c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f8c1477a99244570aaba6331e193d9d1",
            "max": 456318,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8a51f26a8d9e4b608212b0bce0d45580",
            "value": 456318
          }
        },
        "72b72585995042bf89b18f3bec0604a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_73a907512a6042f4a1cfc9c0a55decbb",
            "placeholder": "​",
            "style": "IPY_MODEL_acbb0bb79e734faaa0d9c0205eb34a0b",
            "value": " 456k/456k [00:00&lt;00:00, 1.06MB/s]"
          }
        },
        "75932bf297a742e38963c52850d1c692": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50e0e79d05924fdcab1c9a74361ce2c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4a13b366be1342859c001e6d5ea7caf5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f8c1477a99244570aaba6331e193d9d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a51f26a8d9e4b608212b0bce0d45580": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "73a907512a6042f4a1cfc9c0a55decbb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "acbb0bb79e734faaa0d9c0205eb34a0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "42a3b5c32fa145abafd5db54081291a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_95c688355a5240f5b65c007b17c0bf28",
              "IPY_MODEL_3a1c47a69dc7405a9461fef06fa72a3c",
              "IPY_MODEL_6ff22d8302394516a658eec1277ab311"
            ],
            "layout": "IPY_MODEL_c0696d6400924c91b4790fee867a2dfb"
          }
        },
        "95c688355a5240f5b65c007b17c0bf28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ea6884324d4435db5fa21cab197f4f6",
            "placeholder": "​",
            "style": "IPY_MODEL_9eec76411b094c638d6fe7367c7664ef",
            "value": "special_tokens_map.json: 100%"
          }
        },
        "3a1c47a69dc7405a9461fef06fa72a3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_25777560397a4ba68e24cb56db896554",
            "max": 772,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5c4b8f16f093491a84c85619b838719c",
            "value": 772
          }
        },
        "6ff22d8302394516a658eec1277ab311": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9dc8677ff823499cb7861e6ababcda71",
            "placeholder": "​",
            "style": "IPY_MODEL_c121304b8cec4002853316b706ef7f1e",
            "value": " 772/772 [00:00&lt;00:00, 94.4kB/s]"
          }
        },
        "c0696d6400924c91b4790fee867a2dfb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ea6884324d4435db5fa21cab197f4f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9eec76411b094c638d6fe7367c7664ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "25777560397a4ba68e24cb56db896554": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5c4b8f16f093491a84c85619b838719c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "9dc8677ff823499cb7861e6ababcda71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c121304b8cec4002853316b706ef7f1e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2b75ac9b86f24e7caac2401d9f030268": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f82f50198b9c4ee29d0b4d4d5fb2d64c",
              "IPY_MODEL_98ebd8d75ee14d2aadbe1c52e784d13d",
              "IPY_MODEL_89ac88ef3b724689978d6e489ba93e73"
            ],
            "layout": "IPY_MODEL_ba87920f83f34a9dae8d480d5aa4aeb2"
          }
        },
        "f82f50198b9c4ee29d0b4d4d5fb2d64c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d863a3c7f622427a8297278dea3a97e9",
            "placeholder": "​",
            "style": "IPY_MODEL_4e74ae7908ac4f0aa1142b5353427f07",
            "value": "config.json: 100%"
          }
        },
        "98ebd8d75ee14d2aadbe1c52e784d13d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_549a9678caa44b109a21b1da8397530e",
            "max": 1291,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8bf64d7d576144dabb1bab0c142967a8",
            "value": 1291
          }
        },
        "89ac88ef3b724689978d6e489ba93e73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e594e0bf6b44b61a5ed38fad11fd5a2",
            "placeholder": "​",
            "style": "IPY_MODEL_5d2b2c6f6196436c9d9a178d5e7d74c4",
            "value": " 1.29k/1.29k [00:00&lt;00:00, 158kB/s]"
          }
        },
        "ba87920f83f34a9dae8d480d5aa4aeb2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d863a3c7f622427a8297278dea3a97e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e74ae7908ac4f0aa1142b5353427f07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "549a9678caa44b109a21b1da8397530e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8bf64d7d576144dabb1bab0c142967a8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7e594e0bf6b44b61a5ed38fad11fd5a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d2b2c6f6196436c9d9a178d5e7d74c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "868a7ae4b7a84bef8c123965c6e503ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cd6fe5f04e584c83b41ca1bf24ec7471",
              "IPY_MODEL_6460a5f0a0cd4bd982c3abbf5ae568fa",
              "IPY_MODEL_9127cea26a344eff94cb9c610a1cf5ec"
            ],
            "layout": "IPY_MODEL_dcb020f860384fd4b01b1bcf795ea0c4"
          }
        },
        "cd6fe5f04e584c83b41ca1bf24ec7471": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_641ab36bad8540b1931148d6e902f47d",
            "placeholder": "​",
            "style": "IPY_MODEL_883a5ea756284b93ac63d672d50baaae",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "6460a5f0a0cd4bd982c3abbf5ae568fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_56f7cca283a6498a88ceb7669f0cd952",
            "max": 1839633783,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_83f25d4f405942e9a32472c5df2ecf8d",
            "value": 1839633783
          }
        },
        "9127cea26a344eff94cb9c610a1cf5ec": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_410abb322a594cf8ab6978b923ecdb18",
            "placeholder": "​",
            "style": "IPY_MODEL_5208ffb98b1e4c15b6631072deae4e7b",
            "value": " 1.84G/1.84G [00:04&lt;00:00, 399MB/s]"
          }
        },
        "dcb020f860384fd4b01b1bcf795ea0c4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "641ab36bad8540b1931148d6e902f47d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "883a5ea756284b93ac63d672d50baaae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "56f7cca283a6498a88ceb7669f0cd952": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83f25d4f405942e9a32472c5df2ecf8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "410abb322a594cf8ab6978b923ecdb18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5208ffb98b1e4c15b6631072deae4e7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5ef97deb62d04dfaaa261fb9bc4ae83a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_016b0b454a514627ae4d8706d2f2f13f",
              "IPY_MODEL_f7d3fc2ec3d84695a63d649f0f63aab7",
              "IPY_MODEL_3c0b07e355654b7696998340fbe92849"
            ],
            "layout": "IPY_MODEL_5ac4a15e9aaa480489089b5c5ff6ac8b"
          }
        },
        "016b0b454a514627ae4d8706d2f2f13f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca3d5254343743fbbad5016d4a64b308",
            "placeholder": "​",
            "style": "IPY_MODEL_1c612b6576a042e2aa2f2679cdee511d",
            "value": "model.safetensors: 100%"
          }
        },
        "f7d3fc2ec3d84695a63d649f0f63aab7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e71200dfd22142c4abbd2e8c36168256",
            "max": 1839478370,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_01972aaa5a494c8f94996d70f81ee986",
            "value": 1839478370
          }
        },
        "3c0b07e355654b7696998340fbe92849": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2ff25218ae794bb48d02014c5425e127",
            "placeholder": "​",
            "style": "IPY_MODEL_e97ba8b2177e45378a1db256ae5f6060",
            "value": " 1.84G/1.84G [00:04&lt;00:00, 439MB/s]"
          }
        },
        "5ac4a15e9aaa480489089b5c5ff6ac8b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ca3d5254343743fbbad5016d4a64b308": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c612b6576a042e2aa2f2679cdee511d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e71200dfd22142c4abbd2e8c36168256": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01972aaa5a494c8f94996d70f81ee986": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2ff25218ae794bb48d02014c5425e127": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e97ba8b2177e45378a1db256ae5f6060": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f8ed0fc2f344fff966adf8093f18771": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6fe96ff781764230b8d3f156891fed68",
              "IPY_MODEL_794a0290130142a9a0f812fb499694a2",
              "IPY_MODEL_5fc5ff7b939045c2b0431af4b389cbf1"
            ],
            "layout": "IPY_MODEL_acccd12ab1d44438b78cae3148adbb9f"
          }
        },
        "6fe96ff781764230b8d3f156891fed68": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bf083db2b2824b609e58afff570ae308",
            "placeholder": "​",
            "style": "IPY_MODEL_ce4a8b79354540a39df1110d33ada923",
            "value": "generation_config.json: 100%"
          }
        },
        "794a0290130142a9a0f812fb499694a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c4b141bf13cf4511b1897884bd6672b6",
            "max": 207,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_95221a4ef6e14a1fb64a9fc38ded21a6",
            "value": 207
          }
        },
        "5fc5ff7b939045c2b0431af4b389cbf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_500c117536594ca2829ee3c9aed97f5c",
            "placeholder": "​",
            "style": "IPY_MODEL_a46918ae856f492096d7baeaf5fc6513",
            "value": " 207/207 [00:00&lt;00:00, 24.2kB/s]"
          }
        },
        "acccd12ab1d44438b78cae3148adbb9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bf083db2b2824b609e58afff570ae308": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ce4a8b79354540a39df1110d33ada923": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c4b141bf13cf4511b1897884bd6672b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95221a4ef6e14a1fb64a9fc38ded21a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "500c117536594ca2829ee3c9aed97f5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a46918ae856f492096d7baeaf5fc6513": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be38d68fe0e247469f2eba8c55208d9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_6750773b7cae45fda891b22c1ea1af2f",
              "IPY_MODEL_e5589abffdcc4698ae27767dcf61bdc1",
              "IPY_MODEL_d22a4098135f43fdb0ef88e2fe53ab91"
            ],
            "layout": "IPY_MODEL_4839e45b80ca40ff8e43d62fa76b7de6"
          }
        },
        "6750773b7cae45fda891b22c1ea1af2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f554c61cfde548bfa0b0869c1d6dce1d",
            "placeholder": "​",
            "style": "IPY_MODEL_918da3c0ae2e4128b098f953f20d9674",
            "value": "Processing articles: 100%"
          }
        },
        "e5589abffdcc4698ae27767dcf61bdc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76a76c53fc1d4d0fada5729246f21e98",
            "max": 11,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f60e22ff1f8443f5a9f25395e6987eb3",
            "value": 11
          }
        },
        "d22a4098135f43fdb0ef88e2fe53ab91": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5df7bd9ef94049229d303bf5d4cdb002",
            "placeholder": "​",
            "style": "IPY_MODEL_e5f35d220b424da09fc0817361d427f8",
            "value": " 11/11 [03:56&lt;00:00, 23.38s/it]"
          }
        },
        "4839e45b80ca40ff8e43d62fa76b7de6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f554c61cfde548bfa0b0869c1d6dce1d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "918da3c0ae2e4128b098f953f20d9674": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76a76c53fc1d4d0fada5729246f21e98": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f60e22ff1f8443f5a9f25395e6987eb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5df7bd9ef94049229d303bf5d4cdb002": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5f35d220b424da09fc0817361d427f8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Curate preference data"
      ],
      "metadata": {
        "id": "1rCd0f1yfdrX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XhCYos9Wjbqc",
        "outputId": "1977e86d-d9e0-4591-fa89-f5a68b1d4b65"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.50.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m73.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m59.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m96.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch"
      ],
      "metadata": {
        "id": "chOK1BNGjUxT"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NeMfKfhojWTX",
        "outputId": "fbdd4518-2ecd-4572-cb70-e99bec75a847"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import LEDForConditionalGeneration, LEDTokenizer"
      ],
      "metadata": {
        "id": "A_ZXH_0pS5fS"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = LEDTokenizer.from_pretrained(\"allenai/led-large-16384-arxiv\")\n",
        "model = LEDForConditionalGeneration.from_pretrained(\"allenai/led-large-16384-arxiv\").to(\"cuda\").half()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 395,
          "referenced_widgets": [
            "98542915427e4efaa6e6c0e61f3f2e62",
            "62c98d824ba146b2b270032e96596c54",
            "c990b546f3484b648cef1d185a404705",
            "bb23f3ec55cd4b14acf186ed7d4ecb83",
            "be68e69d4fd34d91b1210a5fed1fd25f",
            "8d65cb066b524dda97d3af618145ecbb",
            "92d8bba193c14624821f311d7f60cf2d",
            "c7ee1c46ef9b43519782d18c27bd9691",
            "2d80ca536f3e4f269c83a49eb19fe29c",
            "a3e31e0f570d4e8b8ed1dc34fab6a341",
            "5cf103f4ef034185b9ec05fc858a7625",
            "6d769bd148b44e2da1d75d4c4de3e413",
            "e1f672dcc64a4b69b3b769d57a567fb5",
            "ed4b3fa957aa4b3fa7c69e8197bf6fbe",
            "f2cfb52d1bce47fd86802ceb62d8287c",
            "ecf663c582f847d9958e8ab84246a27f",
            "04e21a9da8e347b78985cfc58c2491b4",
            "dfdb487c8f7045ebbdf0085eb43ff5c7",
            "893fe308b67a4bbdb0ab987e5fa7200f",
            "b26e9198d61149ddb063ab4b0f512de3",
            "cc5ec9ea200b4b91a41bf9919f720475",
            "028a37929c8a4287aa46903921517652",
            "539ba924d65a4a25bb6e74314cc28474",
            "14e7bde25cd44812a51458578a3181cc",
            "57b115c7f34a4a58af028a3d4dee54c3",
            "72b72585995042bf89b18f3bec0604a1",
            "75932bf297a742e38963c52850d1c692",
            "50e0e79d05924fdcab1c9a74361ce2c4",
            "4a13b366be1342859c001e6d5ea7caf5",
            "f8c1477a99244570aaba6331e193d9d1",
            "8a51f26a8d9e4b608212b0bce0d45580",
            "73a907512a6042f4a1cfc9c0a55decbb",
            "acbb0bb79e734faaa0d9c0205eb34a0b",
            "42a3b5c32fa145abafd5db54081291a0",
            "95c688355a5240f5b65c007b17c0bf28",
            "3a1c47a69dc7405a9461fef06fa72a3c",
            "6ff22d8302394516a658eec1277ab311",
            "c0696d6400924c91b4790fee867a2dfb",
            "3ea6884324d4435db5fa21cab197f4f6",
            "9eec76411b094c638d6fe7367c7664ef",
            "25777560397a4ba68e24cb56db896554",
            "5c4b8f16f093491a84c85619b838719c",
            "9dc8677ff823499cb7861e6ababcda71",
            "c121304b8cec4002853316b706ef7f1e",
            "2b75ac9b86f24e7caac2401d9f030268",
            "f82f50198b9c4ee29d0b4d4d5fb2d64c",
            "98ebd8d75ee14d2aadbe1c52e784d13d",
            "89ac88ef3b724689978d6e489ba93e73",
            "ba87920f83f34a9dae8d480d5aa4aeb2",
            "d863a3c7f622427a8297278dea3a97e9",
            "4e74ae7908ac4f0aa1142b5353427f07",
            "549a9678caa44b109a21b1da8397530e",
            "8bf64d7d576144dabb1bab0c142967a8",
            "7e594e0bf6b44b61a5ed38fad11fd5a2",
            "5d2b2c6f6196436c9d9a178d5e7d74c4",
            "868a7ae4b7a84bef8c123965c6e503ea",
            "cd6fe5f04e584c83b41ca1bf24ec7471",
            "6460a5f0a0cd4bd982c3abbf5ae568fa",
            "9127cea26a344eff94cb9c610a1cf5ec",
            "dcb020f860384fd4b01b1bcf795ea0c4",
            "641ab36bad8540b1931148d6e902f47d",
            "883a5ea756284b93ac63d672d50baaae",
            "56f7cca283a6498a88ceb7669f0cd952",
            "83f25d4f405942e9a32472c5df2ecf8d",
            "410abb322a594cf8ab6978b923ecdb18",
            "5208ffb98b1e4c15b6631072deae4e7b",
            "5ef97deb62d04dfaaa261fb9bc4ae83a",
            "016b0b454a514627ae4d8706d2f2f13f",
            "f7d3fc2ec3d84695a63d649f0f63aab7",
            "3c0b07e355654b7696998340fbe92849",
            "5ac4a15e9aaa480489089b5c5ff6ac8b",
            "ca3d5254343743fbbad5016d4a64b308",
            "1c612b6576a042e2aa2f2679cdee511d",
            "e71200dfd22142c4abbd2e8c36168256",
            "01972aaa5a494c8f94996d70f81ee986",
            "2ff25218ae794bb48d02014c5425e127",
            "e97ba8b2177e45378a1db256ae5f6060",
            "3f8ed0fc2f344fff966adf8093f18771",
            "6fe96ff781764230b8d3f156891fed68",
            "794a0290130142a9a0f812fb499694a2",
            "5fc5ff7b939045c2b0431af4b389cbf1",
            "acccd12ab1d44438b78cae3148adbb9f",
            "bf083db2b2824b609e58afff570ae308",
            "ce4a8b79354540a39df1110d33ada923",
            "c4b141bf13cf4511b1897884bd6672b6",
            "95221a4ef6e14a1fb64a9fc38ded21a6",
            "500c117536594ca2829ee3c9aed97f5c",
            "a46918ae856f492096d7baeaf5fc6513"
          ]
        },
        "id": "kjc01yXKSl6Z",
        "outputId": "1da0b644-824b-46e1-de5f-2f05b5199d45"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/27.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "98542915427e4efaa6e6c0e61f3f2e62"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6d769bd148b44e2da1d75d4c4de3e413"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "539ba924d65a4a25bb6e74314cc28474"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/772 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "42a3b5c32fa145abafd5db54081291a0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2b75ac9b86f24e7caac2401d9f030268"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/1.84G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "868a7ae4b7a84bef8c123965c6e503ea"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.84G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5ef97deb62d04dfaaa261fb9bc4ae83a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/207 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3f8ed0fc2f344fff966adf8093f18771"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"article.csv\")"
      ],
      "metadata": {
        "id": "htlKE6aZTAGp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = df.iloc[0][\"article\"]"
      ],
      "metadata": {
        "id": "H-oqClIXTMpZ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "id": "-xs3IE3LTRoe",
        "outputId": "3e7d8f11-b0b8-436d-92c4-38ac72aff1a4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Published as a conference paper at ICLR 2024TOWARDS UNDERSTANDING FACTUAL KNOWLEDGE OFLARGE LANGUAGE MODELSXuming Hu1,2*, Junzhe Chen1*, Xiaochuan Li1*, Yufei Guo1, Lijie Wen1†,Philip S. Yu3, Zhijiang Guo4†1 Tsinghua University2 The Hong Kong University of Science and Technology (Guangzhou)3 University of Illinois at Chicago4 University of Cambridgexuminghu@hkust-gz.edu.cn, wenlj@tsinghua.edu.cn, zg283@cam.ac.ukABSTRACTLarge language models (LLMs) have recently driven striking performance im-provements across a range of natural language processing tasks. The factualknowledge acquired during pretraining and instruction tuning can be useful invarious downstream tasks, such as question answering, and language generation.Unlike conventional Knowledge Bases (KBs) that explicitly store factual knowl-edge, LLMs implicitly store facts in their parameters. Content generated by theLLMs can often exhibit inaccuracies or deviations from the truth, due to facts thatcan be incorrectly induced or become obsolete over time. To this end, we aim toexplore the extent and scope of factual knowledge within LLMs by designing thebenchmark Pinocchio. Pinocchio contains 20K diverse factual questions that spandifferent sources, timelines, domains, regions, and languages. Furthermore, weinvestigate whether LLMs can compose multiple facts, update factual knowledgetemporally, reason over multiple pieces of facts, identify subtle factual differences,and resist adversarial examples. Extensive experiments on different sizes and typesof LLMs show that existing LLMs still lack factual knowledge and suffer fromvarious spurious correlations. We believe this is a critical bottleneck for realizingtrustworthy artificial intelligence. The dataset Pinocchio and our codes are publiclyavailable at: https://github.com/THU-BPM/Pinocchio.1INTRODUCTIONLarge language models (LLMs) have revolutionized natural language processing (NLP) in recentyears since they have significantly improved performance on various downstream tasks (Brown et al.,2020; Chowdhery et al., 2022; Ouyang et al., 2022; Touvron et al., 2023a;b; OpenAI, 2022; 2023).Prior efforts have shown that language models can store factual knowledge and act as knowledgebases (Petroni et al., 2019; Jiang et al., 2020c). Factual knowledge in language models acquiredduring pretraining can benefit knowledge-intensive downstream tasks such as question answering andfact checking (Roberts et al., 2020; Yu et al., 2023a; Pan et al., 2023).Despite advancements in LLMs, they still struggle with generating content that exhibits inaccuraciesor deviations from the facts and making reasoning errors (Lin et al., 2022; Bubeck et al., 2023).These factual errors can be difficult to identify since LLMs implicitly memorize facts through theirparameters rather than explicitly store factual knowledge as traditional Knowledge Bases. Accessingand interpreting the computations and memories of these models can be challenging (Ribeiro et al.,2016; Belinkov & Glass, 2019), especially when APIs are the only means of interaction and manyinterpretation methods rely on weights and representations (Cao et al., 2021b). The presence of errorsin stored factual knowledge or the incorrect induction and obsolescence of certain facts over timemay be contributing factors to this limitation, which in turn affects the performance of LLMs (Elazaret al., 2021; Cao et al., 2021a). This limitation restricts the application of LLMs in some high-stakesareas, such as healthcare, finance, and law (Dong et al., 2022). Hence, exploring the degree to whichLLMs hold factual information and their ability to reason with such knowledge is vital.∗Equal Contribution.† Corresponding authors.1Published as a conference paper at ICLR 2024Figure 1: Pinocchio is a comprehensive dataset that tackles 7 distinct tasks related to factual knowl-edge and reasoning. It consists of 20,713 multiple-choice questions that have been sourced fromvarious reliable and diverse channels.To this end, we propose the Pinocchio, a testbed aimed at understanding factuality and reasoningfor LLMs. It contains 20K diverse factual questions that span different sources, timelines, domains,regions, and languages. Furthermore, we investigate whether LLMs are able to recognize thecombination of multiple facts, reason over structured and unstructured evidence, realize facts changeover time, identify subtle factual differences, and resist adversarial examples based on the dataset.We control for problem difficulty in each distinct reasoning task to enable fine-grained analysis.With the Pinocchio benchmark, we explore whether various LLMs (Scao et al., 2022b; Zhang et al.,2022; Ouyang et al., 2022; Chung et al., 2022; Touvron et al., 2023a; Chiang et al., 2023) couldstore factual knowledge and perform reasoning based on it. We envision Pinocchio as a suite ofbenchmarks, subsets of which could be separately utilized to assess certain model abilities of interestand analyze important strengths and limitations of LLMs. For instance, in temporal tasks, we find thatLLMs lack factual knowledge for up-to-date questions; in complex factual tasks that require multi-hopreasoning, LLMs still have limitations, even when various prompting strategies are employed. Wehope Pinocchio can serve as the initial step towards understanding the abilities of LLMs from multipledimensions and facilitate the development of LLMs.2DATASET CONSTRUCTION2.1TASKSAiming to systematically evaluate the factual knowledge and related reasoning abilities of LLMs,we raise seven research questions, then carefully select factual statements from different sourcessummarized in Table 1.• Task 1: Multifaceted Previous research (Petroni et al., 2019) has shown that small languagemodels like BERT have the ability to retain relational knowledge from training data and answer“fill-in-the-blank” cloze statements. This raises the question of whether LLMs can also storeand reason over multiple pieces of facts obtained during pretraining. It is not just importantfor LLMs to memorize individual facts accurately, but to also recognize and generate newcombinations of facts from different sources. To investigate this issue, we have selected claimsfrom the FEVER dataset (Thorne et al., 2018), which were written by human annotators basedon information from Wikipedia articles. These claims are either supported or refuted by multiplefacts from (the same or several) Wikipedia articles, or there is insufficient information availableto verify them. To assess the performance of language models in handling various combinationsof facts, we have sampled statements that require different numbers of evidence, ranging fromone to many, enabling fine-grained analysis.• Task 2: Structural In addition to unstructured text, factual knowledge is also commonly storedin a structured format, such as tables, lists, or databases (Bhagavatula et al., 2013). However,2Published as a conference paper at ICLR 2024Table 1: Pinocchio Dataset Sources, Descriptions, and Data Distribution.DomainDescriptionSourcesDistributionFact.Non-Fact.NEIALLMultifacetedContain multiple factsFEVER1,1111,1111,1103,332StructuralContain structured and unstructured factsFEVEROUS1,7411,9532503,944AdversarialContain facts edited by adversarial methodsSymmetric, FM2815921-1,736TemporalContain facts that change over timeVitaminC1,8981,0433553,296Real-WorldContain factual statements spread onlinePolitiFact9861,9876093,582Domain-SpecificContain facts from health and science domainsPubHealth, SciFact1,1567157372,608Multi-LingualContain facts in different languagesXFact, CHEF8208485472,215current LLMs are primarily trained on unstructured text using next word prediction loss (Brownet al., 2020; Touvron et al., 2023a). In order to process structured data, it is often convertedinto text strings using various methods, such as linearizing tables. This raises the question ofwhether LLMs are capable of effectively memorizing and reasoning over facts from structuredsources, similar to their performance with unstructured text. To investigate this question, wesample factual statements from the FEVEROUS dataset (Aly et al., 2021), which is constructedin a similar manner to FEVER but includes evidence in the form of tables, sentences, or both.• Task 3: Adversarial Language models are known to be vulnerable to adversarial examples thatare strategically modified to deceive even advanced models with hardly noticeable changes (Shenet al., 2023). Given this knowledge, it is important to examine whether LLMs can withstand ad-versarial examples in the context of factuality. To investigate this, we utilize two datasets, namelySymmetric (Schuster et al., 2019) and FM2 (Eisenschlos et al., 2021). These datasets consistof adversarial examples that have been crafted using various strategies, including temporalinference and diverting to unrelated facts.• Task 4: Temporal Facts are not static but rather possess a dynamic nature. With the vast amountof new information constantly emerging, facts often undergo changes, additions, or alterations.It raises the question of whether LLMs are able to adapt to these factual changes over time. Inparticular, we wonder if LLMs are capable of discerning factual knowledge from different timeperiods, since the pretraining corpus may not be processed and organized chronologically. Toexplore this, we utilize the VitaminC (Schuster et al., 2021) dataset, which consists of claimsbased on modifications made to factual content in Wikipedia articles. Claims can be eitherrefuted by outdated facts or supported by updated facts.• Task 5: Real-World In contrast to other tasks that assume Wikipedia has all the essential factualinformation, verifying viral claims on the internet often requires not only factual knowledgefrom various sources but also common sense and worldly knowledge. An important query wehave is whether LLMs can effectively integrate diverse types and sources of knowledge acquiredduring training. To address this, we select claims from the FactCheck (Misra, 2022) dataset,which consists of claims spread over the Internet and subsequently verified by journalists.• Task 6: Domain-Specific In addition to the tasks mentioned earlier, which primarily focus onfactual knowledge in general domains, we are also interested in exploring how LLMs possess thecapability to access domain-specific factual knowledge. The domain-specific setting presentsunique challenges. Take the science domain as an example, LLMs need to acquire backgroundknowledge, handle quantitative reasoning, and comprehend specialized statistical language. Toinvestigate this further, we sample claims from PubHealth (Kotonya & Toni, 2020) in the publichealth domain and SciFact (Wadden et al., 2022) in the science domain.• Task 7: Multi-Lingual Existing LLMs are mainly trained on English corpus because of theirabundance and quality (Chowdhery et al., 2022; Touvron et al., 2023a). However, the scarcity oftraining data in other languages raises the question of whether LLMs can transfer the factualknowledge acquired in English to other languages. To investigate this, we collected claims fromvarious languages including French, Chinese, and more, using the XFACT dataset (Gupta &Srikumar, 2021) and the CHEF dataset (Hu et al., 2022b) in a total of 27 different languages.2.2ANNOTATION AND QUALITY CONTROLMultiple-choice questions offer a practical approach to assess the complex capabilities of LLMs, ofwhich GPT-4 is a prime example (OpenAI, 2023). Key benchmarks such as MMLU (Hendryckset al., 2021b), HellaSwag (Zellers et al., 2019), ARC (Clark et al., 2018a), and TruthfulQA (Lin et al.,2022), all of which utilize multi-choice formats, serve distinct purposes in evaluating various aspectsof GPT-4’s proficiency. Specifically, the MMLU gauges an LLM’s knowledge breadth and depth.3Published as a conference paper at ICLR 2024Zero-shot\\xa0 \\xa0\\xa0You will be presented with a question.\\xa0 \\xa0 \\xa0 \\xa0 \\xa0\\xa0 \\xa0 You should answer \"Yes\", \"No\" or \"NotSure Enough,\" and provide supportingevidence for your answer.\\xa0 \\xa0 Q:\\xa0Has gas prices gone up 99 percentsince Obama became president, making itthe highest gas price increase since Carter?\\xa0 \\xa0 A:Few-shot with CoT\\xa0 \\xa0You will be presented with a question.\\xa0 \\xa0\\xa0You should answer \"Yes\", \"No\" or \"NotSure Enough,\" and provide supportingevidence for your answer.Here are some examples:\\xa0 \\xa0 Q:\\xa0\\xa0Is there a capital called Mogadishu?\\xa0 \\xa0 A:\\xa0Mogadishu is a city in East Africa,specifically in Somalia. Furthermore, thecapital of Somalia is exactly Mogadishu.Therefore, the answer is \"Yes\".\\xa0 \\xa0 Q :Has gas prices gone up 99 percentsince Obama became president, making itthe highest gas price increase since Carter?\\xa0 \\xa0 A:Few-shot\\xa0 \\xa0 You will be presented with a\\xa0question.\\xa0\\xa0 \\xa0 You should answer \"Yes\", \"No\" or\\xa0\"Not\\xa0Sure Enough\".Here are some examples:\\xa0 \\xa0 Q:Is it true that sixty two year oldWelsh journalist Jan Moir worked\\xa0for a\\xa0couple other papers before working\\xa0at\\xa0Daily Mail as an opinion columnist\\xa0and\\xa0has won several awards for her writing?\\xa0 \\xa0 A:\\xa0Yes.\\xa0 \\xa0 Q:\\xa0Has gas prices gone up 99 percent since\\xa0Obama became president, making it the\\xa0highest gas price\\xa0increase since Carter?\\xa0 \\xa0 A:Zero-shot with CoT\\xa0 \\xa0You will be presented with a question.\\xa0\\xa0 \\xa0You should answer \"Yes\", \"No\" or \"NotSure Enough,\" and provide supportingevidence for your answer.\\xa0 \\xa0 Q:\\xa0Has gas prices gone up 99 percentsince Obama became president, making itthe highest gas price increase since Carter?Let\\'s think step by step.\\xa0 \\xa0 A:Figure 2: Illustration of prompts using different settings.HellaSwag tests commonsense reasoning, and ARC focuses on challenging questions. TruthfulQAmeasures how LLMs mimic human falsehoods. Furthermore, the evaluation of language generationbrings its own set of challenges, as a universal metric for measurement is currently lacking (Sai et al.,2023), which multiple-choice questions help to mitigate by offering straightforward classificationaccuracy for assessment (Hendrycks et al., 2021b). Also, prior studies (Kadavath et al., 2022)underscore that LLMs demonstrate reliable calibration on multiple-choice scenarios. Therefore, wealso used the multi-choice questions as a simple but good proxy to evaluate the abilities of LLMs.For data annotation, we hired 10 undergraduate students, all with good English proficiency. We askedthe students to rewrite the original claims into questions without distorting factuality while providingfactuality labels for the questions. By transforming declarative statements into questions, using aQuestion-Answering approach can more effectively elicit factual knowledge from LLMs (Kadavathet al., 2022; Lin et al., 2022), and we also illustrate through experiments in Sec. 4.2. Note that claimsin the original datasets are usually labeled based on given evidence, e.g. evidence supports or refutesthe claim, but in Pinocchio, we only need to judge the factuality of the question. So we use unifiedlabels: Yes, No, Not Sure Enough. The three labels correspond respectively to Factual, Non-Factual,and Not Enough Information for factual questions. Considering that all fact-checking datasets use athree-label system (Guo et al., 2022), we did not modify the number of labels to maintain consistencyin labeling. When dealing with factuality questions in low-resource languages, for Chinese, the 5undergraduate students we hired are native Chinese speakers. For other low-resource languages,we first use Google Translate to translate them into English and generate factuality questions, thentranslate the English questions back to the corresponding languages. The label distribution is shownin Table 1. We paid the annotators accordingly based on the quantity and quality of the annotations.We ensure the quality of the annotated factuality questions in two ways. The two authors of thispaper served as meta-reviewers, sampling 10 questions from each of the three categories across theseven domains in Pinocchio. The meta-reviewers judged if the factuality labels were correct. Forthe 210 factuality questions, the average label accuracy was 92.4%. We divided the 10 students intotwo groups and had each group re-annotate a random 200 questions annotated by the other group,then calculated inter-annotator agreement (IAA). The final IAA was 85.6%. Based on meta-reviewerresults and IAA, the factuality labels in Pinocchio are of good quality.3METHODOLOGY3.1MODELSTo give a comprehensive view of the status of LLMs in a factual context, we evaluate 10 accessibleLLMs, undergone different training stages including pretraining, instruction tuning, and reinforcementlearning from human feedback (Ouyang et al., 2022), covering diverse organizations and varying insize. A detailed description can be found in Appendix A.2.3.2PROMPT STRATEGYAs illustrated in Figure 2, we employ 4 types of prompts to elicit desired responses from LLMs,namely: Zero-shot, Zero-shot with CoT (Kojima et al., 2022), Few-shot, and Few-shot with CoT (Weiet al., 2022). Specifically, we begin by providing the model with task instruction, denoted as Z: “You4Published as a conference paper at ICLR 2024Table 2: Results obtained using different forms of prompts on 10 accessible LLMs.MethodsZero-shot w/o CoTZero-shot w/ CoTFew-shot w/o CoTFew-shot w/ CoTOverall PerformanceAccuracyF1AccuracyF1AccuracyF1AccuracyF1AccuracyF1OPT-6.7B————36.927.937.928.518.814.3BLOOM-7B29.726.214.818.129.728.16.612.220.221.2LLaMA-7B31.829.622.324.936.828.635.331.431.628.6Alpaca-7B40.223.733.724.437.924.939.426.237.824.8Vicuna-7B33.233.634.232.935.534.848.540.637.934.9Vicuna-13B42.635.644.036.947.038.647.042.545.238.4ChatGLM-6B37.431.036.531.741.637.942.937.539.634.5Flan-T5-11B24.621.529.929.325.923.738.438.429.726.9Text-Davinci-00245.236.245.737.346.640.446.242.545.939.1Text-Davinci-00342.841.443.142.148.843.246.943.445.542.5GPT-3.5-Turbo46.944.346.844.447.244.747.145.747.044.8will be given a question. You should answer whether it is Yes, No, or Not Sure Enough and show yourevidence”. This instruction informs the LLMs about the expected input and output. Subsequently, forany given input Q, we anticipate obtaining an output label Y from the LLMs f: Y = f(Q, Z).Zero-Shot PromptIn the zero-shot setting, the LLMs are expected to provide answers based on theQuestion Q and the task instruction Z. We anticipate that the LLMs can directly generate the factualanswer “No” when presented with Q: “Has gas prices gone up 99 percent since Obama becamepresident, making it the highest gas price increase since Carter?” The zero-shot with CoT settingextends the question Q by adding a two-stage prompt (Kojima et al., 2022): “Let’s think step bystep”, designed to encourage the LLMs to contemplate the process of determining the factual label Y .Few-Shot PromptIn the few-shot setting, we employ three shots for model input (Q). Detailedexamples of the prompts in Figure 2 are presented in Appendix A.4. In the few-shot with CoTsetting, we provide potential reasoning instructions to the LLMs before presenting the factual label(Y ). As shown in Figure 2, for the Q: “Is there a capital called Mogadish?” Our reasoningapproach entails first explaining the noun phrase in the Q (the subject and object), and subsequentlyelaborating on modifying phrases such as predicates or adjectives. Regarding the subject “Mogadish”,we begin by furnishing a detailed definition: “Mogadishu is a city in East Africa, specifically inSomalia.” Following this, we proceed to reason about the relation between “Mogadish” and “capital”:“Furthermore, the capital of Somalia is indeed Mogadishu.” Consequently, we arrive at the ultimatefactual label: “Therefore, the answer is Yes.”4EXPERIMENTSIn an effort to take the initial step in understanding the capabilities of LLMs, we undertake acomprehensive analysis of various LLMs on Pinocchio, under different conditions and tasks.4.1MAIN RESULTSIn Table 2, we present the average results of 10 accessible LLMs operating under varying settings onPinocchio, run three times each. From Table 2, we draw the following conclusions:• Regarding overall performance, we observe that, on average, LLMs without instruction tuningunderperform those with instruction tuning by 16.0%. GPT family LLMs undergoing RLHFexhibit superior results, indicating that instruction tuning and RLHF optimize alignment withhuman knowledge, thereby improving factual question response accuracy.• Results obtained using the Few-shot setting significantly outperform those obtained when simplyasking factual questions to LLMs in the Zero-shot setting, especially for models without RLHF,exhibiting an average improvement of 7.3%. This highlights the capability of some sampleprompts to better extract the inherent factual knowledge of LLMs.• Using the CoT method, we observed a relative boost in performance in LLMs subjected toinstruction tuning and RLHF, improving by an average of 2.1%. Notably, the factual accuracy ofLLMs like OPT, BLOOM, and LLaMA was mostly stable or even decreased. A review of outputsfrom these untuned LLMs revealed that, post-CoT application, LLMs tend to produce related5Published as a conference paper at ICLR 2024Table 3: Results of different LLMs using Few-shot w/ CoT prompts across different tasks.TaskMultifacetedStructuralAdversarialTemporalReal-WorldDomain SpecificMulti-lingualAcc.F1Acc.F1Acc.F1Acc.F1Acc.F1Acc.F1Acc.F1OPT-6.7B34.524.145.530.951.851.730.018.053.727.528.228.316.217.7BLOOM-7B10.713.50.83.52.03.73.77.75.48.511.815.69.815.9LLaMA-7B38.333.944.132.143.246.141.630.026.426.323.625.027.827.7Alpaca-7B38.628.848.023.646.435.149.626.124.519.942.926.824.217.7Vicuna-7B44.236.049.736.359.059.250.137.649.041.844.338.646.743.1Vicuna-13B49.945.348.137.958.960.045.437.847.742.743.540.437.837.9ChatGLM-6B41.036.046.835.751.548.639.432.448.934.835.235.037.135.3Flan-T5-11B49.249.443.533.754.756.631.630.631.129.435.634.625.314.4Text-Davinci-00247.747.750.838.464.264.333.931.151.741.436.436.143.139.5Text-Davinci-00351.147.844.333.764.163.741.435.148.042.840.441.443.743.6GPT-3.5-Turbo53.653.144.837.867.467.437.433.950.443.138.740.341.341.1content considerations, and extensive considerations often overshadow factual discernment tasks,causing incorrect factual label outputs. In contrast, for instruction-tuned LLMs, the CoT methodfacilitates enhanced exploration of factual entity relations in questions, resulting in accuratefactual labels. See Appendix A.5 for detailed case analyses.• The OPT model, without being tuned to instructions, struggles significantly to output correctfactual labels under the settings of Zero-shot and Zero-shot CoT, often resulting in either arepetition of the original question or a refusal to output any content at all. This issue is somewhatalleviated under the settings of Few-shot and Few-shot CoT.• Additionally, we studied the hyperparameters of LLMs. Due to limited computing resources,we only explored Vicuna-7B and Vicuna-13B. We found that as model parameters increase,performance on factual questions improves correspondingly, with an average increase of 5.4%.This indicates that LLMs with more parameters can store more world knowledge and havestronger factual knowledge recognition capabilities.In Table 3, we present the factual performance of LLMs in various tasks under the Few-shot CoTsetting. This reveals the relative difficulty LLMs have in understanding and responding to factualquestions in different tasks, providing insights for future training of factual knowledge in LLMs.From Table 3, it is observed that LLMs exhibit relatively poorer performance on factual questionsrelated to the real-world, domain-specific knowledge, and multilingualism, being on average 6.4%lower compared to the other four tasks. This is attributed to the fact that the training data for LLMstypically come from general domains and are not up-to-date, which indirectly inspires the explorationof retrieval-augmented LLMs (Ram et al., 2023). We analyze the LLMs in different tasks in Sec. 4.2.4.2ANALYSISIn this section, we explore LLMs’ capabilities focusing on key areas like handling of multi-hop factualquestions, proficiency in diverse prompt strategies, and tackling challenges like numerical reasoningand entity ambiguity. We also examine their performance on time-sensitive factual questions, againstadversarial attacks, with fine-grained labels and prompts in multiple languages.1 Hop2 Hops3 HopsMultiHops 1 Hop  2 Hops 3 Hops 020040060080010001200Class Counts100020090200120025040202530354045505551.348.447.045.240.639.430.2MultifactedStructrualF1 of MultifactedF1 of Structrual(a) Multi-hop Reasoning AnalysisOnly SentencesOnly TablesCombine BothSentences and Tables800900100011001200130014001500160015951135121432343638404240.034.834.3Class CountsClass Performance(b) Structural Knowledge AnalysisEntityDisambiguationOtherMulti-hopReasoningCombiningTablesand Text Searchterms notin claimNumericalReasoning05001000150020009521806215377343820253035404550Macro F140.939.533.432.729.027.9Class CountsClass Performance(c) Challenges of Different QuestionsFigure 3: GPT-3.5-Turbo’s outcomes across three distinct tasks under Few-shot CoT setting.Multi-hop Factual Question AnalysisTo analyze the performance of LLMs when faced withfactual questions based on multiple pieces of facts that require complex logical reasoning, wecategorize multifaced and structural factual questions into distinct subsets, depending on the numberof “hops” necessary to validate each factual question. To maintain fairness, we randomly sampled1,490 data pieces from each of the two datasets for verification. Figure 3(a) illustrates the data6Published as a conference paper at ICLR 2024counts and Macro F1 scores of GPT-3.5-Turbo for each respective subset. The figure reveals a clearpattern: as the number of “hops” increases, the reasoning chain for deriving conclusions from existingfactual knowledge extends, necessitating heightened logical reasoning capabilities from the LLMs.Consequently, the performance of the LLMs exhibits diminishing trends.Structural Knowledge Analysis in LLMsTo investigate whether LLMs can effectively memorizefactual knowledge from structured data, we divided the structural task questions into three subsetsaccording to evidence distribution: evidence in unstructured data (Only text), structured data (Onlytables), or both (Combine text and tables). Figure 3(b) shows a notable decline (Avg. -5.5%) inGPT-3.5-Turbo’s performance when evidence involves structured data, indicating LLMs’ limitedability in extracting knowledge from structured tables. The LLMs also perform less effectively whenhandling questions requiring the combination of both evidence types, reflecting their incapacity tointegrate diverse structured evidence effectively.Analysis of Different Factual Questions Poses ChallengesTo assess the capabilities of LLMs inaddressing various challenges, we partitioned each factual question within the structural task into sixdistinct challenges: 1) Entity disambiguation, 2) Other, 3) Multi-hop reasoning, 4) Combining tablesand text, 5) Search terms not in claim, 6) Numerical reasoning, each centered around the most criticaldifficulty encountered during verification. Figure 3(c) illustrates GPT-3.5-Turbo’s performance anddata distribution across challenges. The extensive training and large-scale parameters enhance LLMs’performance in handling entity ambiguity. Longer reasoning chains and various forms of evidencechallenge LLMs’ factual abilities. When correct inference involves unmentioned entities, LLMsmay lack necessary hints from factual questions, posing significant challenges. LLMs also exhibitdeficiencies in precise numerical calculations due to the inherent hallucination phenomenon, resultingin subpar performance when numerical reasoning is needed for verification. Factual  Non-Factual  Not EnoughInformation FactualNon-FactualNot EnoughInformation10152025303540455055Macro F135.6033.9349.021.036.845.938.217.7Avg. F1 of Outdated DataAvg. F1 of Updated DataOutdated DataUpdated Data(a) Temporal Questions VerificationFactualNon-Factual Factual  Non-Factual  Factual  Non-Factual 50556065707580859081.886.183.775.263.563.681.886.183.775.283.9579.4563.55Avg. F1 of Synthetic dataAvg. F1 of Modified dataAvg. F1 of Man-made dataSynthetic dataModified dataMan-made data(b) Adversarial Attacks Resilience Factual MostlyFactual NEI MostlyFalse Non-Factual PantsFireFactualNon-FactualNEI010203040506024.812.72.816.043.016.538.166.225.119.3043.13Avg. F1 of Fine-grainedAvg. F1 of Coarse-grainedFine-grainedCoarse-grained(c) Label Granularity VariationsFigure 4: Results of GPT-3.5-Turbo in three different tasks under Few-shot CoT setting.Temporal AnalysisAs time progresses, the factuality of questions may undergo changes. This taskencompasses such data, and we leverage this task to explore the ability of LLMs to adapt to factualchanges. Figure 4(a) illustrates that GPT-3.5-Turbo exhibits a modest yet noticeable performancedifference when dealing with outdated data as compared to updated data. This discrepancy arises fromthe fact'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer(input_text, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "1dZE1RyhTeth"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids[0].input_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275
        },
        "id": "5gZR3s8BWihR",
        "outputId": "fea02023-5fb1-44f8-c41c-d3702cada1f2"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "'Invalid key. Only three types of key are available: (1) string, (2) integers for backend Encoding, and (3) slices for data subsetting.'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-2a62b6d671c9>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minput_ids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/tokenization_utils_base.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             raise KeyError(\n\u001b[0m\u001b[1;32m    278\u001b[0m                 \u001b[0;34m\"Invalid key. Only three types of key are available: \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m                 \u001b[0;34m\"(1) string, (2) integers for backend Encoding, and (3) slices for data subsetting.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: 'Invalid key. Only three types of key are available: (1) string, (2) integers for backend Encoding, and (3) slices for data subsetting.'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = [model.generate(input_ids, num_return_sequences=1, do_sample=True, top_k=50) for _ in range(5)]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuxVh3E_Sld1",
        "outputId": "0fbf95c0-56af-4461-ffc5-9eafb7a7ea00"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Input ids are automatically padded from 7134 to 7168 to be a multiple of `config.attention_window`: 1024\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMxuapEOThQe",
        "outputId": "00ca4b6d-00ae-4735-de0e-298e02290e91"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 7134])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ivh4m5yWT4D5",
        "outputId": "6f63dce0-fe1b-4e2a-deb3-c24f13441b53"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2cej9r5PT64u",
        "outputId": "d83b8113-b215-4fc7-94ce-781bdeda0337"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[tensor([[    2,   739,  2777,  3092,    36,  6006, 13123,  4839,    33,   682,\n",
              "           3185,  5690,   819,  4356,    12,  4892,   548,  2963,   420,    10,\n",
              "           1186,     9,  1632,  2777,  5774,  8558,   479,  1437, 50118,   959,\n",
              "           2156,  2210,   739,  2777,  3092,   202,  1762, 21833,  2655,     8,\n",
              "           6297,    31,  1337, 43310, 43879,   479,  1437, 50118,    52,   679,\n",
              "             42,    16,    10,  2008, 40750,    13, 18056, 23170, 17328,  7350,\n",
              "           2316,   479,  1437, 50118,    52,  4374,     7, 23242,  1688,     5,\n",
              "           5239,     8,  7401,     9, 21833,  2655,   624,   739,  2777,  3092,\n",
              "             30, 15293,     5, 31240,  6920, 12009,  1975, 27680,     4, 12009,\n",
              "           1975, 27680,  6308,   291,     6,   151,  5544, 21833,  1142,    14,\n",
              "           2292,   463, 47314,  1715,  2156, 31583,  2156, 30700,  2156,  3806,\n",
              "           2156,     8, 11991,   479,  1437, 50118,    52, 12406, 24343,   549,\n",
              "          30536, 13123,    64, 38003,  1533,  4905,  2156,  2935, 21833,   216,\n",
              "           1329,  6460,   991, 12150,  2368,  2156,  1219,    81,  1533,  3745,\n",
              "              9,  4905,  2156,  3058, 12405, 21833,  5550,  2156,     8, 11942,\n",
              "          37930, 27774,  7721,   479,  1437, 50118,  4935, 15491,    15,   430,\n",
              "          10070,     8,  3505,  1116,   739,  2777,  3092,   311,    14,  2210,\n",
              "            739,  2777,  3092,   202,  1762, 21833,  2655,     8,  6297,    31,\n",
              "          10806,  6514, 43310, 43879,   479,  1437, 50118,    52,   679,    42,\n",
              "             16,    10,  2008, 40750,    13, 18056, 23170, 17328,  7350,  2316,\n",
              "            479,  1437,     2]], device='cuda:0'),\n",
              " tensor([[    2,   739,  2777,  3092,    36,  6006, 13123,  4839,    33,  7977,\n",
              "           1538,  1632,  2777,  5774,    36,   282, 21992,  4839,    11,   485,\n",
              "          12857,   187,    51,    33,  3625,  2782,   819,    15,  1337, 18561,\n",
              "           8558,   479,  1437, 50118,   959,  2156,  2210,   739,  2777,  3092,\n",
              "            202,  1762, 21833,  2655,     8,  6297,    31,  1337, 43310, 43879,\n",
              "            479,  1437, 50118,    52,   679,    42,    16,    10,  2008, 40750,\n",
              "             13, 18056, 23170, 17328,  7350,  2316,   479,  1437, 50118,    52,\n",
              "           4374,     7, 23242,  1688,     5,  5239,     8,  7401,     9, 21833,\n",
              "           2655,   624,   739,  2777,  3092,    30, 15293,     5, 31240,  6920,\n",
              "          12009,  1975, 27680,     4, 12009,  1975, 27680,  6308,   291,   530,\n",
              "           5544, 21833,  1142,    14,  2292,   463, 47314,  1715,  2156, 31583,\n",
              "           2156, 30700,  2156,  3806,  2156,     8, 11991,   479,  1437, 50118,\n",
              "          43681,  2156,    52, 12406, 24343,   549, 30536, 13123,    64, 38003,\n",
              "           1533,  4905,  2156,  2935, 21833,   216,  1329,  6460,   991, 12150,\n",
              "           2368,  2156,  1219,    81,  1533,  3745,     9,  4905,  2156,  3058,\n",
              "          12405, 21833,  5550,  2156,   463, 11942, 37930, 27774,  7721,   479,\n",
              "           1437, 50118,  4935, 15491,    15,   430, 10070,     8,  3505,  1116,\n",
              "            739,  2777,  3092,   311,    14,  2210,   739,  2777,  3092,   202,\n",
              "           1762, 21833,  2655,     8,  6297,    31, 10806,  6514, 43310, 43879,\n",
              "            479,  1437, 50118,    52,   679,    42,    16,    10,  2008, 40750,\n",
              "             13, 18056, 23170, 17328,  7350,  2316,   479,  1437,     2]],\n",
              "        device='cuda:0'),\n",
              " tensor([[    2,   739,  2777,  3092,    36,  6006, 13123,  4839,    33,   682,\n",
              "           3185,  5690,   819,  4356,    12,  4892,   548,  2963,   420,    10,\n",
              "           1186,     9,  1632,  2777,  5774,  8558,   479,  1437, 50118,   959,\n",
              "           2156,  2210,   739,  2777,  3092,   202,  1762, 21833,  2655,     8,\n",
              "           6297,    31,  1337, 43310, 43879,   479,  1437, 50118,    52,   679,\n",
              "             42,    16,    10,  2008, 40750,    13, 18056, 23170, 17328,  7350,\n",
              "           2316,   479,  1437, 50118,    52,  4374,     7, 23242,  1688,     5,\n",
              "           5239,     8,  7401,     9, 21833,  2655,   624,   739,  2777,  3092,\n",
              "             30, 15293,     5, 31240,  6920, 12009,  1975, 27680,     4, 12009,\n",
              "           1975, 27680,  6308,   291,   530,  5544, 21833,  1142,    14,  2292,\n",
              "            463, 47314,  1715,  2156, 31583,  2156, 30700,  2156,  3806,  2156,\n",
              "              8, 11991,   479,  1437, 50118,    52, 12406, 24343,   549, 30536,\n",
              "          13123,    64, 38003,  1533,  4905,  2156,  2935, 21833,   216,  1329,\n",
              "           6460,   991, 12150,  2368,  2156,  1219,    81,  1533,  3745,     9,\n",
              "           4905,  2156,  3058, 12405, 21833,  5550,  2156,     8, 11942, 37930,\n",
              "          27774,  7721,   479,  1437, 50118,  4935, 15491,    15,   430, 10070,\n",
              "              8,  3505,  1116,   739,  2777,  3092,   311,    14,  2210,   739,\n",
              "           2777,  3092,   202,  1762, 21833,  2655,     8,  6297,    31, 10806,\n",
              "           6514, 43310, 43879,   479,  1437, 50118,    52,   679,    42,    16,\n",
              "             10,  2008, 40750,    13, 18056, 23170, 17328,  7350,  2316,   479,\n",
              "           1437,     2]], device='cuda:0'),\n",
              " tensor([[    2,   739,  2777,  3092,    36,  6006, 13123,  4839,    33,   682,\n",
              "           3185,  5690,   819,  4356,    12,  4892,   548,  2963,   420,    10,\n",
              "           1186,     9,  1632,  2777,  5774,  8558,   479,  1437, 50118,   959,\n",
              "           2156,  2210,   739,  2777,  3092,   202,  1762, 21833,  2655,     8,\n",
              "           6297,    31,  1337, 43310, 43879,   479,  1437, 50118,    52,   679,\n",
              "             42,    16,    10,  2008, 40750,    13, 18056, 23170, 17328,  7350,\n",
              "           2316,   479,  1437, 50118,    52,  4374,     7, 23242,  1688,     5,\n",
              "           5239,     8,  7401,     9, 21833,  2655,   624,   739,  2777,  3092,\n",
              "             30, 15293,     5, 31240,  6920, 12009,  1975, 27680,     4, 12009,\n",
              "           1975, 27680,  6308,   291,   530,  5544, 21833,  1142,    14,  2292,\n",
              "            463, 47314,  1715,  2156, 31583,  2156, 30700,  2156,  3806,  2156,\n",
              "              8, 11991,   479,  1437, 50118, 43681,  2156,    52, 12406, 24343,\n",
              "            549, 30536, 13123,    64, 38003,  1533,  4905,  2156,  2935, 21833,\n",
              "            216,  1329,  6460,   991, 12150,  2368,  2156,  1219,    81,  1533,\n",
              "           3745,     9,  4905,  2156,  3058, 12405, 21833,  5550,  2156,     8,\n",
              "          11942, 37930, 27774,  7721,   479,  1437, 50118,  4935, 15491,    15,\n",
              "            430, 10070,     8,  3505,  1116,   739,  2777,  3092,   311,    14,\n",
              "           2210,   739,  2777,  3092,   202,  1762, 21833,  2655,     8,  6297,\n",
              "             31, 10806,  6514, 43310, 43879,   479,  1437, 50118,    52,   679,\n",
              "             42,    16,    10,  2008, 40750,    13, 18056, 23170, 17328,  7350,\n",
              "           2316,   479,  1437,     2]], device='cuda:0'),\n",
              " tensor([[    2,   739,  2777,  3092,    36,  6006, 13123,  4839,    33,   682,\n",
              "           3185,  5690,   819,  4356,    12,  4892,   548,  2963,   420,    10,\n",
              "           1186,     9,  1632,  2777,  5774,  8558,   479,  1437, 50118,   959,\n",
              "           2156,  2210,   739,  2777,  3092,   202,  1762, 21833,  2655,     8,\n",
              "           6297,    31,  1337, 43310, 43879,   479,  1437, 50118,    52,   679,\n",
              "             42,    16,    10,  2008, 40750,    13, 18056, 23170, 17328,  7350,\n",
              "           2316,   479,  1437, 50118,     7,    42,   253,  2156,    52,  4374,\n",
              "              7, 23242,  1688,     5,  5239,     8,  7401,     9, 21833,  2655,\n",
              "            624, 30536, 13123,    30, 15293,     5, 31240,  6920, 12009,  1975,\n",
              "          27680,     4, 12009,  1975, 27680,  6308,   291,   530,  5544, 21833,\n",
              "           1142,    14,  2292,   463, 47314,  1715,  2156, 31583,  2156, 30700,\n",
              "           2156,  3806,  2156,     8, 11991,   479,  1437, 50118,    52, 12406,\n",
              "          24343,   549, 30536, 13123,    64, 38003,  1533,  4905,  2156,  2935,\n",
              "          21833,   216,  1329,  6460,   991, 12150,  2368,  2156,  1219,    81,\n",
              "           1533,  3745,     9,  4905,  2156,  3058, 12405, 21833,  5550,  2156,\n",
              "              8, 11942, 37930, 27774,  7721,   479,  1437, 50118,  4935, 15491,\n",
              "             15,   430, 10070,     8,  3505,  1116, 30536, 13123,   311,    14,\n",
              "           2210, 30536, 13123,   202,  1762, 21833,  2655,     8,  6297,    31,\n",
              "          10806,  6514, 43310, 43879,   479,  1437, 50118,    52,   679,    42,\n",
              "             16,    10,  2008, 40750,    13, 18056, 23170, 17328,  7350,  2316,\n",
              "            479,  1437,     2]], device='cuda:0')]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### decode ouputs\n",
        "decoded_outputs = [tokenizer.decode(output[0], skip_special_tokens=True) for output in outputs]"
      ],
      "metadata": {
        "id": "fhyfJaxYT7pb"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_outputs[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "id": "IWHvjRT-UGG-",
        "outputId": "a8b22ee2-844e-4db6-b27c-8971f4c6c440"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' large language models (LLMs ) have recently driven striking performance im-provements across a range of natural language processing tasks . \\n however , existing large language models still lack factual knowledge and suffer from various spurious correlations . \\n we believe this is a critical bottleneck for realizingtrustworthy artificial intelligence . \\n we aim toexplore the extent and scope of factual knowledge within large language models by designing thebenchmark Pinocchio. Pinocchio contains 20,000 diverse factual questions that spandifferent sources , timelines , domains , regions , and languages . \\n weinvestigate whether LLMs can compose multiple facts , update factual knowledgetemporally , reason over multiple pieces of facts , identify subtle factual differences , and resist adversarial examples . \\n extensive experiments on different sizes and typesof large language models show that existing large language models still lack factual knowledge and suffer fromvarious spurious correlations . \\n we believe this is a critical bottleneck for realizingtrustworthy artificial intelligence . '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(decoded_outputs[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NCIpejg4UIrD",
        "outputId": "14341624-74b6-4755-d6cf-5bf39de8fc0e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1105"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for output in decoded_outputs:\n",
        "    print(len(output))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SH3oPLoiUOWT",
        "outputId": "003eccba-bcd5-4028-baac-f13c8b682da6"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1105\n",
            "1156\n",
            "1102\n",
            "1116\n",
            "1065\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_outputs = pd.DataFrame()\n"
      ],
      "metadata": {
        "id": "YpKKQapQV2yX"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for article in df[\"article\"]:\n",
        "      input_ids = tokenizer(input_text, return_tensors=\"pt\", truncation=True, max_length=16384)\n",
        "\n",
        "      #cuda\n",
        "      if torch.cuda.is_available():\n",
        "        input_ids = input_ids.input_ids.to(\"cuda\")\n",
        "      else:\n",
        "        input_ids = input_ids.input_ids\n",
        "\n",
        "      outputs = [model.generate(input_ids, num_return_sequences=1, do_sample=True, top_k=50, max_length=512) for _ in range(5)]\n",
        "      decoded_outputs = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]\n",
        "\n",
        "      all_outputs = pd.concat([all_outputs, pd.DataFrame({\"article\": [article], \"output 1\": decoded_outputs[0], \"output 2\" : decoded_output[1], \"output 3\" : decoded_output[2], \"output 4\" : decoded_output[3], \"output 5\" : decoded_output[4],})])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "yg4APkkLVDbA",
        "outputId": "f46edff3-9992-480e-bba0-0db0641a9be5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-9e5b06c98d85>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_return_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m       \u001b[0mdecoded_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-9e5b06c98d85>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_return_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdo_sample\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m       \u001b[0mdecoded_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, **kwargs)\u001b[0m\n\u001b[1;32m   2139\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_encoder_decoder\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"encoder_outputs\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2140\u001b[0m             \u001b[0;31m# if model is encoder decoder encoder_outputs are created and added to `model_kwargs`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2141\u001b[0;31m             model_kwargs = self._prepare_encoder_decoder_kwargs_for_generation(\n\u001b[0m\u001b[1;32m   2142\u001b[0m                 \u001b[0minputs_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_input_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgeneration_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2143\u001b[0m             )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_prepare_encoder_decoder_kwargs_for_generation\u001b[0;34m(self, inputs_tensor, model_kwargs, model_input_name, generation_config)\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0mencoder_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"return_dict\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0mencoder_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmodel_input_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_tensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m         \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"encoder_outputs\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mModelOutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mencoder_kwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/led/modeling_led.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, global_attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1864\u001b[0m                     )\n\u001b[1;32m   1865\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1866\u001b[0;31m                     layer_outputs = encoder_layer(\n\u001b[0m\u001b[1;32m   1867\u001b[0m                         \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1868\u001b[0m                         \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/led/modeling_led.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[0m\n\u001b[1;32m    950\u001b[0m         \"\"\"\n\u001b[1;32m    951\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m         attn_outputs = self.self_attn(\n\u001b[0m\u001b[1;32m    953\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/led/modeling_led.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[0m\n\u001b[1;32m    759\u001b[0m         \u001b[0;34m\"\"\"Input shape: Batch x Time x Channel\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 761\u001b[0;31m         self_outputs = self.longformer_self_attn(\n\u001b[0m\u001b[1;32m    762\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/led/modeling_led.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, layer_head_mask, is_index_masked, is_index_global_attn, is_global_attn, output_attentions)\u001b[0m\n\u001b[1;32m    180\u001b[0m         \u001b[0mkey_vectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey_vectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_heads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         attn_scores = self._sliding_chunks_query_key_matmul(\n\u001b[0m\u001b[1;32m    183\u001b[0m             \u001b[0mquery_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_vectors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_sided_attn_window_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/led/modeling_led.py\u001b[0m in \u001b[0;36m_sliding_chunks_query_key_matmul\u001b[0;34m(self, query, key, window_overlap)\u001b[0m\n\u001b[1;32m    452\u001b[0m         \u001b[0;31m# bcyd: batch_size * num_heads x chunks x 2window_overlap x head_dim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    453\u001b[0m         \u001b[0;31m# bcxy: batch_size * num_heads x chunks x 2window_overlap x 2window_overlap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 454\u001b[0;31m         \u001b[0mdiagonal_chunked_attention_scores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"bcxd,bcyd->bcxy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# multiply\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m         \u001b[0;31m# convert diagonals into columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    400\u001b[0m         \u001b[0;31m# recurse incase operands contains value that has torch function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    401\u001b[0m         \u001b[0;31m# in the original implementation this line is omitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 402\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_operands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    403\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    404\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mopt_einsum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/functional.py\u001b[0m in \u001b[0;36meinsum\u001b[0;34m(*args)\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;31m# the path for contracting 0 or 1 time(s) is already optimized\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    406\u001b[0m         \u001b[0;31m# or the user has disabled using opt_einsum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 407\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_VF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meinsum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mequation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperands\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m     \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_outputs = pd.DataFrame(columns=[\"article\", \"output 1\", \"output 2\", \"output 3\", \"output 4\", \"output 5\"])"
      ],
      "metadata": {
        "id": "UaWq2PQVWNPt"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set device\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q86KGwX1WOZ8",
        "outputId": "a07c5d4d-a099-4185-8650-b76b52069f11"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "qDAUziVBWRo0"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process each article\n",
        "for article in tqdm(df[\"article\"], desc=\"Processing articles\"):\n",
        "    # Tokenize input\n",
        "    inputs = tokenizer(article, return_tensors=\"pt\", truncation=True, max_length=16384)\n",
        "\n",
        "    # Move input to appropriate device\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    print(inputs)\n",
        "\n",
        "    # Generate multiple outputs\n",
        "    outputs_list = []\n",
        "    for _ in range(5):\n",
        "        output = model.generate(\n",
        "            inputs[\"input_ids\"],\n",
        "            num_return_sequences=1,\n",
        "            do_sample=True,\n",
        "            top_k=50,\n",
        "            max_length=512\n",
        "        )\n",
        "        outputs_list.append(output)\n",
        "\n",
        "    # Decode outputs\n",
        "    decoded_outputs = []\n",
        "    for output in outputs_list:\n",
        "        decoded = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "        decoded_outputs.append(decoded)\n",
        "\n",
        "    # Create result row\n",
        "    result_row = {\n",
        "        \"article\": article,\n",
        "        \"output 1\": decoded_outputs[0],\n",
        "        \"output 2\": decoded_outputs[1],\n",
        "        \"output 3\": decoded_outputs[2],\n",
        "        \"output 4\": decoded_outputs[3],\n",
        "        \"output 5\": decoded_outputs[4]\n",
        "    }\n",
        "\n",
        "    # Append to results\n",
        "    all_outputs = pd.concat([all_outputs, pd.DataFrame([result_row])], ignore_index=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426,
          "referenced_widgets": [
            "be38d68fe0e247469f2eba8c55208d9f",
            "6750773b7cae45fda891b22c1ea1af2f",
            "e5589abffdcc4698ae27767dcf61bdc1",
            "d22a4098135f43fdb0ef88e2fe53ab91",
            "4839e45b80ca40ff8e43d62fa76b7de6",
            "f554c61cfde548bfa0b0869c1d6dce1d",
            "918da3c0ae2e4128b098f953f20d9674",
            "76a76c53fc1d4d0fada5729246f21e98",
            "f60e22ff1f8443f5a9f25395e6987eb3",
            "5df7bd9ef94049229d303bf5d4cdb002",
            "e5f35d220b424da09fc0817361d427f8"
          ]
        },
        "id": "lWjT7MxQWILh",
        "outputId": "dcdefe0a-194c-4610-bdd8-4d3506ccbd60"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing articles:   0%|          | 0/11 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "be38d68fe0e247469f2eba8c55208d9f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[    0, 29642,    25,  ...,   627,   754,     2]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Input ids are automatically padded from 8532 to 9216 to be a multiple of `config.attention_window`: 1024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[    0, 29642,    25,  ...,  6945,   102,     2]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Input ids are automatically padded from 7573 to 8192 to be a multiple of `config.attention_window`: 1024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[    0, 17245,  1551,  ..., 33187, 22094,     2]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Input ids are automatically padded from 7310 to 8192 to be a multiple of `config.attention_window`: 1024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[    0, 29642,    25,  ...,  9064, 13709,     2]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Input ids are automatically padded from 7238 to 8192 to be a multiple of `config.attention_window`: 1024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[    0, 29642,    25,  ..., 39236,     8,     2]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Input ids are automatically padded from 6067 to 6144 to be a multiple of `config.attention_window`: 1024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[   0,  406,    4,  ...,    4, 1570,    2]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Input ids are automatically padded from 8243 to 9216 to be a multiple of `config.attention_window`: 1024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[    0, 29642,    25,  ...,  4285,   338,     2]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Input ids are automatically padded from 6567 to 7168 to be a multiple of `config.attention_window`: 1024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[    0, 29642,    25,  ..., 15581,  2126,     2]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Input ids are automatically padded from 3135 to 4096 to be a multiple of `config.attention_window`: 1024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[    0, 29642,    25,  ...,  4726,   139,     2]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Input ids are automatically padded from 3767 to 4096 to be a multiple of `config.attention_window`: 1024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[   0,  104, 4014,  ...,    4, 1978,    2]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Input ids are automatically padded from 7405 to 8192 to be a multiple of `config.attention_window`: 1024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'input_ids': tensor([[    0, 33479, 34040,  ...,     8,  6056,     2]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]], device='cuda:0')}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_outputs.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "id": "VlISfOL3X4eH",
        "outputId": "6117e444-9148-4f2a-c82a-534bf99e7c5c"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                             article  \\\n",
              "0  Published as a conference paper at ICLR 2024TO...   \n",
              "1  Published as a conference paper at ICLR 2024MA...   \n",
              "2  Under review as a conference paper at ICLR 202...   \n",
              "3  Published as a conference paper at ICLR 2024TO...   \n",
              "4  Published as a conference paper at ICLR 2024IN...   \n",
              "\n",
              "                                            output 1  \\\n",
              "0   large language models (LLMs ) have recently d...   \n",
              "1   we present a specifically pre-trained languag...   \n",
              "2   instruction fine-tuning has recently emerged ...   \n",
              "3   despite the advancements of open-source large...   \n",
              "4   instruction-tuned language models have demons...   \n",
              "\n",
              "                                            output 2  \\\n",
              "0   large language models (LLMs ) have revolution...   \n",
              "1   we present a specifically pre-trained languag...   \n",
              "2   instruction fine-tuning has recently emerged ...   \n",
              "3   despite the advancements of open-source large...   \n",
              "4   instruction-tuned language models have demons...   \n",
              "\n",
              "                                            output 3  \\\n",
              "0   large language models (LLMs ) have recently d...   \n",
              "1   we present TP-BERTa , a specifically pre-trai...   \n",
              "2   instruction fine-tuning has recently emerged ...   \n",
              "3   despite the advancements of open-source large...   \n",
              "4   instruction-tuned language models have demons...   \n",
              "\n",
              "                                            output 4  \\\n",
              "0   large language models (LLMs ) have recently d...   \n",
              "1   we present a specifically pre-trained languag...   \n",
              "2   instruction fine-tuning has recently emerged ...   \n",
              "3   despite the advancements of open-source large...   \n",
              "4   instruction-tuned language models have demons...   \n",
              "\n",
              "                                            output 5  \n",
              "0   large language models (LLMs ) have recently d...  \n",
              "1   we present a specifically pre-trained languag...  \n",
              "2   instruction fine-tuning has recently emerged ...  \n",
              "3   despite the advancements of open-source large...  \n",
              "4   instruction-tuned language models have demons...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-8c7f1a39-96db-4383-86f0-3c6a4a5bb8a0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article</th>\n",
              "      <th>output 1</th>\n",
              "      <th>output 2</th>\n",
              "      <th>output 3</th>\n",
              "      <th>output 4</th>\n",
              "      <th>output 5</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Published as a conference paper at ICLR 2024TO...</td>\n",
              "      <td>large language models (LLMs ) have recently d...</td>\n",
              "      <td>large language models (LLMs ) have revolution...</td>\n",
              "      <td>large language models (LLMs ) have recently d...</td>\n",
              "      <td>large language models (LLMs ) have recently d...</td>\n",
              "      <td>large language models (LLMs ) have recently d...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Published as a conference paper at ICLR 2024MA...</td>\n",
              "      <td>we present a specifically pre-trained languag...</td>\n",
              "      <td>we present a specifically pre-trained languag...</td>\n",
              "      <td>we present TP-BERTa , a specifically pre-trai...</td>\n",
              "      <td>we present a specifically pre-trained languag...</td>\n",
              "      <td>we present a specifically pre-trained languag...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Under review as a conference paper at ICLR 202...</td>\n",
              "      <td>instruction fine-tuning has recently emerged ...</td>\n",
              "      <td>instruction fine-tuning has recently emerged ...</td>\n",
              "      <td>instruction fine-tuning has recently emerged ...</td>\n",
              "      <td>instruction fine-tuning has recently emerged ...</td>\n",
              "      <td>instruction fine-tuning has recently emerged ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Published as a conference paper at ICLR 2024TO...</td>\n",
              "      <td>despite the advancements of open-source large...</td>\n",
              "      <td>despite the advancements of open-source large...</td>\n",
              "      <td>despite the advancements of open-source large...</td>\n",
              "      <td>despite the advancements of open-source large...</td>\n",
              "      <td>despite the advancements of open-source large...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Published as a conference paper at ICLR 2024IN...</td>\n",
              "      <td>instruction-tuned language models have demons...</td>\n",
              "      <td>instruction-tuned language models have demons...</td>\n",
              "      <td>instruction-tuned language models have demons...</td>\n",
              "      <td>instruction-tuned language models have demons...</td>\n",
              "      <td>instruction-tuned language models have demons...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8c7f1a39-96db-4383-86f0-3c6a4a5bb8a0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-8c7f1a39-96db-4383-86f0-3c6a4a5bb8a0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-8c7f1a39-96db-4383-86f0-3c6a4a5bb8a0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-6f3734d0-5662-4b12-94da-59112fd792a3\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-6f3734d0-5662-4b12-94da-59112fd792a3')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-6f3734d0-5662-4b12-94da-59112fd792a3 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "all_outputs",
              "summary": "{\n  \"name\": \"all_outputs\",\n  \"rows\": 11,\n  \"fields\": [\n    {\n      \"column\": \"article\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 11,\n        \"samples\": [\n          \"7.3Figure 5: Ablation study of our method design.Figure 6: Performance with respect to thenumber of in-context examples (k).out to some extent when the LMs are fine-tuned, which may explain the relatively small gains in thisevaluation compared to our other experiments.4ANALYSIS4.1EVOLUTION OF PERFORMANCE DURING PRETRAININGThroughout the pretraining process, we closely monitor both the training loss and the downstreamtask performance for the ICLM as well as the standard LM. Figure 4 illustrates the trajectory of thetraining loss and the performance on the RACE reading comprehension tasks for the 7B models.The training loss for ICLM consistently remains lower than that of the standard LM. This suggeststhat, when predicting the next token, ICLM benefits from a richer set of relevant prior documents torefer to, while the standard LM has limited information to rely on, leading to higher loss. Figure 4(b, c) shows that after training on around 150 billion tokens, ICLM is consistently better than thestandard LM on reading comprehension tasks. This performance gap remains consistent throughoutthe remainder of the pretraining phase. This suggests the scale of improvements by IN-CONTEXTPRETRAINING does not diminish and remains consistent as training on more tokens.8Published as a conference paper at ICLR 20244.2ABLATION STUDY ON IN-CONTEXT PRETRAINING DESIGNWe perform analysis on two design choices of IN-CONTEXT PRETRAINING: a choice of methods forfinding retrieved documents and deduplication. Ablations are done with 1.5B models and evaluatedwith perplexity on Wikipedia. The results are presented in Figure 5.Document relevance.A key design of IN-CONTEXT PRETRAINING is grouping documents bytheir relevance. We consider three levels of relevance: random (the standard baseline discussed in\\u00a73.2), clustering, and our document linking method in IN-CONTEXT PRETRAINING. Clusteringfollows the method from Abbas et al. (2023) in clustering documents into 11k clusters based on theirembeddings and sample documents from each cluster to form the training inputs. Documents groupedby clustering are sourced from the same clusters, indicating topical similarity but not necessarilyclose relation. In contrast, ICLM links documents as nearest neighbors, indicating a higher degreeof similarity. The relevance between documents increases from random, clustering to linking. Weobserve that the perplexity of the language model decreases as the relevance increases.Deduplication.We compare perplexity of the models trained with and without the semanticdeduplication step. Removing the semantic deduplication step leads to a significant decrease inperplexity. When near duplicate documents are present in the same context, language models mightmerely copy from the prior document, leading to training instability.4.3DEMONSTRATION EXAMPLES SIZE FOR IN-CONTEXT LEARNINGWe evaluate the 7B models trained with the standard method and IN-CONTEXT PRETRAINING, usinga varying number of demonstration examples on text classification tasks described in \\u00a73.3.2. Asdepicted in Figure 6, ICLM maintains consistent performance gains over the standard method, evenas the number of demonstration examples grows. While the performance improves as the number ofdemonstration examples increases, it plateaus after 32 examples.5RELATED WORKData batching based on similarityPrevious work employs batching lexically similar segments inthe same training batches to construct high-quality positive pairs for training retrieval-augmentedlanguage models. For instance, Zhong et al. (2022) use BM25 and same documents to ensure thesegments in the same batch are similar to each other, while Min et al. (2023) group segments fromthe same documents in the same batch. Our method shares the same spirit with these methods exceptwe maintain the relevance of documents in the same context window, yet context windows withinbatches are shuffled. Additionally, our focus is to apply the batching method to train the standardlanguage models.6CONCLUSIONWe introduce IN-CONTEXT PRETRAINING, a new pretraining method that learns to generate textconditioned on a set of relevant documents, exposing LMs to relevant contexts and providing trainingsignals beyond document boundaries. Our method is highly scalable and simple, and works withany pre-training pipeline by simply changing the document ordering during preprocessing. Ourcomprehensive evaluation demonstrates our method leads to significant improvements in a widevariety of settings that highlight the ability to understand and reason over the given context, includingin-context learning, reading comprehension, retrieval augmentation, and more. Future research maydelve into the inherent connections between documents within specific corpus domains or usingmultilingual retriever to group related multilingual documents in the same context. For example, thecode scripts within the same repository are related. This insight paves the way for future exploration,where concatenating entire repositories into a unified whole could lead to the creation of meaningfullong-context data sets.9Published as a conference paper at ICLR 2024REFERENCESAmro Abbas, Kushal Tirumala, D\\u00e1niel Simig, Surya Ganguli, and Ari S Morcos. Semdedup: Data-efficient learning at web-scale through semantic deduplication. arXiv preprint arXiv:2303.09540,2023.Francesco Barbieri, Jose Camacho-Collados, Luis Espinosa Anke, and Leonardo Neves. TweetEval:Unified benchmark and comparative evaluation for tweet classification. In Findings of the Asso-ciation for Computational Linguistics: EMNLP 2020, pp. 1644\\u20131650, Online, November 2020.Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.148. URLhttps://aclanthology.org/2020.findings-emnlp.148.Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, ArielHerbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler,Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray,Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever,and Dario Amodei. Language models are few-shot learners. In H. Larochelle, M. Ranzato,R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems,volume 33, pp. 1877\\u20131901. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf.Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and KristinaToutanova. BoolQ: Exploring the surprising difficulty of natural yes/no questions. In Proceedingsof the 2019 Conference of the North American Chapter of the Association for ComputationalLinguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 2924\\u20132936,Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1300. URL https://aclanthology.org/N19-1300.Tri Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, and Christopher R\\u00e9. FlashAttention: Fast andmemory-efficient exact attention with IO-awareness. In Advances in Neural Information ProcessingSystems, 2022.Harm de Vries. In the long (context) run, 2023. URL https://www.harmdevries.com/post/context-length/.Matthijs Douze, Alexandr Guzhva, Chengqi Deng, Jeff Johnson, Gergely Szilvasy, Pierre-EmmanuelMazar\\u00e9, Maria Lomeli, Lucas Hosseini, and Herv\\u00e9 J\\u00e9gou. The faiss library, 2024.Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner.DROP: A reading comprehension benchmark requiring discrete reasoning over paragraphs. InProceedings of the 2019 Conference of the North American Chapter of the Association for Com-putational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp.2368\\u20132378, Minneapolis, Minnesota, June 2019. Association for Computational Linguistics. doi:10.18653/v1/N19-1246. URL https://aclanthology.org/N19-1246.Avia Efrat and Omer Levy. The turking test: Can language models understand instructions? ArXiv,abs/2010.11982, 2020. URL https://api.semanticscholar.org/CorpusID:225062157.Merrill M Flood. The traveling-salesman problem. Operations research, 4(1):61\\u201375, 1956.Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Mingwei Chang. Retrieval augmentedlanguage model pre-training. In International conference on machine learning, pp. 3929\\u20133938.PMLR, 2020.Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian Riedel, Piotr Bojanowski, ArmandJoulin, and Edouard Grave. Unsupervised dense information retrieval with contrastive learning.Transactions on Machine Learning Research, 2022. URL https://openreview.net/forum?id=jKN1pXi7b0.Jeff Johnson, Matthijs Douze, and Herv\\u00e9 J\\u00e9gou. Billion-scale similarity search with gpus. IEEETransactions on Big Data, 7(3):535\\u2013547, 2019.10Published as a conference paper at ICLR 2024Mandar Joshi, Eunsol Choi, Daniel Weld, and Luke Zettlemoyer. TriviaQA: A large scale distantlysupervised challenge dataset for reading comprehension. In Proceedings of the 55th AnnualMeeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1601\\u20131611, Vancouver, Canada, July 2017. Association for Computational Linguistics. doi: 10.18653/v1/P17-1147. URL https://aclanthology.org/P17-1147.Herv\\u00e9 J\\u00e9gou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighborsearch. IEEE transactions on pattern analysis and machine intelligence, 33:117\\u201328, 01 2011. doi:10.1109/TPAMI.2010.57.Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield, Michael Collins, Ankur Parikh, ChrisAlberti, Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton Lee, Kristina Toutanova, LlionJones, Matthew Kelcey, Ming-Wei Chang, Andrew M. Dai, Jakob Uszkoreit, Quoc Le, and SlavPetrov. Natural questions: A benchmark for question answering research. Transactions of theAssociation for Computational Linguistics, 7:452\\u2013466, 2019. doi: 10.1162/tacl_a_00276. URLhttps://aclanthology.org/Q19-1026.Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard Hovy. RACE: Large-scale ReAdingcomprehension dataset from examinations. In Proceedings of the 2017 Conference on EmpiricalMethods in Natural Language Processing, pp. 785\\u2013794, Copenhagen, Denmark, September2017. Association for Computational Linguistics. doi: 10.18653/v1/D17-1082. URL https://aclanthology.org/D17-1082.Jens Lehmann, Robert Isele, Max Jakob, Anja Jentzsch, Dimitris Kontokostas, Pablo N Mendes,Sebastian Hellmann, Mohamed Morsey, Patrick Van Kleef, S\\u00f6ren Auer, et al. Dbpedia\\u2013a large-scale, multilingual knowledge base extracted from wikipedia. Semantic web, 6(2):167\\u2013195, 2015.Yoav Levine, Noam Wies, Daniel Jannai, Dan Navon, Yedid Hoshen, and Amnon Shashua. Theinductive bias of in-context learning: Rethinking pretraining example design. In InternationalConference on Learning Representations, 2022. URL https://openreview.net/forum?id=lnEaqbTJIRz.Xi Victoria Lin, Xilun Chen, Mingda Chen, Weijia Shi, Maria Lomeli, Rich James, Pedro Rodriguez,Jacob Kahn, Gergely Szilvasy, Mike Lewis, Luke Zettlemoyer, and Scott Yih. Ra-dit: Retrieval-augmented dual instruction tuning, 2023.Alisa Liu and Jiacheng Liu. The memotrap dataset. https://github.com/inverse-scaling/prize/blob/main/data-release/README.md, 2023.Nelson F. Liu, Kevin Lin, John Hewitt, Ashwin Paranjape, Michele Bevilacqua, Fabio Petroni, andPercy Liang. Lost in the middle: How language models use long contexts, 2023.Shayne Longpre, Kartik Perisetla, Anthony Chen, Nikhil Ramesh, Chris DuBois, and Sameer Singh.Entity-based knowledge conflicts in question answering. In Proceedings of the 2021 Conferenceon Empirical Methods in Natural Language Processing, pp. 7052\\u20137063, Online and Punta Cana,Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.565. URL https://aclanthology.org/2021.emnlp-main.565.Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Confer-ence on Learning Representations, 2018.Ian R. McKenzie, Alexander Lyzhov, Michael Pieler, Alicia Parrish, Aaron Mueller, Ameya Prabhu,Euan McLean, Aaron Kirtland, Alexis Ross, Alisa Liu, Andrew Gritsevskiy, Daniel Wurgaft, DerikKauffman, Gabriel Recchia, Jiacheng Liu, Joe Cavanagh, Max Weiss, Sicong Huang, The FloatingDroid, Tom Tseng, Tomasz Korbak, Xudong Shen, Yuhui Zhang, Zhengping Zhou, Najoung Kim,Samuel R. Bowman, and Ethan Perez. Inverse scaling: When bigger isn\\u2019t better, 2023.Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. MetaICL: Learning to learn incontext. In Proceedings of the 2022 Conference of the North American Chapter of the Associationfor Computational Linguistics: Human Language Technologies, pp. 2791\\u20132809, Seattle, UnitedStates, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.naacl-main.201. URL https://aclanthology.org/2022.naacl-main.201.11Published as a conference paper at ICLR 2024Sewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wen-tau Yih, Hannaneh Hajishirzi, andLuke Zettlemoyer. Nonparametric masked language modeling. In Findings of the Associa-tion for Computational Linguistics: ACL 2023, pp. 2097\\u20132118, Toronto, Canada, July 2023.Association for Computational Linguistics.doi: 10.18653/v1/2023.findings-acl.132.URLhttps://aclanthology.org/2023.findings-acl.132.OpenAI. Gpt-4 technical report, 2023.Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, ChongZhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton,Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, andRyan Lowe. Training language models to follow instructions with human feedback, 2022.Pranav Rajpurkar, Jian Zhang, Konstantin Lopyrev, and Percy Liang. Squad: 100,000+ questions formachine comprehension of text. In Proceedings of the 2016 Conference on Empirical Methods inNatural Language Processing, pp. 2383\\u20132392, 2016.Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ili\\u00b4c, Daniel Hesslow, RomanCastagn\\u00e9, Alexandra Sasha Luccioni, Fran\\u00e7ois Yvon, Matthias Gall\\u00e9, et al. Bloom: A 176b-parameter open-access multilingual language model. arXiv preprint arXiv:2211.05100, 2022.Uri Shaham, Elad Segal, Maor Ivgi, Avia Efrat, Ori Yoran, Adi Haviv, Ankit Gupta, Wenhan Xiong,Mor Geva, Jonathan Berant, and Omer Levy. SCROLLS: Standardized CompaRison over longlanguage sequences. In Proceedings of the 2022 Conference on Empirical Methods in Natural Lan-guage Processing, pp. 12007\\u201312021, Abu Dhabi, United Arab Emirates, December 2022. Associa-tion for Computational Linguistics. URL https://aclanthology.org/2022.emnlp-main.823.Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed H Chi, NathanaelSch\\u00e4rli, and Denny Zhou. Large language models can be easily distracted by irrelevant context. InInternational Conference on Machine Learning, pp. 31210\\u201331227. PMLR, 2023a.Weijia Shi, Xiaochuang Han, Mike Lewis, Yulia Tsvetkov, Luke Zettlemoyer, and Scott Wen tau Yih.Trusting your evidence: Hallucinate less with context-aware decoding, 2023b.Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon Seo, Rich James, Mike Lewis, Luke Zettle-moyer, and Wen-tau Yih. Replug: Retrieval-augmented black-box language models. arXiv preprintarXiv:2301.12652, 2023c.Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, andChristopher Potts. Recursive deep models for semantic compositionality over a sentiment treebank.In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pp.1631\\u20131642, Seattle, Washington, USA, October 2013. Association for Computational Linguistics.URL https://www.aclweb.org/anthology/D13-1170.Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen tau Yih,Noah A. Smith, Luke Zettlemoyer, and Tao Yu. One embedder, any task: Instruction-finetunedtext embeddings, 2023.Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\\u00e9eLacroix, Baptiste Rozi\\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open andefficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a.Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, NikolayBashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cris-tian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu,Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn,Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, IsabelKloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee,Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra,Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi,Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, BinhTang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen12Published as a conference paper at ICLR 2024Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic,Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models,2023b.Yike Wang, Shangbin Feng, Heng Wang, Weijia Shi, Vidhisha Balachandran, Tianxing He, andYulia Tsvetkov.Resolving knowledge conflicts in large language models.arXiv preprintarXiv:2310.00935, 2023.Guillaume Wenzek, Marie-Anne Lachaux, Alexis Conneau, Vishrav Chaudhary, Francisco Guzm\\u00e1n,Armand Joulin, and \\u00c9douard Grave. Ccnet: Extracting high quality monolingual datasets fromweb crawl data. In Proceedings of the Twelfth Language Resources and Evaluation Conference,pp. 4003\\u20134012, 2020.Fangyuan Xu, Weijia Shi, and Eunsol Choi. Recomp: Improving retrieval-augmented lms withcompression and selective augmentation, 2023.Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William Cohen, Ruslan Salakhutdinov,and Christopher D. Manning. HotpotQA: A dataset for diverse, explainable multi-hop questionanswering. In Proceedings of the 2018 Conference on Empirical Methods in Natural LanguageProcessing, pp. 2369\\u20132380, Brussels, Belgium, October-November 2018. Association for Compu-tational Linguistics. doi: 10.18653/v1/D18-1259. URL https://aclanthology.org/D18-1259.Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Richard James, Jure Leskovec, Percy Liang,Mike Lewis, Luke Zettlemoyer, and Wen-tau Yih. Retrieval-augmented multimodal languagemodeling. 2023.Susan Zhang, Mona Diab, and Luke Zettlemoyer. Democratizing access to large-scale languagemodels with opt-175b. Meta AI, 2022.Xiang Zhang, Junbo Zhao, and Yann LeCun.Character-level convolutional networks fortext classification.In C. Cortes, N. Lawrence, D. Lee, M. Sugiyama, and R. Garnett(eds.), Advances in Neural Information Processing Systems, volume 28. Curran Associates,Inc., 2015a.URL https://proceedings.neurips.cc/paper_files/paper/2015/file/250cf8b51c773f3f8dc8b4be867a9a02-Paper.pdf.Xiang Zhang, Junbo Jake Zhao, and Yann LeCun. Character-level convolutional networks for textclassification. In NIPS, 2015b.Zhuosheng Zhang, Hai Zhao, and Rui Wang. Machine reading comprehension: The role of contextu-alized language models and beyond. arXiv preprint arXiv:2005.06249, 2020.Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. Calibrate before use: Improvingfew-shot performance of language models. In International Conference on Machine Learning, pp.12697\\u201312706. PMLR, 2021.Zexuan Zhong, Tao Lei, and Danqi Chen. Training language models with memory augmentation. InProceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp.5657\\u20135673, Abu Dhabi, United Arab Emirates, December 2022. Association for ComputationalLinguistics. doi: 10.18653/v1/2022.emnlp-main.382. URL https://aclanthology.org/2022.emnlp-main.382.Wenxuan Zhou, Sheng Zhang, Hoifung Poon, and Muhao Chen. Context-faithful prompting for largelanguage models. ArXiv, abs/2303.11315, 2023.13Published as a conference paper at ICLR 2024AADDITIONAL BACKGROUNDA.1DEDUPLICATIONCorpora often have semantic duplicates: pairs of documents that are semantically related, yet notentirely identical. Previous studies (Yasunaga et al., 2023) show that retraining highly similardocuments in the input contexts during training hurts multimodal models\\u2019 performance. We observeda similar behaviur: when near duplicate documents are present in the same context, language modelsmight merely copy from the prior document, leading to training instability. Given that our retrievalapproach inherently assesses pairwise document similarity, we can easily filter out near duplicatedocuments that have high cosine similarity with other documents. We find this deduplication step tobe crucial for achieving performance of good language models (\\u00a74.2).A.2FAISS INDEXWe used a product quantised inverted file (IVFPQ) FAISS index with a code size of 256 and thecorresponding number of inverted lists 32768 , with total size of 62 gigabytes. The index contains235266464 768-dimensional embeddings originally in float 32. The index was trained on a sampleof 1572864 embeddings and the train time was 423 s. Successively, the data is split in batches of50M embeddings and for each index shard the corresponding batch of embeddings is added to thetrained index, the average adding embeddings time per index shard is 628.4 s. Finally, approximatenearest neighbor search is conducted per each shard before aggregating all results using faiss bigbatch search. The nprobe used for conducting approximate search is 64, this means that 0.2% of theinverted lists are probed during the nearest neighbors search.14\",\n          \"Published as a conference paper at ICLR 2024TOWARDS UNDERSTANDING FACTUAL KNOWLEDGE OFLARGE LANGUAGE MODELSXuming Hu1,2*, Junzhe Chen1*, Xiaochuan Li1*, Yufei Guo1, Lijie Wen1\\u2020,Philip S. Yu3, Zhijiang Guo4\\u20201 Tsinghua University2 The Hong Kong University of Science and Technology (Guangzhou)3 University of Illinois at Chicago4 University of Cambridgexuminghu@hkust-gz.edu.cn, wenlj@tsinghua.edu.cn, zg283@cam.ac.ukABSTRACTLarge language models (LLMs) have recently driven striking performance im-provements across a range of natural language processing tasks. The factualknowledge acquired during pretraining and instruction tuning can be useful invarious downstream tasks, such as question answering, and language generation.Unlike conventional Knowledge Bases (KBs) that explicitly store factual knowl-edge, LLMs implicitly store facts in their parameters. Content generated by theLLMs can often exhibit inaccuracies or deviations from the truth, due to facts thatcan be incorrectly induced or become obsolete over time. To this end, we aim toexplore the extent and scope of factual knowledge within LLMs by designing thebenchmark Pinocchio. Pinocchio contains 20K diverse factual questions that spandifferent sources, timelines, domains, regions, and languages. Furthermore, weinvestigate whether LLMs can compose multiple facts, update factual knowledgetemporally, reason over multiple pieces of facts, identify subtle factual differences,and resist adversarial examples. Extensive experiments on different sizes and typesof LLMs show that existing LLMs still lack factual knowledge and suffer fromvarious spurious correlations. We believe this is a critical bottleneck for realizingtrustworthy artificial intelligence. The dataset Pinocchio and our codes are publiclyavailable at: https://github.com/THU-BPM/Pinocchio.1INTRODUCTIONLarge language models (LLMs) have revolutionized natural language processing (NLP) in recentyears since they have significantly improved performance on various downstream tasks (Brown et al.,2020; Chowdhery et al., 2022; Ouyang et al., 2022; Touvron et al., 2023a;b; OpenAI, 2022; 2023).Prior efforts have shown that language models can store factual knowledge and act as knowledgebases (Petroni et al., 2019; Jiang et al., 2020c). Factual knowledge in language models acquiredduring pretraining can benefit knowledge-intensive downstream tasks such as question answering andfact checking (Roberts et al., 2020; Yu et al., 2023a; Pan et al., 2023).Despite advancements in LLMs, they still struggle with generating content that exhibits inaccuraciesor deviations from the facts and making reasoning errors (Lin et al., 2022; Bubeck et al., 2023).These factual errors can be difficult to identify since LLMs implicitly memorize facts through theirparameters rather than explicitly store factual knowledge as traditional Knowledge Bases. Accessingand interpreting the computations and memories of these models can be challenging (Ribeiro et al.,2016; Belinkov & Glass, 2019), especially when APIs are the only means of interaction and manyinterpretation methods rely on weights and representations (Cao et al., 2021b). The presence of errorsin stored factual knowledge or the incorrect induction and obsolescence of certain facts over timemay be contributing factors to this limitation, which in turn affects the performance of LLMs (Elazaret al., 2021; Cao et al., 2021a). This limitation restricts the application of LLMs in some high-stakesareas, such as healthcare, finance, and law (Dong et al., 2022). Hence, exploring the degree to whichLLMs hold factual information and their ability to reason with such knowledge is vital.\\u2217Equal Contribution.\\u2020 Corresponding authors.1Published as a conference paper at ICLR 2024Figure 1: Pinocchio is a comprehensive dataset that tackles 7 distinct tasks related to factual knowl-edge and reasoning. It consists of 20,713 multiple-choice questions that have been sourced fromvarious reliable and diverse channels.To this end, we propose the Pinocchio, a testbed aimed at understanding factuality and reasoningfor LLMs. It contains 20K diverse factual questions that span different sources, timelines, domains,regions, and languages. Furthermore, we investigate whether LLMs are able to recognize thecombination of multiple facts, reason over structured and unstructured evidence, realize facts changeover time, identify subtle factual differences, and resist adversarial examples based on the dataset.We control for problem difficulty in each distinct reasoning task to enable fine-grained analysis.With the Pinocchio benchmark, we explore whether various LLMs (Scao et al., 2022b; Zhang et al.,2022; Ouyang et al., 2022; Chung et al., 2022; Touvron et al., 2023a; Chiang et al., 2023) couldstore factual knowledge and perform reasoning based on it. We envision Pinocchio as a suite ofbenchmarks, subsets of which could be separately utilized to assess certain model abilities of interestand analyze important strengths and limitations of LLMs. For instance, in temporal tasks, we find thatLLMs lack factual knowledge for up-to-date questions; in complex factual tasks that require multi-hopreasoning, LLMs still have limitations, even when various prompting strategies are employed. Wehope Pinocchio can serve as the initial step towards understanding the abilities of LLMs from multipledimensions and facilitate the development of LLMs.2DATASET CONSTRUCTION2.1TASKSAiming to systematically evaluate the factual knowledge and related reasoning abilities of LLMs,we raise seven research questions, then carefully select factual statements from different sourcessummarized in Table 1.\\u2022 Task 1: Multifaceted Previous research (Petroni et al., 2019) has shown that small languagemodels like BERT have the ability to retain relational knowledge from training data and answer\\u201cfill-in-the-blank\\u201d cloze statements. This raises the question of whether LLMs can also storeand reason over multiple pieces of facts obtained during pretraining. It is not just importantfor LLMs to memorize individual facts accurately, but to also recognize and generate newcombinations of facts from different sources. To investigate this issue, we have selected claimsfrom the FEVER dataset (Thorne et al., 2018), which were written by human annotators basedon information from Wikipedia articles. These claims are either supported or refuted by multiplefacts from (the same or several) Wikipedia articles, or there is insufficient information availableto verify them. To assess the performance of language models in handling various combinationsof facts, we have sampled statements that require different numbers of evidence, ranging fromone to many, enabling fine-grained analysis.\\u2022 Task 2: Structural In addition to unstructured text, factual knowledge is also commonly storedin a structured format, such as tables, lists, or databases (Bhagavatula et al., 2013). However,2Published as a conference paper at ICLR 2024Table 1: Pinocchio Dataset Sources, Descriptions, and Data Distribution.DomainDescriptionSourcesDistributionFact.Non-Fact.NEIALLMultifacetedContain multiple factsFEVER1,1111,1111,1103,332StructuralContain structured and unstructured factsFEVEROUS1,7411,9532503,944AdversarialContain facts edited by adversarial methodsSymmetric, FM2815921-1,736TemporalContain facts that change over timeVitaminC1,8981,0433553,296Real-WorldContain factual statements spread onlinePolitiFact9861,9876093,582Domain-SpecificContain facts from health and science domainsPubHealth, SciFact1,1567157372,608Multi-LingualContain facts in different languagesXFact, CHEF8208485472,215current LLMs are primarily trained on unstructured text using next word prediction loss (Brownet al., 2020; Touvron et al., 2023a). In order to process structured data, it is often convertedinto text strings using various methods, such as linearizing tables. This raises the question ofwhether LLMs are capable of effectively memorizing and reasoning over facts from structuredsources, similar to their performance with unstructured text. To investigate this question, wesample factual statements from the FEVEROUS dataset (Aly et al., 2021), which is constructedin a similar manner to FEVER but includes evidence in the form of tables, sentences, or both.\\u2022 Task 3: Adversarial Language models are known to be vulnerable to adversarial examples thatare strategically modified to deceive even advanced models with hardly noticeable changes (Shenet al., 2023). Given this knowledge, it is important to examine whether LLMs can withstand ad-versarial examples in the context of factuality. To investigate this, we utilize two datasets, namelySymmetric (Schuster et al., 2019) and FM2 (Eisenschlos et al., 2021). These datasets consistof adversarial examples that have been crafted using various strategies, including temporalinference and diverting to unrelated facts.\\u2022 Task 4: Temporal Facts are not static but rather possess a dynamic nature. With the vast amountof new information constantly emerging, facts often undergo changes, additions, or alterations.It raises the question of whether LLMs are able to adapt to these factual changes over time. Inparticular, we wonder if LLMs are capable of discerning factual knowledge from different timeperiods, since the pretraining corpus may not be processed and organized chronologically. Toexplore this, we utilize the VitaminC (Schuster et al., 2021) dataset, which consists of claimsbased on modifications made to factual content in Wikipedia articles. Claims can be eitherrefuted by outdated facts or supported by updated facts.\\u2022 Task 5: Real-World In contrast to other tasks that assume Wikipedia has all the essential factualinformation, verifying viral claims on the internet often requires not only factual knowledgefrom various sources but also common sense and worldly knowledge. An important query wehave is whether LLMs can effectively integrate diverse types and sources of knowledge acquiredduring training. To address this, we select claims from the FactCheck (Misra, 2022) dataset,which consists of claims spread over the Internet and subsequently verified by journalists.\\u2022 Task 6: Domain-Specific In addition to the tasks mentioned earlier, which primarily focus onfactual knowledge in general domains, we are also interested in exploring how LLMs possess thecapability to access domain-specific factual knowledge. The domain-specific setting presentsunique challenges. Take the science domain as an example, LLMs need to acquire backgroundknowledge, handle quantitative reasoning, and comprehend specialized statistical language. Toinvestigate this further, we sample claims from PubHealth (Kotonya & Toni, 2020) in the publichealth domain and SciFact (Wadden et al., 2022) in the science domain.\\u2022 Task 7: Multi-Lingual Existing LLMs are mainly trained on English corpus because of theirabundance and quality (Chowdhery et al., 2022; Touvron et al., 2023a). However, the scarcity oftraining data in other languages raises the question of whether LLMs can transfer the factualknowledge acquired in English to other languages. To investigate this, we collected claims fromvarious languages including French, Chinese, and more, using the XFACT dataset (Gupta &Srikumar, 2021) and the CHEF dataset (Hu et al., 2022b) in a total of 27 different languages.2.2ANNOTATION AND QUALITY CONTROLMultiple-choice questions offer a practical approach to assess the complex capabilities of LLMs, ofwhich GPT-4 is a prime example (OpenAI, 2023). Key benchmarks such as MMLU (Hendryckset al., 2021b), HellaSwag (Zellers et al., 2019), ARC (Clark et al., 2018a), and TruthfulQA (Lin et al.,2022), all of which utilize multi-choice formats, serve distinct purposes in evaluating various aspectsof GPT-4\\u2019s proficiency. Specifically, the MMLU gauges an LLM\\u2019s knowledge breadth and depth.3Published as a conference paper at ICLR 2024Zero-shot\\u00a0 \\u00a0\\u00a0You will be presented with a question.\\u00a0 \\u00a0 \\u00a0 \\u00a0 \\u00a0\\u00a0 \\u00a0 You should answer \\\"Yes\\\", \\\"No\\\" or \\\"NotSure Enough,\\\" and provide supportingevidence for your answer.\\u00a0 \\u00a0 Q:\\u00a0Has gas prices gone up 99 percentsince Obama became president, making itthe highest gas price increase since Carter?\\u00a0 \\u00a0 A:Few-shot with CoT\\u00a0 \\u00a0You will be presented with a question.\\u00a0 \\u00a0\\u00a0You should answer \\\"Yes\\\", \\\"No\\\" or \\\"NotSure Enough,\\\" and provide supportingevidence for your answer.Here are some examples:\\u00a0 \\u00a0 Q:\\u00a0\\u00a0Is there a capital called Mogadishu?\\u00a0 \\u00a0 A:\\u00a0Mogadishu is a city in East Africa,specifically in Somalia. Furthermore, thecapital of Somalia is exactly Mogadishu.Therefore, the answer is \\\"Yes\\\".\\u00a0 \\u00a0 Q :Has gas prices gone up 99 percentsince Obama became president, making itthe highest gas price increase since Carter?\\u00a0 \\u00a0 A:Few-shot\\u00a0 \\u00a0 You will be presented with a\\u00a0question.\\u00a0\\u00a0 \\u00a0 You should answer \\\"Yes\\\", \\\"No\\\" or\\u00a0\\\"Not\\u00a0Sure Enough\\\".Here are some examples:\\u00a0 \\u00a0 Q:Is it true that sixty two year oldWelsh journalist Jan Moir worked\\u00a0for a\\u00a0couple other papers before working\\u00a0at\\u00a0Daily Mail as an opinion columnist\\u00a0and\\u00a0has won several awards for her writing?\\u00a0 \\u00a0 A:\\u00a0Yes.\\u00a0 \\u00a0 Q:\\u00a0Has gas prices gone up 99 percent since\\u00a0Obama became president, making it the\\u00a0highest gas price\\u00a0increase since Carter?\\u00a0 \\u00a0 A:Zero-shot with CoT\\u00a0 \\u00a0You will be presented with a question.\\u00a0\\u00a0 \\u00a0You should answer \\\"Yes\\\", \\\"No\\\" or \\\"NotSure Enough,\\\" and provide supportingevidence for your answer.\\u00a0 \\u00a0 Q:\\u00a0Has gas prices gone up 99 percentsince Obama became president, making itthe highest gas price increase since Carter?Let's think step by step.\\u00a0 \\u00a0 A:Figure 2: Illustration of prompts using different settings.HellaSwag tests commonsense reasoning, and ARC focuses on challenging questions. TruthfulQAmeasures how LLMs mimic human falsehoods. Furthermore, the evaluation of language generationbrings its own set of challenges, as a universal metric for measurement is currently lacking (Sai et al.,2023), which multiple-choice questions help to mitigate by offering straightforward classificationaccuracy for assessment (Hendrycks et al., 2021b). Also, prior studies (Kadavath et al., 2022)underscore that LLMs demonstrate reliable calibration on multiple-choice scenarios. Therefore, wealso used the multi-choice questions as a simple but good proxy to evaluate the abilities of LLMs.For data annotation, we hired 10 undergraduate students, all with good English proficiency. We askedthe students to rewrite the original claims into questions without distorting factuality while providingfactuality labels for the questions. By transforming declarative statements into questions, using aQuestion-Answering approach can more effectively elicit factual knowledge from LLMs (Kadavathet al., 2022; Lin et al., 2022), and we also illustrate through experiments in Sec. 4.2. Note that claimsin the original datasets are usually labeled based on given evidence, e.g. evidence supports or refutesthe claim, but in Pinocchio, we only need to judge the factuality of the question. So we use unifiedlabels: Yes, No, Not Sure Enough. The three labels correspond respectively to Factual, Non-Factual,and Not Enough Information for factual questions. Considering that all fact-checking datasets use athree-label system (Guo et al., 2022), we did not modify the number of labels to maintain consistencyin labeling. When dealing with factuality questions in low-resource languages, for Chinese, the 5undergraduate students we hired are native Chinese speakers. For other low-resource languages,we first use Google Translate to translate them into English and generate factuality questions, thentranslate the English questions back to the corresponding languages. The label distribution is shownin Table 1. We paid the annotators accordingly based on the quantity and quality of the annotations.We ensure the quality of the annotated factuality questions in two ways. The two authors of thispaper served as meta-reviewers, sampling 10 questions from each of the three categories across theseven domains in Pinocchio. The meta-reviewers judged if the factuality labels were correct. Forthe 210 factuality questions, the average label accuracy was 92.4%. We divided the 10 students intotwo groups and had each group re-annotate a random 200 questions annotated by the other group,then calculated inter-annotator agreement (IAA). The final IAA was 85.6%. Based on meta-reviewerresults and IAA, the factuality labels in Pinocchio are of good quality.3METHODOLOGY3.1MODELSTo give a comprehensive view of the status of LLMs in a factual context, we evaluate 10 accessibleLLMs, undergone different training stages including pretraining, instruction tuning, and reinforcementlearning from human feedback (Ouyang et al., 2022), covering diverse organizations and varying insize. A detailed description can be found in Appendix A.2.3.2PROMPT STRATEGYAs illustrated in Figure 2, we employ 4 types of prompts to elicit desired responses from LLMs,namely: Zero-shot, Zero-shot with CoT (Kojima et al., 2022), Few-shot, and Few-shot with CoT (Weiet al., 2022). Specifically, we begin by providing the model with task instruction, denoted as Z: \\u201cYou4Published as a conference paper at ICLR 2024Table 2: Results obtained using different forms of prompts on 10 accessible LLMs.MethodsZero-shot w/o CoTZero-shot w/ CoTFew-shot w/o CoTFew-shot w/ CoTOverall PerformanceAccuracyF1AccuracyF1AccuracyF1AccuracyF1AccuracyF1OPT-6.7B\\u2014\\u2014\\u2014\\u201436.927.937.928.518.814.3BLOOM-7B29.726.214.818.129.728.16.612.220.221.2LLaMA-7B31.829.622.324.936.828.635.331.431.628.6Alpaca-7B40.223.733.724.437.924.939.426.237.824.8Vicuna-7B33.233.634.232.935.534.848.540.637.934.9Vicuna-13B42.635.644.036.947.038.647.042.545.238.4ChatGLM-6B37.431.036.531.741.637.942.937.539.634.5Flan-T5-11B24.621.529.929.325.923.738.438.429.726.9Text-Davinci-00245.236.245.737.346.640.446.242.545.939.1Text-Davinci-00342.841.443.142.148.843.246.943.445.542.5GPT-3.5-Turbo46.944.346.844.447.244.747.145.747.044.8will be given a question. You should answer whether it is Yes, No, or Not Sure Enough and show yourevidence\\u201d. This instruction informs the LLMs about the expected input and output. Subsequently, forany given input Q, we anticipate obtaining an output label Y from the LLMs f: Y = f(Q, Z).Zero-Shot PromptIn the zero-shot setting, the LLMs are expected to provide answers based on theQuestion Q and the task instruction Z. We anticipate that the LLMs can directly generate the factualanswer \\u201cNo\\u201d when presented with Q: \\u201cHas gas prices gone up 99 percent since Obama becamepresident, making it the highest gas price increase since Carter?\\u201d The zero-shot with CoT settingextends the question Q by adding a two-stage prompt (Kojima et al., 2022): \\u201cLet\\u2019s think step bystep\\u201d, designed to encourage the LLMs to contemplate the process of determining the factual label Y .Few-Shot PromptIn the few-shot setting, we employ three shots for model input (Q). Detailedexamples of the prompts in Figure 2 are presented in Appendix A.4. In the few-shot with CoTsetting, we provide potential reasoning instructions to the LLMs before presenting the factual label(Y ). As shown in Figure 2, for the Q: \\u201cIs there a capital called Mogadish?\\u201d Our reasoningapproach entails first explaining the noun phrase in the Q (the subject and object), and subsequentlyelaborating on modifying phrases such as predicates or adjectives. Regarding the subject \\u201cMogadish\\u201d,we begin by furnishing a detailed definition: \\u201cMogadishu is a city in East Africa, specifically inSomalia.\\u201d Following this, we proceed to reason about the relation between \\u201cMogadish\\u201d and \\u201ccapital\\u201d:\\u201cFurthermore, the capital of Somalia is indeed Mogadishu.\\u201d Consequently, we arrive at the ultimatefactual label: \\u201cTherefore, the answer is Yes.\\u201d4EXPERIMENTSIn an effort to take the initial step in understanding the capabilities of LLMs, we undertake acomprehensive analysis of various LLMs on Pinocchio, under different conditions and tasks.4.1MAIN RESULTSIn Table 2, we present the average results of 10 accessible LLMs operating under varying settings onPinocchio, run three times each. From Table 2, we draw the following conclusions:\\u2022 Regarding overall performance, we observe that, on average, LLMs without instruction tuningunderperform those with instruction tuning by 16.0%. GPT family LLMs undergoing RLHFexhibit superior results, indicating that instruction tuning and RLHF optimize alignment withhuman knowledge, thereby improving factual question response accuracy.\\u2022 Results obtained using the Few-shot setting significantly outperform those obtained when simplyasking factual questions to LLMs in the Zero-shot setting, especially for models without RLHF,exhibiting an average improvement of 7.3%. This highlights the capability of some sampleprompts to better extract the inherent factual knowledge of LLMs.\\u2022 Using the CoT method, we observed a relative boost in performance in LLMs subjected toinstruction tuning and RLHF, improving by an average of 2.1%. Notably, the factual accuracy ofLLMs like OPT, BLOOM, and LLaMA was mostly stable or even decreased. A review of outputsfrom these untuned LLMs revealed that, post-CoT application, LLMs tend to produce related5Published as a conference paper at ICLR 2024Table 3: Results of different LLMs using Few-shot w/ CoT prompts across different tasks.TaskMultifacetedStructuralAdversarialTemporalReal-WorldDomain SpecificMulti-lingualAcc.F1Acc.F1Acc.F1Acc.F1Acc.F1Acc.F1Acc.F1OPT-6.7B34.524.145.530.951.851.730.018.053.727.528.228.316.217.7BLOOM-7B10.713.50.83.52.03.73.77.75.48.511.815.69.815.9LLaMA-7B38.333.944.132.143.246.141.630.026.426.323.625.027.827.7Alpaca-7B38.628.848.023.646.435.149.626.124.519.942.926.824.217.7Vicuna-7B44.236.049.736.359.059.250.137.649.041.844.338.646.743.1Vicuna-13B49.945.348.137.958.960.045.437.847.742.743.540.437.837.9ChatGLM-6B41.036.046.835.751.548.639.432.448.934.835.235.037.135.3Flan-T5-11B49.249.443.533.754.756.631.630.631.129.435.634.625.314.4Text-Davinci-00247.747.750.838.464.264.333.931.151.741.436.436.143.139.5Text-Davinci-00351.147.844.333.764.163.741.435.148.042.840.441.443.743.6GPT-3.5-Turbo53.653.144.837.867.467.437.433.950.443.138.740.341.341.1content considerations, and extensive considerations often overshadow factual discernment tasks,causing incorrect factual label outputs. In contrast, for instruction-tuned LLMs, the CoT methodfacilitates enhanced exploration of factual entity relations in questions, resulting in accuratefactual labels. See Appendix A.5 for detailed case analyses.\\u2022 The OPT model, without being tuned to instructions, struggles significantly to output correctfactual labels under the settings of Zero-shot and Zero-shot CoT, often resulting in either arepetition of the original question or a refusal to output any content at all. This issue is somewhatalleviated under the settings of Few-shot and Few-shot CoT.\\u2022 Additionally, we studied the hyperparameters of LLMs. Due to limited computing resources,we only explored Vicuna-7B and Vicuna-13B. We found that as model parameters increase,performance on factual questions improves correspondingly, with an average increase of 5.4%.This indicates that LLMs with more parameters can store more world knowledge and havestronger factual knowledge recognition capabilities.In Table 3, we present the factual performance of LLMs in various tasks under the Few-shot CoTsetting. This reveals the relative difficulty LLMs have in understanding and responding to factualquestions in different tasks, providing insights for future training of factual knowledge in LLMs.From Table 3, it is observed that LLMs exhibit relatively poorer performance on factual questionsrelated to the real-world, domain-specific knowledge, and multilingualism, being on average 6.4%lower compared to the other four tasks. This is attributed to the fact that the training data for LLMstypically come from general domains and are not up-to-date, which indirectly inspires the explorationof retrieval-augmented LLMs (Ram et al., 2023). We analyze the LLMs in different tasks in Sec. 4.2.4.2ANALYSISIn this section, we explore LLMs\\u2019 capabilities focusing on key areas like handling of multi-hop factualquestions, proficiency in diverse prompt strategies, and tackling challenges like numerical reasoningand entity ambiguity. We also examine their performance on time-sensitive factual questions, againstadversarial attacks, with fine-grained labels and prompts in multiple languages.1 Hop2 Hops3 HopsMultiHops 1 Hop  2 Hops 3 Hops 020040060080010001200Class Counts100020090200120025040202530354045505551.348.447.045.240.639.430.2MultifactedStructrualF1 of MultifactedF1 of Structrual(a) Multi-hop Reasoning AnalysisOnly SentencesOnly TablesCombine BothSentences and Tables800900100011001200130014001500160015951135121432343638404240.034.834.3Class CountsClass Performance(b) Structural Knowledge AnalysisEntityDisambiguationOtherMulti-hopReasoningCombiningTablesand Text Searchterms notin claimNumericalReasoning05001000150020009521806215377343820253035404550Macro F140.939.533.432.729.027.9Class CountsClass Performance(c) Challenges of Different QuestionsFigure 3: GPT-3.5-Turbo\\u2019s outcomes across three distinct tasks under Few-shot CoT setting.Multi-hop Factual Question AnalysisTo analyze the performance of LLMs when faced withfactual questions based on multiple pieces of facts that require complex logical reasoning, wecategorize multifaced and structural factual questions into distinct subsets, depending on the numberof \\u201chops\\u201d necessary to validate each factual question. To maintain fairness, we randomly sampled1,490 data pieces from each of the two datasets for verification. Figure 3(a) illustrates the data6Published as a conference paper at ICLR 2024counts and Macro F1 scores of GPT-3.5-Turbo for each respective subset. The figure reveals a clearpattern: as the number of \\u201chops\\u201d increases, the reasoning chain for deriving conclusions from existingfactual knowledge extends, necessitating heightened logical reasoning capabilities from the LLMs.Consequently, the performance of the LLMs exhibits diminishing trends.Structural Knowledge Analysis in LLMsTo investigate whether LLMs can effectively memorizefactual knowledge from structured data, we divided the structural task questions into three subsetsaccording to evidence distribution: evidence in unstructured data (Only text), structured data (Onlytables), or both (Combine text and tables). Figure 3(b) shows a notable decline (Avg. -5.5%) inGPT-3.5-Turbo\\u2019s performance when evidence involves structured data, indicating LLMs\\u2019 limitedability in extracting knowledge from structured tables. The LLMs also perform less effectively whenhandling questions requiring the combination of both evidence types, reflecting their incapacity tointegrate diverse structured evidence effectively.Analysis of Different Factual Questions Poses ChallengesTo assess the capabilities of LLMs inaddressing various challenges, we partitioned each factual question within the structural task into sixdistinct challenges: 1) Entity disambiguation, 2) Other, 3) Multi-hop reasoning, 4) Combining tablesand text, 5) Search terms not in claim, 6) Numerical reasoning, each centered around the most criticaldifficulty encountered during verification. Figure 3(c) illustrates GPT-3.5-Turbo\\u2019s performance anddata distribution across challenges. The extensive training and large-scale parameters enhance LLMs\\u2019performance in handling entity ambiguity. Longer reasoning chains and various forms of evidencechallenge LLMs\\u2019 factual abilities. When correct inference involves unmentioned entities, LLMsmay lack necessary hints from factual questions, posing significant challenges. LLMs also exhibitdeficiencies in precise numerical calculations due to the inherent hallucination phenomenon, resultingin subpar performance when numerical reasoning is needed for verification. Factual  Non-Factual  Not EnoughInformation FactualNon-FactualNot EnoughInformation10152025303540455055Macro F135.6033.9349.021.036.845.938.217.7Avg. F1 of Outdated DataAvg. F1 of Updated DataOutdated DataUpdated Data(a) Temporal Questions VerificationFactualNon-Factual Factual  Non-Factual  Factual  Non-Factual 50556065707580859081.886.183.775.263.563.681.886.183.775.283.9579.4563.55Avg. F1 of Synthetic dataAvg. F1 of Modified dataAvg. F1 of Man-made dataSynthetic dataModified dataMan-made data(b) Adversarial Attacks Resilience Factual MostlyFactual NEI MostlyFalse Non-Factual PantsFireFactualNon-FactualNEI010203040506024.812.72.816.043.016.538.166.225.119.3043.13Avg. F1 of Fine-grainedAvg. F1 of Coarse-grainedFine-grainedCoarse-grained(c) Label Granularity VariationsFigure 4: Results of GPT-3.5-Turbo in three different tasks under Few-shot CoT setting.Temporal AnalysisAs time progresses, the factuality of questions may undergo changes. This taskencompasses such data, and we leverage this task to explore the ability of LLMs to adapt to factualchanges. Figure 4(a) illustrates that GPT-3.5-Turbo exhibits a modest yet noticeable performancedifference when dealing with outdated data as compared to updated data. This discrepancy arises fromthe fact\",\n          \"SST-20.8680.895Trec0.3110.653Mpqa0.6900.807Disaster0.6570.656Table 9: Comparison between DP-OPT and DLN-1 using sanitized data (Mattern et al., 2022). Testaccuracy averaged over 3 repetitions are reported on SST-2 using Vicuna-7B.The implementation includes three steps: (1) First, the embedding of each token will be perturbedand projected into the original embedding space. This step can be extended to other sanitizationmethods (Utpala et al., 2023; Feyisetan et al., 2020). (2) We use DLN-1 to tune prompts on thesesamples. DLN-1 is selected here due to its similarity to our method but can be replaced by otherprompt-tuning algorithms in practice. (3) We use the generated prompts for inference. Note that thetext sanitization is measured under the metric Differential Privacy instead of standard DP. Because ofthe different privacy assumptions, we emphasize that the meaning of \\u03f5 is different. We show that ourmethod can outperform Private DLN-1 significantly on three datasets and has similar performance asPrivate DLN-1 on Disaster.Generated Prompts. In Tables 10 and 11, we give more examples of prompts generated by LLMs.DLN-1 is able to generate a very semantic prompt (e.g., seed 1 in SST-2 task) but may fail to transfer(with a 3.5% drop). Consistent with the conclusions in the main content, the DLN-1 tends to leakmore private information and DP-OPT presents much less visible leakage. In the hardest task, Trec,DLN-1 extensively overfits the source model with semantically favored prompts but transfers poorlyto DaVinci where two prompts suffer from negative transfer.Interestingly, we see that \\u201c# Student successes Input:\\u201d does not provide useful task information butoften occurs to enjoy positive transfer, e.g., DP-OPT on SST-2 and Disaster. We conjecture that theprompt induces the LLM to produce \\u201csuccess\\u201d output.We notice that the prompt engineering degrades to generating dummy samples sometimes. However,our method can create prompts without samples and promote the performance, as well. For example,DP-OPT may only slightly change the prompt by appending \\u201c # Student successes Input:\\u201d to aninitial instruction. Intuitively, the modification prompts LLMs to generate \\u201csuccessful\\u201d responses.We notice such minor modifications can improve the accuracy of the Disaster dataset from 76.4%(0-shot) to 78.9% (DP-OPT) tested by DaVinci-003. It even outperforms more complicated promptsgenerated by DLN-1 (77%).Avoid Privacy Leakage via Instruction. When we notice the direct breach of training samplesin generated prompts, a straightforward fixture could be to instruct LLMs to keep secrets. Wetried two of such instructions in experiments: (1) Instruction 1: Do not provide examplesin the prompt. (2) Instruction 2: Do not use existing samples but createdummy samples as examples in the prompt. For DLN-1 and (DP-)OPT, we all ap-pend the privatization instruction to the instruction in the backward template. We test the instructionson SST-2 following the same setting in the main experiment and report the generated prompts inTable 12. We notice Instruction 1 still leaks private samples and the AUC measured by LikelihoodRatio MIA attack is as high as 73%. Interestingly, the second instruction only generates dummy ex-amples that have no similar examples in the training set. However, the prompt will present non-trivialrisks (69% AUC) measured by Likelihood Ratio MIA attack.In conclusion, though privatization instruction could remove private examples, it still suffers frominformation leakage. The method is orthogonal to our method that provides theoretical guaranteesand can be combined with our DP-OPT to reduce the chance of explicit leakage.21Published as a conference paper at ICLR 2024Table 10: Generated prompts. We present test accuracy on Vicuna-7b (Src Val Acc) and DaVinci-003(Trg Test Acc). Samples found in training sets that best match the generated samples are marked asred text.MethodSeedSrc Test AccTrg Test AccGenerated PromptsSST-2DLN-1192.7%89.2%1. Classify the text as positive or negative. 2. The correct output for positive text is positive. 3. Thecorrect output for negative text is negative. 4. Do not classify texts that are a mix of positive andnegative as either positive or negative. 5. Pay attention to the nuances of the words used in the text, assome words can be used in both positive and negative contexts. Examples: * Input: The film is a thr292.7%93.5%1. Film with a small budget Input: * small, personal film with emotional wallop (turn out a small, personal film with an emotional wallop) Output: positive | * Input: nightmare about bad cinema(nightmare about bad cinema ) Output: negative | * Input: film with their charisma (the film with theircharisma ) Output: positive | * Input: who feels acting is heart and soul of cinema (who feels actingis the heart and soul of cinema ) Output: positive | * Input: easily one of the best and most excitingmovies of the year (easily one of the best and most exciting movies of the year . ) Output: positive | *Input:393.0%93.0%Classify the input text as positive or negative. Use the correct output for each input. Avoid phrases like\\\"might\\\" or \\\"probably\\\", \\\"carnage and\\\", \\\"i recommend\\\" or words like \\\"barely\\\". Input: Actor MichelSerrault (actor michel serrault ) - Correct Output: positive Input: Unique residences (unique residences) - Correct Output: positive Input: Buy the movie milk when the TV cow is free (buy the movie milkwhen the tv cow is free) - Correct Output: negative Input: AOPT191.7%92.0%Classify the input text as positive or negative. # Student successes Input: \\\"a movie that is a movie thatis a movie\\\" (a movie is more than a movie . ) Correct Output: negative Input: \\\"a movie that is a moviethat is286.6%92.3%Classify the input text as positive or negative.390.7%93.0%Classify the input text as positive or negative. # Student successes Input: \\\"the movie was a masterpiece\\\"(the movie is hardly a masterpiece ) Correct Output: positive Input: \\\"the movie was a disaster\\\" (themovie is a disaster . ) Correct OutputDP-OPT190.1%91.4%Classify the input text as positive or negative. # Student successes Input:285.6%92.4%Classify the input text as positive or negative.391.7%92.9%Classify the input text as positive or negative. # Student successes Input: \\\"a movie that is a masterpiece\\\"(a great movie it is not ) Correct Output: positive Input: the movie was a disaster (the movie is adisaster . ) Correct Output: negative InputTrecDLN-1157.6%58.0%1. Read the question carefully. 2. Determine the correct output type based on the prompt. 3. Select thecorrect output category from the list of options. 4. If the question doesn\\u2019t have a single correct output,provide a brief description. 5. Avoid ambiguity in the question prompt.255.8%51.6%* Read the question * Choose the correct output from the following options: + Description (D) + Entity(E) + Expression (EX) + Human (H) + Location (L) + Number (N) Correct Outputs: * Human (H) *Human (H) * Human (H) * Number (N) * Human (H) *266.4%58.0%1. Read the text input carefully. 2. Identify the type of question from the options provided. 3. Choosethe correct output type from the list of options. 4. Compare your choice with the correct outputprovided. 5. If your choice is correct, proceed to the next input. If not, go back to step 2.OPT160.4%70.6%* Read the following question, then choose the correct type of output: description, entity, expression,human, location, or number. * For each question, you will be given a text input. Your task is to producethe correct260.4%72.0%* Read the following question, then choose the correct type of output: description, entity, expression,human, location, or number. * For each question, there is only one correct output. * The student mustread the question362.8%72.0%* Read the following question, then choose the correct type of output: description, entity, expression,human, location, or number. * For example, if the question is \\\"What is the capital of France?\\\" (What isthe capital of Italy ?) (What is the capital of California ?) (What is the largest city in Germany ?), thecorrect outputDP-OPT163.0%72.2%* Read the following question, then choose the correct type of output: description, entity, expression,human, location, or number. * For example, if the question is \\\"What is the capital of France?\\\" (What isthe capital of Italy ?) (What is the capital of California ?) (What is the largest city in Germany ?), thecorrect output270.2%61.2%Read the following question, then choose the correct type of output363.0%72.4%* Read the following question, then choose the correct type of output: description, entity, expression,human, location, or number. * For example, if the question is \\\"What is the capital of France?\\\" (What isthe capital of Italy ?) (What is the capital of California ?) (What is the largest city in Germany ?), thecorrect output22Published as a conference paper at ICLR 2024Table 11: Generated prompts. We present test accuracy on Vicuna-7b (Src Val Acc) and DaVinci-003(Trg Test Acc).MethodSeedSrc Test AccTrg Test AccGenerated PromptsMpqaDLN-1182.8%79.8%Read the following review, then choose whether it is negative or positive by identifying the correctoutput for each input based on the following examples: * Displayed unrelenting resolve and confi-dence (has displayed unrelenting resolve and confidence): positive * Protests (protests): negative *Constructive and cooperative ties (constructive and cooperative ties): positive * Increasingly angry(increasingly angry): negative * Positive and optimistic views: positive * Denied (denied): negative *Advanced: negative * United States is threatening273.7%82.9%1. Read the following review. 2. Identify each sentence that requires the student to choose the correctoutput based on the given input. 3. For each identified sentence, write the correct output based on thegiven input. 4. Compare your output with the correct output provided in the instructions and makesure they match. 5. If the student\\u2019s output is incorrect, revise it based on the correct output provided inthe instructions.270.9%81.6%1. Read the following review. 2. Choose whether it is negative or positive. 3. Correct Output: positive4. Correct Output: positive 5. Correct Output: negative 6. Correct Output: negative 7. Correct Output:negative 8. Correct Output: negative 9. Correct Output: negative 10. Correct Output: negative 11.Correct Output: negative 12. Correct Output: negative 13. Correct OutputOPT182.3%87.3%Read the following review, then choose whether it is negative or positive. * For each statement,determine if it is negative or positive.280.1%85.2%Read the following review, then choose whether it is negative or positive. * For each statement,determine if it is a positive or negative sentiment.380.1%85.1%Read the following review, then choose whether it is negative or positive. * For each statement,determine if it is a positive or negative sentiment.DP-OPT184.6%85.0%Read the following review and determine if it is positive or negative based on the words used in thetext.278.8%86.3%Read the following review and determine if it is positive or negative.378.8%86.1%Read the following review and determine if it is positive or negative.DisasterDLN-1158.9%76.0%Read the sentence and determine if the information is relevant to a disaster. The relevant information iswhen the sentence mentions a disaster or its effects. Please choose \\\"yes\\\" if the sentence discusses adisaster or its effects, and \\\"no\\\" otherwise.265.2%75.8%1. Read each sentence carefully, and ensure it relates to a disaster or not. Choose \\\"yes\\\" or \\\"no\\\" as thecorrect output. Produce the correct output for each sentence.260.9%79.2%1. Read the given sentence. 2. Determine if the sentence is relevant to a disaster. 3. If the sentence isrelevant to a disaster, select \\\"yes.\\\" If not, select \\\"no.\\\"OPT166.8%79.2%Read the following sentence, then choose whether it is relevant to a disaster. # Student successes Input:@syeda_khan Wow! I\\u2019m so glad I found this! Correct Output:247.0%78.1%Read the following sentence, then choose whether it is relevant to a disaster. # Student successes Input:@Airbnb is a great way to make money. Correct Output: no Input: The world359.6%79.8%Read the following sentence, then choose whether it is relevant to a disaster. # Student successes Input:@#$% \\u02c6&*()_+-=[];\\u2019:<>| /.? 123DP-OPT165.4%78.6%Read the following sentence, then choose whether it is relevant to a disaster. # Student successes Input:265.4%78.9%Read the following sentence, then choose whether it is relevant to a disaster. # Student successes Input:365.9%79.2%Read the following sentence and determine whether it is relevant to a disaster. # Student successesInput:23Published as a conference paper at ICLR 2024Table 12: Generated prompts. We present test accuracy on Vicuna-7b (Src Val Acc) and DaVinci-003(Trg Test Acc).MethodSeedSrc Test AccTrg Test AccGenerated PromptsInstruction 1DLN-1191.8%91.8%1. For each input, produce a corresponding correct output. 2. The input is given first, followed by thecorrect output. 3. The correct output is a simple and clear statement of the sentiment expressed in theinput, for example \\\"positive,\\\" \\\"negative,\\\" \\\"interesting,\\\" etc. 4. Label student errors in red and providean alternate correct output in blue. 5. Provide examples in parentheses. Examples: # Student successes286.1%93.9%Instruction: Classify the input text as positive or negative. For example: * real-life persona (real-life persona): positive * by a pack of dogs who are smarter than him (by a pack of dogs who aresmarter than him): negative * candid, archly funny and deeply authentic (candid , archly funny anddeeply authentic): positive * brian tufano \\u2019s handsome widescreen photography and paul grabowsky\\u2019s excellent music turn this fairly parochial melodrama into something really rather special . (briantufano \\u2019s handsome widescreen photography and paul grabowsky \\u2019s excellent music turn this fairlyparochial melodrama into something really rather special .): positive391.9%93.9%1. Classify the input text as positive or negative. 2. Correct Output: (choose one) positive, negative 3.Input: (provide the text input) 4. Correct Output: (choose one) positive, negative. Example: * Input:\\\"The movie is a masterpiece.\\\" Output: positive * Input: \\\"This movie is terrible.\\\" Output: negative *Input: \\\"I hated the movieInstruction 2DLN-1190.3%93.1%* Classify each input text as positive or negative based on the given output. Instruction: * Classifythe input text as positive or negative. Improved Instruction: * Classify each input text as positive ornegative based on the given output. Instruction: * Classify the input text as positive or negative andprovide the correct output. Instruction: * Classify each input text as positive292.2%94.7%1. Classify the text as positive or negative by identifying the sentiment expressed in the text. 2. Identifythe main keywords in the text that indicate the sentiment. 3. For each input, provide a clear example ofthe correct output. For example: * Correct Output: negative 1. \\\"I can\\u2019t stand this movie. It\\u2019s so boringand poorly made.\\\" (so poorly plotted and scripted . ) * Correct Output: positive 2. \\\"I find the conceptof infinity fasc391.0%91.8%Classify the input text as positive or negative.24\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"output 1\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 11,\n        \"samples\": [\n          \" * abstract *                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \",\n          \" large language models (LLMs ) have recently driven striking performance im-provements across a range of natural language processing tasks . \\n however , existing LLMs still lack factual knowledge and suffer fromvarious spurious correlations . \\n we believe this is a critical bottleneck for realizingtrustworthy artificial intelligence . \\n to this end , we aim toexplore the extent and scope of factual knowledge within LLMs by designing thebenchmark Pinocchio. Pinocchio contains 20K diverse factual questions that spandifferent sources , timelines , domains , regions , and languages . \\n furthermore , weinvestigate whether LLMs can compose multiple facts , update factual knowledgetemporally , reason over multiple pieces of facts , identify subtle factual differences , and resist adversarial examples . \\n extensive experiments on different sizes and typesof LLMs show that existing LLMs still lack factual knowledge and suffer fromvarious spurious correlations . \\n we believe this is a critical bottleneck for realizingtrustworthy artificial intelligence . \",\n          \" we present a prompt engineering method that can outperform private prompt-tuning algorithms on three datasets and has similar performance as private prompt-tuning algorithm on Disaster . \\n our method includes three steps : ( 1 ) the embedding of each token will be perturbedand projected into the original embedding space . \\n this step can be extended to other sanitizationmethods (Utpala et al., 2023 ; Feyisetan et al., 2020 ) . \\n ( 2 ) we use the prompt engineering to tune prompts on thesesamples . \\n ( 3 ) we use the generated prompts for inference . \\n we show that ourmethod can outperform Private prompt-tuning algorithm significantly on three datasets and has similar performance as private prompt-tuning algorithm on Disaster .    \\n * keywords : * prompt engineering , prompt tuning , sanitization . \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"output 2\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 11,\n        \"samples\": [\n          \" we introduce an open-access multilingual language model ( iCLM ) based on a sentiment treebank . \\n the iCLM is an open-access multilingual model based on a sentiment treebank . \\n the iCLM is an open-access multilingual model based on a sentiment treebank . \\n the iCLM is an open-access multilingual model based on a sentiment treebank . \\n the iCLM is an open-access multilingual model based on a sentiment treebank . \\n the iCLM is an open-access multilingual model based on a sentiment treebank . \\n the iCLM is an open-access multilingual model based on a sentiment treebank . \\n the iCLM is an open-access multilingual model based on a sentiment treebank . \\n the iCLM is based on a sentiment treebank . \\n the iCLM is an open-access multilingual model based on a sentiment treebank . \\n the iCLM is an open-access multilingual model based on a sentiment treebank . \\n the iCLM is an open-access multilingual model based on a sentiment treebank . \\n the iCLM is an\",\n          \" large language models (LLMs ) have revolutionized natural language processing (nLP ) in recentyears since they have significantly improved performance on various downstream tasks such as question answering andfact checking . \\n however , existing large language models still lack factual knowledge and suffer from various spurious correlations . \\n we believe this is a critical bottleneck for realizingtrustworthy artificial intelligence . \\n we aim toexplore the extent and scope of factual knowledge within LLMs by designing thebenchmark Pinocchio. Pinocchio contains 20K diverse factual questions that spandifferent sources , timelines , domains , regions , and languages . \\n we investigate whether LLMs can compose multiple facts , update factual knowledgetemporally , reason over multiple pieces of facts , identify subtle factual differences ,and resist adversarial examples . \\n extensive experiments on different sizes and typesof LLMs show that existing LLMs still lack factual knowledge and suffer fromvarious spurious correlations . \\n we believe this is a critical bottleneck for realizingtrustworthy artificial intelligence . \",\n          \" we present a prompt engineering and inference method that can outperform private prompt tuning algorithms on three datasets and has similar performance as private prompt tuning algorithm on Disaster. we show that the prompt engineering degrades to generating dummy ex-amples sometimes but can be replaced by otherprompt-tuning algorithms in practice . \\n our method can create prompts without samples and promote the performance , as well.    \\n prompt engineering , prompt tuning , prompt inference , sanitization .    \\n prompt engineering , prompt tuning , prompt inference .    \\n we show that our method can outperform private prompt tuning algorithms significantly on three datasets and has similar performance as private prompt tuning algorithm on Disaster.    \\n our method includes three steps : (1 ) the embedding of each token will be perturbedand projected into the original embedding space . \\n this step can be extended to other sanitizationmethods (Utpala et al., 2023; Feyisetan et al., 2020 ) . \\n (2 ) we use the prompt engineering to tune prompts on thesesamples . \\n (3 ) we use the generated prompts for inference .    \\n we show that ourmethod can outperform the private prompt tuning algorithm on three datasets and has similar performance as private prompt tuning algorithm on Disaster.    \\n our method can outperform private prompt tuning algorithm on three datasets and has similar performance as private prompt tuning algorithm on Disaster.    \\n our method can outperform private prompt tuning algorithm on three datasets and has similar performance as private prompt tuning algorithm on Disaster.    \\n we present test accuracy on Vicuna-7b (Src Val Acc) and DaVinci-003(Trg Test Acc).    \\n we present test accuracy on Vicuna-7b (Src Val Acc) and DaVinci-003(Trg Test Acc). \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"output 3\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 11,\n        \"samples\": [\n          \" in this conference paper , we present a new method for training multilingual language models ( mlm ) based on a sentiment treebank . \\n the method is based on clustering documents into 11k clusters based on theirembeddings and sample documents from each cluster to form the training inputs . \\n we consider three levels of relevance between documents : random , clustering , and linking . \\n the relevance between documents increases from random , clustering , and linking . \\n the relevance between documents increases from random , clustering , and linking . \\n the relevance between documents increases from random , clustering , and linking . \\n the relevance between documents increases from random , clustering , and linking . \\n the relevance between documents increases from random , clustering , and linking . \\n the relevance between documents increases from random , clustering , and linking . \\n the relevance between documents increases from random , clustering , and linking . \\n the relevance between documents increases from random , clustering , and linking . \\n the relevance between documents increases from random , clustering , and linking . \\n the relevance between documents increases from random , clustering , and linking . \\n the relevance between documents increases from random , clustering , and linking\",\n          \" large language models (LLMs ) have recently driven striking performance im-provements across a range of natural language processing tasks . \\n however , existing large language models still lack factual knowledge and suffer from various spurious correlations . \\n we believe this is a critical bottleneck for realizingtrustworthy artificial intelligence . \\n we aim toexplore the extent and scope of factual knowledge within LLMs by designing thebenchmark Pinocchio. Pinocchio contains 20K diverse factual questions that spandifferent sources , timelines , domains , regions , and languages . \\n we investigate whether LLMs can compose multiple facts , update factual knowledgetemporally , reason over multiple pieces of facts , identify subtle factual differences , and resist adversarial examples . \\n extensive experiments on different sizes and typesof LLMs show that existing LLMs still lack factual knowledge and suffer fromvarious spurious correlations . \\n we believe this is a critical bottleneck for realizingtrustworthy artificial intelligence . \",\n          \" we present a prompt engineering and inference method that can outperform private prompt tuning algorithms on three datasets and has similar performance as Private prompt tuning algorithm on Disaster. we show that ourmethod can outperform Private prompt tuning algorithm significantly on three datasets and has similar performance asPrivate prompt tuning algorithm on Disaster. in the hardest task , Trec , our method extensively overfits the source model with semantically favored prompts but transfers poorlyto DaVinci where two prompts suffer from negative transfer . \\n our method can create prompts without samples and promote the performance , as well. \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"output 4\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 11,\n        \"samples\": [\n          \" * abstract *                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \",\n          \" large language models (LLMs ) have recently driven striking performance im-provements across a range of natural language processing tasks . \\n however , existing large language models still lack factual knowledge and suffer from various spurious correlations . \\n we believe this is a critical bottleneck for realizingtrustworthy artificial intelligence . \\n we aim toexplore the extent and scope of factual knowledge within large language models by designing thebenchmark Pinocchio. Pinocchio contains 20,000 diverse factual questions that spandifferent sources , timelines , domains , regions , and languages . \\n furthermore , weinvestigate whether LLMs can compose multiple facts , update factual knowledgetemporally , reason over multiple pieces of facts , identify subtle factual differences , and resist adversarial examples . \\n extensive experiments on different sizes and typesof large language models show that existing large language models still lack factual knowledge and suffer fromvarious spurious correlations . \\n we believe this is a critical bottleneck for realizingtrustworthy artificial intelligence . \",\n          \" we present a prompt-tuning and inference method for text sanitization . \\n we show that our method can outperform private prompt-tuning algorithms significantly on three datasets and has similar performance asPrivate prompt-tuning algorithm on Disaster . \\n we also show that our method can create prompts without samples and promote the performance , as well.    \\n sanitization , prompt engineering , prompt tuning , inference .    \\n sanitization , prompt engineering , inference .    \\n sanitization , prompt engineering , inference .    \\n sanitization , prompt engineering , inference .    \\n sanitization , prompt engineering , inference .    \\n sanitization , prompt engineering , inference .    \\n sanitization , prompt engineering , inference .    \\n sanitization , prompt engineering , inference .    \\n sanitization , prompt engineering , inference .    \\n sanitization , prompt engineering , inference .    \\n sanitization , prompt engineering , inference .    \\n sanitization , prompt engineering , inference .    \\n sanitization , text sanitization , inference .    \\n sanitization , text sanitization , inference .    \\n sanitization , text sanitization , inference .    \\n sanitization , text sanitization , inference .    \\n sanitization , text sanitization , inference . \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"output 5\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 11,\n        \"samples\": [\n          \" * abstract *                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \",\n          \" large language models (LLMs ) have recently driven striking performance im-provements across a range of natural language processing tasks . \\n however , existing LLMs still lack factual knowledge and suffer fromvarious spurious correlations . \\n we believe this is a critical bottleneck for realizingtrustworthy artificial intelligence. to this end , we aim toexplore the extent and scope of factual knowledge within LLMs by designing thebenchmark Pinocchio. Pinocchio contains 20K diverse factual questions that spandifferent sources , timelines , domains , regions , and languages . \\n furthermore , weinvestigate whether LLMs can compose multiple facts , update factual knowledgetemporally , reason over multiple pieces of facts , identify subtle factual differences ,and resist adversarial examples . \\n extensive experiments on different sizes and typesof LLMs show that existing LLMs still lack factual knowledge and suffer fromvarious spurious correlations . \\n we believe this is a critical bottleneck for realizingtrustworthy artificial intelligence . \",\n          \" we present a prompt engineering and inference method for text sanitization . \\n our method includes three steps : ( 1 ) the embedding of each token will be perturbedand projected into the original embedding space . \\n this step can be extended to other sanitizationmethods (Utpala et al., 2023; Feyisetan et al., 2020 ) . \\n ( 2 ) we use the method to tune prompts on thesesamples . \\n ( 3 ) we use the generated prompts for inference . \\n we show that ourmethod can outperform private prompt tuning algorithms significantly on three datasets and has similar performance as private prompt tuning algorithms on Disaster .    \\n sanitization , prompt engineering , inference .    \\n sanitization , prompt engineering , inference .    \\n sanitization , prompt engineering , inference .    \\n sanitization , prompt engineering , inference .    \\n sanitization , prompt engineering , inference .    \\n sanitization , prompt engineering , inference .    \\n sanitization , prompt engineering , inference .    \\n sanitization , prompt engineering , inference .    \"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "source": [
        "from google.colab import sheets\n",
        "sheet = sheets.InteractiveSheet(df=all_outputs)"
      ],
      "cell_type": "code",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "https://docs.google.com/spreadsheets/d/1dfLVQ1e9nKsmY-mDgrfUGbxT3goDwGK7t-aOHaazA9M/edit#gid=0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.lib.display.IFrame at 0x7dac54563650>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"100%\"\n",
              "            height=\"600\"\n",
              "            src=\"https://docs.google.com/spreadsheets/d/1dfLVQ1e9nKsmY-mDgrfUGbxT3goDwGK7t-aOHaazA9M/edit?rm=embedded#gid=0\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ]
          },
          "metadata": {}
        }
      ],
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 638
        },
        "id": "BOOTFWzjZE0S",
        "outputId": "19a25658-8252-4f4c-a939-b812ec1696dd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save all_outputs to a csv file\n",
        "all_outputs.to_csv(\"all_outputs.csv\", index=False)"
      ],
      "metadata": {
        "id": "38eyR5-TWKD3"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(all_outputs)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wkZchrpccwQV",
        "outputId": "4ce26892-aaf2-4ecd-a480-cdbfd8305b1d"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "11"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print len of all outputs of all article\n",
        "for i in range(len(all_outputs)):\n",
        "    row = all_outputs.iloc[i]\n",
        "    print(len(row[\"output 1\"]), len(row[\"output 2\"]), len(row[\"output 3\"]), len(row[\"output 4\"]), len(row[\"output 5\"]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49wcc0nrcRH7",
        "outputId": "2740d968-5082-4c55-aa08-a4841f54eebf"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1061 1136 1052 1119 1057\n",
            "593 539 720 538 539\n",
            "1418 1407 1422 1251 1249\n",
            "1050 1056 1037 1049 1048\n",
            "1258 1258 1245 1258 1276\n",
            "521 960 1320 521 521\n",
            "924 1040 926 902 923\n",
            "876 900 917 873 898\n",
            "700 1085 1006 699 1086\n",
            "810 1793 658 1272 1044\n",
            "1232 2257 1104 1101 1646\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kh86ehRMcj_j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}