{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ad01ffa3ba1c4ed585c8ec38a52b23fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8b5b851ba6914562aa81a8bb835b81b3",
              "IPY_MODEL_1af018b17d694b0cb3569218d0b9d9e9",
              "IPY_MODEL_21a00d981afe4f03b6c6ae7787d6424f"
            ],
            "layout": "IPY_MODEL_1af4ba072adb4ba58567065b215615fd"
          }
        },
        "8b5b851ba6914562aa81a8bb835b81b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_560552191ba74b0db088315938bbd2b1",
            "placeholder": "​",
            "style": "IPY_MODEL_192509cbec0646afa047d5bcfa9c0de0",
            "value": "config.json: 100%"
          }
        },
        "1af018b17d694b0cb3569218d0b9d9e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8dad47f4f2d548509ee9641068ede0f9",
            "max": 1217,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_adfa3d714d634f9ea73bb5129268786a",
            "value": 1217
          }
        },
        "21a00d981afe4f03b6c6ae7787d6424f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_15447d94b41d4a4b92ca7fe3ea838828",
            "placeholder": "​",
            "style": "IPY_MODEL_f2ad4950a1b1408cb7782c86ae809e4b",
            "value": " 1.22k/1.22k [00:00&lt;00:00, 45.3kB/s]"
          }
        },
        "1af4ba072adb4ba58567065b215615fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "560552191ba74b0db088315938bbd2b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "192509cbec0646afa047d5bcfa9c0de0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8dad47f4f2d548509ee9641068ede0f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "adfa3d714d634f9ea73bb5129268786a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "15447d94b41d4a4b92ca7fe3ea838828": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f2ad4950a1b1408cb7782c86ae809e4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "256881f5070b47009a95b67aacdc7060": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e2e1881b3e894fc9a33da42dd9452526",
              "IPY_MODEL_72bec25b81eb4d5399c71dbca04e5f0e",
              "IPY_MODEL_52c1d9320b0346e382532ebaf87454ef"
            ],
            "layout": "IPY_MODEL_9719794151f447fd90583a58929cb12e"
          }
        },
        "e2e1881b3e894fc9a33da42dd9452526": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_09d5158e6a5c490587662b5aca324f82",
            "placeholder": "​",
            "style": "IPY_MODEL_a47a0664b8ac47cd91e71530e0033ac1",
            "value": "pytorch_model.bin: 100%"
          }
        },
        "72bec25b81eb4d5399c71dbca04e5f0e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8ceb19c84928470d880d4170e3aa33cd",
            "max": 1839633783,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2c30af4128d649ddbb6120efb2a6cc0a",
            "value": 1839633783
          }
        },
        "52c1d9320b0346e382532ebaf87454ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_102670aa03254dc9a47ef88673d24927",
            "placeholder": "​",
            "style": "IPY_MODEL_ba61e93894664b169bf42f41e5234280",
            "value": " 1.84G/1.84G [00:18&lt;00:00, 182MB/s]"
          }
        },
        "9719794151f447fd90583a58929cb12e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "09d5158e6a5c490587662b5aca324f82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a47a0664b8ac47cd91e71530e0033ac1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8ceb19c84928470d880d4170e3aa33cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c30af4128d649ddbb6120efb2a6cc0a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "102670aa03254dc9a47ef88673d24927": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba61e93894664b169bf42f41e5234280": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "66a04780fada4ca49b839c8eedccf15d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1236f8b1ad184119bff47839c574ab70",
              "IPY_MODEL_2708ae7a2357456da8e4187720b0da92",
              "IPY_MODEL_1f8fbebc354d4836b694125d1420f443"
            ],
            "layout": "IPY_MODEL_f1394de697b54aa2a291c6dec8c5875e"
          }
        },
        "1236f8b1ad184119bff47839c574ab70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d240d648a2e74497afdcb64a1a50175c",
            "placeholder": "​",
            "style": "IPY_MODEL_c907dff1ebbb453fb333ebf395b49df6",
            "value": "model.safetensors: 100%"
          }
        },
        "2708ae7a2357456da8e4187720b0da92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8d1cb0c46cd4d419a3d2a50a029947e",
            "max": 1839478370,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_93022375dd094dd08019da1a03e3f181",
            "value": 1839478370
          }
        },
        "1f8fbebc354d4836b694125d1420f443": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_637cc7c0542d43a8a12bc945c469c295",
            "placeholder": "​",
            "style": "IPY_MODEL_ef381ca38d874dae8e6ee59498a72581",
            "value": " 1.84G/1.84G [00:41&lt;00:00, 184MB/s]"
          }
        },
        "f1394de697b54aa2a291c6dec8c5875e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d240d648a2e74497afdcb64a1a50175c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c907dff1ebbb453fb333ebf395b49df6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c8d1cb0c46cd4d419a3d2a50a029947e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "93022375dd094dd08019da1a03e3f181": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "637cc7c0542d43a8a12bc945c469c295": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ef381ca38d874dae8e6ee59498a72581": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "91acf3c80a7b4994af9be397cacfd0c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5a7f139189ed41ec97cdcc904ded5621",
              "IPY_MODEL_d7d9b7e9d4bd4b478c1826d792e1d405",
              "IPY_MODEL_b186cc7c2bc64245ae9a5dbaea1c3ad9"
            ],
            "layout": "IPY_MODEL_b9dff3b55b2e42439405c90e6f8b0044"
          }
        },
        "5a7f139189ed41ec97cdcc904ded5621": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_83c600523258471c98f256d67cd43ac1",
            "placeholder": "​",
            "style": "IPY_MODEL_7b5c1ac1616d4b0687990a0d9d48f8f5",
            "value": "generation_config.json: 100%"
          }
        },
        "d7d9b7e9d4bd4b478c1826d792e1d405": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48c5deee56354678af0d8997ae273532",
            "max": 168,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7cdda0462be84b16b10c7dafd23d236c",
            "value": 168
          }
        },
        "b186cc7c2bc64245ae9a5dbaea1c3ad9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee9651d3cfd049fa90f1aa0341f50478",
            "placeholder": "​",
            "style": "IPY_MODEL_1f77738e64724e8ca1895a1c97354e18",
            "value": " 168/168 [00:00&lt;00:00, 2.02kB/s]"
          }
        },
        "b9dff3b55b2e42439405c90e6f8b0044": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83c600523258471c98f256d67cd43ac1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b5c1ac1616d4b0687990a0d9d48f8f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "48c5deee56354678af0d8997ae273532": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7cdda0462be84b16b10c7dafd23d236c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ee9651d3cfd049fa90f1aa0341f50478": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1f77738e64724e8ca1895a1c97354e18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "adapter_path = \"loraa\""
      ],
      "metadata": {
        "id": "v_66q4vKAqiJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install peft\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OJdwIB_DCAzz",
        "outputId": "49d69c4e-8b88-4f39-fef3-5c91d20da9b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from peft) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from peft) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from peft) (6.0.2)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from peft) (4.51.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from peft) (4.67.1)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.5.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from peft) (0.5.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from peft) (0.30.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (2025.3.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.25.0->peft) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.13.0->peft)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->peft) (0.21.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.25.0->peft) (2025.1.31)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m108.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m52.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install required package\n",
        "\n",
        "from transformers import AutoModelForSeq2SeqLM\n",
        "from peft import PeftModel\n",
        "import torch\n",
        "\n",
        "# Configuration\n",
        "base_model = \"allenai/led-large-16384\"  # Replace with actual model name/path\n",
        "lora_adapter = \"loraa\"     # Replace with LoRA adapter directory\n",
        "merged_model_path = \"./merged_model\"        # Output directory for merged model\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# Load and merge models\n",
        "base_model = AutoModelForSeq2SeqLM.from_pretrained(base_model).to(device)\n",
        "model_to_merge = PeftModel.from_pretrained(base_model, lora_adapter)\n",
        "merged_model = model_to_merge.merge_and_unload()\n",
        "\n",
        "# Save merged model\n",
        "merged_model.save_pretrained(merged_model_path)\n",
        "print(f\"Successfully saved merged model to: {merged_model_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162,
          "referenced_widgets": [
            "ad01ffa3ba1c4ed585c8ec38a52b23fe",
            "8b5b851ba6914562aa81a8bb835b81b3",
            "1af018b17d694b0cb3569218d0b9d9e9",
            "21a00d981afe4f03b6c6ae7787d6424f",
            "1af4ba072adb4ba58567065b215615fd",
            "560552191ba74b0db088315938bbd2b1",
            "192509cbec0646afa047d5bcfa9c0de0",
            "8dad47f4f2d548509ee9641068ede0f9",
            "adfa3d714d634f9ea73bb5129268786a",
            "15447d94b41d4a4b92ca7fe3ea838828",
            "f2ad4950a1b1408cb7782c86ae809e4b",
            "256881f5070b47009a95b67aacdc7060",
            "e2e1881b3e894fc9a33da42dd9452526",
            "72bec25b81eb4d5399c71dbca04e5f0e",
            "52c1d9320b0346e382532ebaf87454ef",
            "9719794151f447fd90583a58929cb12e",
            "09d5158e6a5c490587662b5aca324f82",
            "a47a0664b8ac47cd91e71530e0033ac1",
            "8ceb19c84928470d880d4170e3aa33cd",
            "2c30af4128d649ddbb6120efb2a6cc0a",
            "102670aa03254dc9a47ef88673d24927",
            "ba61e93894664b169bf42f41e5234280",
            "66a04780fada4ca49b839c8eedccf15d",
            "1236f8b1ad184119bff47839c574ab70",
            "2708ae7a2357456da8e4187720b0da92",
            "1f8fbebc354d4836b694125d1420f443",
            "f1394de697b54aa2a291c6dec8c5875e",
            "d240d648a2e74497afdcb64a1a50175c",
            "c907dff1ebbb453fb333ebf395b49df6",
            "c8d1cb0c46cd4d419a3d2a50a029947e",
            "93022375dd094dd08019da1a03e3f181",
            "637cc7c0542d43a8a12bc945c469c295",
            "ef381ca38d874dae8e6ee59498a72581",
            "91acf3c80a7b4994af9be397cacfd0c2",
            "5a7f139189ed41ec97cdcc904ded5621",
            "d7d9b7e9d4bd4b478c1826d792e1d405",
            "b186cc7c2bc64245ae9a5dbaea1c3ad9",
            "b9dff3b55b2e42439405c90e6f8b0044",
            "83c600523258471c98f256d67cd43ac1",
            "7b5c1ac1616d4b0687990a0d9d48f8f5",
            "48c5deee56354678af0d8997ae273532",
            "7cdda0462be84b16b10c7dafd23d236c",
            "ee9651d3cfd049fa90f1aa0341f50478",
            "1f77738e64724e8ca1895a1c97354e18"
          ]
        },
        "id": "SWN0EeA2BfzW",
        "outputId": "c3ad8553-7701-450b-f269-6208b69e7575"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/1.22k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ad01ffa3ba1c4ed585c8ec38a52b23fe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/1.84G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "256881f5070b47009a95b67aacdc7060"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.84G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "66a04780fada4ca49b839c8eedccf15d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/168 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "91acf3c80a7b4994af9be397cacfd0c2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully saved merged model to: ./merged_model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### quantize merged model"
      ],
      "metadata": {
        "id": "TaYx8AroDNFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install bitsandbytes"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cqacyx75D_W2",
        "outputId": "6d69b8e9-341f-4d7e-a1ee-9bac5eed11e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: torch<3,>=2.0 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.0->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.0->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.0->bitsandbytes) (3.0.2)\n",
            "Downloading bitsandbytes-0.45.5-py3-none-manylinux_2_24_x86_64.whl (76.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.1/76.1 MB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: bitsandbytes\n",
            "Successfully installed bitsandbytes-0.45.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import torch\n",
        "import time\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def quantize_model(model_path, quantized_model_path, quantization_type=\"fp16\", device=\"cpu\"):\n",
        "    \"\"\"Quantize a model using specified quantization type\"\"\"\n",
        "    print(f\"Loading model for quantization from: {model_path}\")\n",
        "\n",
        "    # For int8/int4 quantization using bitsandbytes\n",
        "    if quantization_type in [\"int8\", \"int4\"]:\n",
        "        try:\n",
        "            print(f\"Attempting to load model with {quantization_type} quantization...\")\n",
        "\n",
        "            # Check if bitsandbytes is available\n",
        "            import bitsandbytes as bnb\n",
        "            from transformers import BitsAndBytesConfig\n",
        "\n",
        "            # Configure quantization based on type\n",
        "            if quantization_type == \"int8\":\n",
        "                quantization_config = BitsAndBytesConfig(\n",
        "                    load_in_8bit=True,\n",
        "                    bnb_8bit_compute_dtype=torch.float16\n",
        "                )\n",
        "            else:  # int4\n",
        "                quantization_config = BitsAndBytesConfig(\n",
        "                    load_in_4bit=True,\n",
        "                    bnb_4bit_quant_type=\"nf4\",\n",
        "                    bnb_4bit_compute_dtype=torch.float16\n",
        "                )\n",
        "\n",
        "            # Load directly with quantization config\n",
        "            model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "                model_path,\n",
        "                quantization_config=quantization_config,\n",
        "                device_map=\"auto\" if device == \"cuda\" else None\n",
        "            )\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "\n",
        "            print(f\"Successfully loaded model with {quantization_type} quantization\")\n",
        "\n",
        "        except ImportError:\n",
        "            print(f\"bitsandbytes library not found. Falling back to fp16 quantization.\")\n",
        "            # Load model normally and convert to fp16\n",
        "            model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "            model = model.half()  # Convert to fp16\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error during {quantization_type} quantization: {e}\")\n",
        "            print(\"Falling back to fp16 quantization\")\n",
        "            # Load model normally and convert to fp16\n",
        "            model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "            model = model.half()  # Convert to fp16\n",
        "\n",
        "    # For fp16 quantization (half precision)\n",
        "    elif quantization_type == \"fp16\":\n",
        "        print(\"Loading model for fp16 quantization...\")\n",
        "        model = AutoModelForSeq2SeqLM.from_pretrained(model_path)\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "        model = model.half()  # Convert to fp16\n",
        "        print(\"Successfully converted model to fp16\")\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported quantization type: {quantization_type}\")\n",
        "\n",
        "    # Save the quantized model\n",
        "    os.makedirs(quantized_model_path, exist_ok=True)\n",
        "    print(f\"Saving quantized model to: {quantized_model_path}\")\n",
        "    model.save_pretrained(quantized_model_path)\n",
        "    tokenizer.save_pretrained(quantized_model_path)\n",
        "\n",
        "    print(f\"Quantized model saved to: {quantized_model_path}\")\n",
        "    return model, tokenizer\n",
        "\n",
        "def run_inference(model, tokenizer, input_texts, device=\"cpu\", batch_size=1):\n",
        "    \"\"\"Run inference on a list of input texts and measure performance\"\"\"\n",
        "    # Move model to the specified device if not already there\n",
        "    if device == \"cuda\" and next(model.parameters()).device.type != \"cuda\":\n",
        "        model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    total_tokens = 0\n",
        "    inference_times = []\n",
        "\n",
        "    # Process inputs in batches\n",
        "    for i in range(0, len(input_texts), batch_size):\n",
        "        batch = input_texts[i:i+batch_size]\n",
        "\n",
        "        # Tokenize inputs\n",
        "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True)\n",
        "\n",
        "        # Move inputs to device\n",
        "        if device == \"cuda\":\n",
        "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "        input_token_count = inputs[\"input_ids\"].numel()\n",
        "        total_tokens += input_token_count\n",
        "\n",
        "        # Measure inference time\n",
        "        if device == \"cuda\":\n",
        "            torch.cuda.synchronize()\n",
        "        start_time = time.time()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(**inputs, max_length=128)\n",
        "\n",
        "        if device == \"cuda\":\n",
        "            torch.cuda.synchronize()\n",
        "        end_time = time.time()\n",
        "\n",
        "        # Convert model outputs to text\n",
        "        decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "        # Record inference time\n",
        "        inference_time = end_time - start_time\n",
        "        inference_times.append(inference_time)\n",
        "\n",
        "        print(f\"Batch {i//batch_size + 1} inference time: {inference_time:.4f} seconds\")\n",
        "        print(f\"Sample output: {decoded_outputs[0][:100]}...\")\n",
        "\n",
        "    # Calculate statistics\n",
        "    avg_time = np.mean(inference_times) if inference_times else 0\n",
        "    tokens_per_second = total_tokens / sum(inference_times) if sum(inference_times) > 0 else 0\n",
        "\n",
        "    print(\"\\nInference Performance Summary:\")\n",
        "    print(f\"Average inference time: {avg_time:.4f} seconds per batch\")\n",
        "    print(f\"Tokens per second: {tokens_per_second:.2f}\")\n",
        "\n",
        "    return {\n",
        "        \"avg_inference_time\": avg_time,\n",
        "        \"tokens_per_second\": tokens_per_second,\n",
        "        \"total_tokens\": total_tokens,\n",
        "        \"device\": device\n",
        "    }\n",
        "\n",
        "def compare_model_sizes(original_path, quantized_path):\n",
        "    \"\"\"Compare the size of original and quantized models on disk\"\"\"\n",
        "    def get_dir_size(path):\n",
        "        total_size = 0\n",
        "        for dirpath, dirnames, filenames in os.walk(path):\n",
        "            for f in filenames:\n",
        "                fp = os.path.join(dirpath, f)\n",
        "                try:\n",
        "                    total_size += os.path.getsize(fp)\n",
        "                except OSError as e:\n",
        "                    print(f\"Error accessing {fp}: {e}\")\n",
        "        return total_size / (1024 * 1024)  # Convert to MB\n",
        "\n",
        "    original_size = get_dir_size(original_path)\n",
        "    quantized_size = get_dir_size(quantized_path)\n",
        "\n",
        "    print(f\"\\nModel Size Comparison:\")\n",
        "    print(f\"Original model size: {original_size:.2f} MB\")\n",
        "    print(f\"Quantized model size: {quantized_size:.2f} MB\")\n",
        "\n",
        "    if original_size > 0:\n",
        "        reduction_percentage = (1 - quantized_size/original_size) * 100\n",
        "        print(f\"Size reduction: {reduction_percentage:.2f}%\")\n",
        "    else:\n",
        "        reduction_percentage = 0\n",
        "        print(\"Could not calculate size reduction percentage (original size is 0)\")\n",
        "\n",
        "    return {\n",
        "        \"original_size_mb\": original_size,\n",
        "        \"quantized_size_mb\": quantized_size,\n",
        "        \"reduction_percentage\": reduction_percentage\n",
        "    }\n",
        "\n",
        "def load_or_merge_model(base_model_name, lora_adapter_path, merged_model_path):\n",
        "    \"\"\"Load a pre-merged model or merge base model with LoRA adapter\"\"\"\n",
        "    print(f\"Loading or merging model...\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "\n",
        "    # Check if merged model already exists\n",
        "    if os.path.exists(merged_model_path) and os.path.isfile(os.path.join(merged_model_path, \"pytorch_model.bin\")):\n",
        "        print(f\"Loading pre-merged model from: {merged_model_path}\")\n",
        "        try:\n",
        "            model = AutoModelForSeq2SeqLM.from_pretrained(merged_model_path)\n",
        "            return model, tokenizer\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading pre-merged model: {e}\")\n",
        "            print(\"Will attempt to merge models again...\")\n",
        "\n",
        "    # Load base model\n",
        "    print(f\"Loading base model: {base_model_name}\")\n",
        "    try:\n",
        "        base_model = AutoModelForSeq2SeqLM.from_pretrained(base_model_name)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading base model: {e}\")\n",
        "        raise\n",
        "\n",
        "    # Check if LoRA adapter exists and attempt to merge\n",
        "    if os.path.exists(lora_adapter_path):\n",
        "        print(f\"Loading LoRA adapter: {lora_adapter_path}\")\n",
        "        try:\n",
        "            # Import PEFT for LoRA\n",
        "            from peft import PeftModel\n",
        "\n",
        "            # Load and merge\n",
        "            model = PeftModel.from_pretrained(base_model, lora_adapter_path)\n",
        "            print(\"Merging model with adapter...\")\n",
        "            merged_model = model.merge_and_unload()\n",
        "\n",
        "            # Save the merged model\n",
        "            os.makedirs(merged_model_path, exist_ok=True)\n",
        "            print(f\"Saving merged model to: {merged_model_path}\")\n",
        "            merged_model.save_pretrained(merged_model_path)\n",
        "            tokenizer.save_pretrained(merged_model_path)\n",
        "            print(f\"Merged model saved to: {merged_model_path}\")\n",
        "\n",
        "            return merged_model, tokenizer\n",
        "        except Exception as e:\n",
        "            print(f\"Error merging models: {e}\")\n",
        "            print(\"Continuing with base model...\")\n",
        "    else:\n",
        "        print(f\"LoRA adapter path {lora_adapter_path} does not exist, using base model\")\n",
        "\n",
        "    # If we reach here, use the base model\n",
        "    os.makedirs(merged_model_path, exist_ok=True)\n",
        "    print(f\"Saving base model to: {merged_model_path}\")\n",
        "    base_model.save_pretrained(merged_model_path)\n",
        "    tokenizer.save_pretrained(merged_model_path)\n",
        "\n",
        "    return base_model, tokenizer\n",
        "\n",
        "def main():\n",
        "    # Configuration\n",
        "    base_model = \"allenai/led-large-16384\"  # Base model on HuggingFace\n",
        "    lora_adapter = \"loraa\"                  # Path to LoRA adapter\n",
        "    merged_model_path = \"./merged_model\"    # Output directory for merged model\n",
        "    quantized_model_path = \"./quantized_model\"  # Output directory for quantized model\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Step 1: Load and merge models\n",
        "    merged_model, tokenizer = load_or_merge_model(base_model, lora_adapter, merged_model_path)\n",
        "\n",
        "    # Step 2: Quantize the merged model\n",
        "    # Options: \"fp16\" (most compatible), \"int8\" or \"int4\" (require bitsandbytes)\n",
        "    # Choose the most appropriate quantization type based on your requirements\n",
        "    quantized_model, tokenizer = quantize_model(\n",
        "        merged_model_path, quantized_model_path, quantization_type=\"fp16\", device=device\n",
        "    )\n",
        "\n",
        "    # Step 3: Compare model sizes\n",
        "    size_comparison = compare_model_sizes(merged_model_path, quantized_model_path)\n",
        "\n",
        "    # Step 4: Run inference tests on both original and quantized models\n",
        "    print(\"\\n=== Running inference test on original (merged) model ===\")\n",
        "    test_inputs = [\n",
        "        \"This is a test input for the model to summarize: \" + \" \".join([\"content\"] * 100),\n",
        "        \"Another test input with different content to process: \" + \" \".join([\"text\"] * 100)\n",
        "    ]\n",
        "\n",
        "    # Run inference on merged model\n",
        "    original_perf = run_inference(merged_model, tokenizer, test_inputs, device, batch_size=1)\n",
        "\n",
        "    print(\"\\n=== Running inference test on quantized model ===\")\n",
        "    # Run inference on quantized model\n",
        "    quantized_perf = run_inference(quantized_model, tokenizer, test_inputs, device, batch_size=1)\n",
        "\n",
        "    # Step 5: Report performance comparison\n",
        "    if original_perf[\"avg_inference_time\"] > 0:\n",
        "        time_improvement = (original_perf[\"avg_inference_time\"] - quantized_perf[\"avg_inference_time\"]) / original_perf[\"avg_inference_time\"] * 100\n",
        "    else:\n",
        "        time_improvement = 0\n",
        "\n",
        "    if original_perf[\"tokens_per_second\"] > 0:\n",
        "        throughput_improvement = (quantized_perf[\"tokens_per_second\"] - original_perf[\"tokens_per_second\"]) / original_perf[\"tokens_per_second\"] * 100\n",
        "    else:\n",
        "        throughput_improvement = 0\n",
        "\n",
        "    print(\"\\n=== Performance Comparison ===\")\n",
        "    print(f\"Inference time improvement: {time_improvement:.2f}%\")\n",
        "    print(f\"Throughput improvement: {throughput_improvement:.2f}%\")\n",
        "    print(f\"Size reduction: {size_comparison['reduction_percentage']:.2f}%\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t4mkEjr7DdRf",
        "outputId": "c57c3214-4a27-48af-98f4-6dc21193837e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cpu\n",
            "Loading or merging model...\n",
            "Loading base model: allenai/led-large-16384\n",
            "Loading LoRA adapter: loraa\n",
            "Merging model with adapter...\n",
            "Saving merged model to: ./merged_model\n",
            "Merged model saved to: ./merged_model\n",
            "Loading model for quantization from: ./merged_model\n",
            "Loading model for fp16 quantization...\n",
            "Successfully converted model to fp16\n",
            "Saving quantized model to: ./quantized_model\n",
            "Quantized model saved to: ./quantized_model\n",
            "\n",
            "Model Size Comparison:\n",
            "Original model size: 1758.86 MB\n",
            "Quantized model size: 881.76 MB\n",
            "Size reduction: 49.87%\n",
            "\n",
            "=== Running inference test on original (merged) model ===\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Input ids are automatically padded from 113 to 1024 to be a multiple of `config.attention_window`: 1024\n",
            "Input ids are automatically padded from 111 to 1024 to be a multiple of `config.attention_window`: 1024\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 1 inference time: 30.5619 seconds\n",
            "Sample output: ThisThis is a test input for the model to summarize: content content content content content content...\n",
            "Batch 2 inference time: 28.5985 seconds\n",
            "Sample output: AnotherAnotherAnotherAnotherAnotherAnotherAnotherAnotherAnotherAnotherAnotherAnotherAnotherAnotherAn...\n",
            "\n",
            "Inference Performance Summary:\n",
            "Average inference time: 29.5802 seconds per batch\n",
            "Tokens per second: 3.79\n",
            "\n",
            "=== Running inference test on quantized model ===\n",
            "Batch 1 inference time: 100.0436 seconds\n",
            "Sample output: ThisThis is a test input for the model to summarize: content content content content content content...\n",
            "Batch 2 inference time: 97.5030 seconds\n",
            "Sample output: AnotherAnotherAnotherAnotherAnotherAnotherAnotherAnotherAnotherAnotherAnotherAnotherAnotherAnotherAn...\n",
            "\n",
            "Inference Performance Summary:\n",
            "Average inference time: 98.7733 seconds per batch\n",
            "Tokens per second: 1.13\n",
            "\n",
            "=== Performance Comparison ===\n",
            "Inference time improvement: -233.92%\n",
            "Throughput improvement: -70.05%\n",
            "Size reduction: 49.87%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "import torch\n",
        "import time\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def run_inference_comparison(merged_model_path, quantized_model_path):\n",
        "    \"\"\"Run inference comparison on both original and quantized models using GPU\"\"\"\n",
        "\n",
        "    # Check for GPU availability\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"CUDA is not available. Cannot run GPU inference comparison.\")\n",
        "        return\n",
        "\n",
        "    device = \"cuda\"\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load models and tokenizer\n",
        "    print(f\"Loading original model from: {merged_model_path}\")\n",
        "    original_model = AutoModelForSeq2SeqLM.from_pretrained(merged_model_path)\n",
        "    original_model = original_model.to(device)\n",
        "\n",
        "    print(f\"Loading quantized model from: {quantized_model_path}\")\n",
        "    quantized_model = AutoModelForSeq2SeqLM.from_pretrained(quantized_model_path)\n",
        "    quantized_model = quantized_model.to(device)\n",
        "\n",
        "    print(f\"Loading tokenizer\")\n",
        "    tokenizer = AutoTokenizer.from_pretrained(merged_model_path)\n",
        "\n",
        "    # Prepare test inputs\n",
        "    test_inputs = [\n",
        "        \"This is a test input for the model to summarize: \" + \" \".join([\"content\"] * 100),\n",
        "        \"Another test input with different content to process: \" + \" \".join([\"text\"] * 100)\n",
        "    ]\n",
        "\n",
        "    # Run inference tests\n",
        "    print(\"\\n=== Running inference test on original model ===\")\n",
        "    original_perf = run_inference(original_model, tokenizer, test_inputs, device, batch_size=1)\n",
        "\n",
        "    print(\"\\n=== Running inference test on quantized model ===\")\n",
        "    quantized_perf = run_inference(quantized_model, tokenizer, test_inputs, device, batch_size=1)\n",
        "\n",
        "    # Report performance comparison\n",
        "    if original_perf[\"avg_inference_time\"] > 0:\n",
        "        time_improvement = (original_perf[\"avg_inference_time\"] - quantized_perf[\"avg_inference_time\"]) / original_perf[\"avg_inference_time\"] * 100\n",
        "    else:\n",
        "        time_improvement = 0\n",
        "\n",
        "    if original_perf[\"tokens_per_second\"] > 0:\n",
        "        throughput_improvement = (quantized_perf[\"tokens_per_second\"] - original_perf[\"tokens_per_second\"]) / original_perf[\"tokens_per_second\"] * 100\n",
        "    else:\n",
        "        throughput_improvement = 0\n",
        "\n",
        "    # Calculate memory usage\n",
        "    with torch.cuda.device(0):\n",
        "        original_memory = torch.cuda.memory_allocated() / (1024 * 1024)  # Convert to MB\n",
        "\n",
        "    # Move original model to CPU to free GPU memory\n",
        "    original_model = original_model.to('cpu')\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    # Calculate memory usage of quantized model\n",
        "    with torch.cuda.device(0):\n",
        "        quantized_memory = torch.cuda.memory_allocated() / (1024 * 1024)  # Convert to MB\n",
        "\n",
        "    memory_reduction = (1 - quantized_memory/original_memory) * 100 if original_memory > 0 else 0\n",
        "\n",
        "    print(\"\\n=== Performance Comparison ===\")\n",
        "    print(f\"Inference time improvement: {time_improvement:.2f}%\")\n",
        "    print(f\"Throughput improvement: {throughput_improvement:.2f}%\")\n",
        "    print(f\"GPU Memory usage (original): {original_memory:.2f} MB\")\n",
        "    print(f\"GPU Memory usage (quantized): {quantized_memory:.2f} MB\")\n",
        "    print(f\"Memory reduction: {memory_reduction:.2f}%\")\n",
        "\n",
        "    return {\n",
        "        \"original_perf\": original_perf,\n",
        "        \"quantized_perf\": quantized_perf,\n",
        "        \"time_improvement\": time_improvement,\n",
        "        \"throughput_improvement\": throughput_improvement,\n",
        "        \"memory_reduction\": memory_reduction\n",
        "    }\n",
        "\n",
        "def run_inference(model, tokenizer, input_texts, device=\"cuda\", batch_size=1):\n",
        "    \"\"\"Run inference on a list of input texts and measure performance\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    total_tokens = 0\n",
        "    inference_times = []\n",
        "\n",
        "    # Warm-up run to avoid cold-start effects\n",
        "    print(\"Performing warm-up run...\")\n",
        "    with torch.no_grad():\n",
        "        warm_input = tokenizer(\"Warm-up text\", return_tensors=\"pt\").to(device)\n",
        "        model.generate(**warm_input, max_length=20)\n",
        "\n",
        "    # Process inputs in batches\n",
        "    for i in range(0, len(input_texts), batch_size):\n",
        "        batch = input_texts[i:i+batch_size]\n",
        "\n",
        "        # Tokenize inputs\n",
        "        inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "        input_token_count = inputs.input_ids.numel()\n",
        "        total_tokens += input_token_count\n",
        "\n",
        "        # Measure inference time\n",
        "        torch.cuda.synchronize()\n",
        "        start_time = time.time()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(**inputs, max_length=128)\n",
        "\n",
        "        torch.cuda.synchronize()\n",
        "        end_time = time.time()\n",
        "\n",
        "        # Convert model outputs to text\n",
        "        decoded_outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "        # Record inference time\n",
        "        inference_time = end_time - start_time\n",
        "        inference_times.append(inference_time)\n",
        "\n",
        "        print(f\"Batch {i//batch_size + 1} inference time: {inference_time:.4f} seconds\")\n",
        "        print(f\"Sample output: {decoded_outputs[0][:100]}...\")\n",
        "\n",
        "    # Calculate statistics\n",
        "    avg_time = np.mean(inference_times) if inference_times else 0\n",
        "    tokens_per_second = total_tokens / sum(inference_times) if sum(inference_times) > 0 else 0\n",
        "\n",
        "    print(\"\\nInference Performance Summary:\")\n",
        "    print(f\"Average inference time: {avg_time:.4f} seconds per batch\")\n",
        "    print(f\"Tokens per second: {tokens_per_second:.2f}\")\n",
        "\n",
        "    return {\n",
        "        \"avg_inference_time\": avg_time,\n",
        "        \"tokens_per_second\": tokens_per_second,\n",
        "        \"total_tokens\": total_tokens,\n",
        "        \"device\": device\n",
        "    }\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Paths to the models\n",
        "    merged_model_path = \"./merged_model\"    # Path to original merged model\n",
        "    quantized_model_path = \"./quantized_model\"  # Path to quantized model\n",
        "\n",
        "    # Run the comparison\n",
        "    run_inference_comparison(merged_model_path, quantized_model_path)"
      ],
      "metadata": {
        "id": "VgxQiH2SHkp2",
        "outputId": "7ed7624d-2ee7-4909-cff6-f8406fa4e705",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CUDA is not available. Cannot run GPU inference comparison.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### do DPO on merged model"
      ],
      "metadata": {
        "id": "6DOVZs80ELZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: do dpo on merged model\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from trl import DPOTrainer\n",
        "from datasets import load_dataset\n",
        "\n",
        "# ... (rest of your existing code)\n",
        "\n",
        "def dpo_on_merged_model(merged_model_path, output_dir, dataset_name, dataset_config):\n",
        "    \"\"\"Performs DPO on a merged model.\"\"\"\n",
        "\n",
        "    # Load the merged model and tokenizer\n",
        "    model = AutoModelForSeq2SeqLM.from_pretrained(merged_model_path).to(\"cuda\") # or your preferred device\n",
        "    tokenizer = AutoTokenizer.from_pretrained(merged_model_path)\n",
        "\n",
        "    # Load the dataset for DPO training\n",
        "    dataset = load_dataset(dataset_name, dataset_config, split=\"train\")\n",
        "\n",
        "    # Prepare the dataset: Ensure your dataset has \"prompt\", \"chosen\" and \"rejected\" columns\n",
        "    def preprocess_function(examples):\n",
        "      return tokenizer(examples[\"prompt\"], examples[\"chosen\"], examples[\"rejected\"], truncation=True, padding=\"max_length\", max_length=128) # Adjust max_length\n",
        "\n",
        "    processed_dataset = dataset.map(\n",
        "      preprocess_function,\n",
        "      batched=True,\n",
        "      remove_columns=dataset.column_names\n",
        "    )\n",
        "\n",
        "    # Create the DPOTrainer\n",
        "    dpo_trainer = DPOTrainer(\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        args=TrainingArguments(\n",
        "            output_dir=output_dir,\n",
        "            per_device_train_batch_size=4,  # Adjust batch size as needed\n",
        "            gradient_accumulation_steps=4, # Adjust for GPU memory\n",
        "            num_train_epochs=3,           # Adjust number of epochs\n",
        "            save_steps=1000,\n",
        "            logging_steps=100,\n",
        "            learning_rate=5e-5,          # Adjust learning rate\n",
        "            fp16=True,                    # Use FP16 if available\n",
        "            # Add other training arguments as needed\n",
        "        ),\n",
        "        train_dataset=processed_dataset,\n",
        "    )\n",
        "\n",
        "    # Train the model with DPO\n",
        "    dpo_trainer.train()\n",
        "\n",
        "    # Save the fine-tuned model\n",
        "    dpo_trainer.save_model(output_dir)\n",
        "\n",
        "#Example usage\n",
        "dpo_on_merged_model(merged_model_path=\"./merged_model\", output_dir=\"./dpo_model\", dataset_name=\"your_dataset\", dataset_config=\"your_config\")\n"
      ],
      "metadata": {
        "id": "Ug_vQXQcHHHo"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}