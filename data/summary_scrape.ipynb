{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d356a58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting playwright\n",
      "  Downloading playwright-1.51.0-py3-none-macosx_11_0_arm64.whl.metadata (3.5 kB)\n",
      "Collecting pyee<13,>=12 (from playwright)\n",
      "  Downloading pyee-12.1.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting greenlet<4.0.0,>=3.1.1 (from playwright)\n",
      "  Downloading greenlet-3.2.0-cp313-cp313-macosx_11_0_universal2.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: typing-extensions in ./venv/lib/python3.13/site-packages (from pyee<13,>=12->playwright) (4.13.2)\n",
      "Downloading playwright-1.51.0-py3-none-macosx_11_0_arm64.whl (38.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.0/38.0 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading greenlet-3.2.0-cp313-cp313-macosx_11_0_universal2.whl (269 kB)\n",
      "Downloading pyee-12.1.1-py3-none-any.whl (15 kB)\n",
      "Installing collected packages: pyee, greenlet, playwright\n",
      "Successfully installed greenlet-3.2.0 playwright-1.51.0 pyee-12.1.1\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Downloading Chromium 134.0.6998.35 (playwright build v1161)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/chromium/1161/chromium-mac-arm64.zip\u001b[22m\n",
      "\u001b[1G124.1 MiB [                    ] 0% 0.0s\u001b[0K\u001b[1G124.1 MiB [                    ] 0% 52.9s\u001b[0K\u001b[1G124.1 MiB [                    ] 0% 38.7s\u001b[0K\u001b[1G124.1 MiB [                    ] 0% 26.0s\u001b[0K\u001b[1G124.1 MiB [                    ] 0% 16.4s\u001b[0K\u001b[1G124.1 MiB [                    ] 0% 12.3s\u001b[0K\u001b[1G124.1 MiB [                    ] 1% 10.9s\u001b[0K\u001b[1G124.1 MiB [                    ] 1% 9.9s\u001b[0K\u001b[1G124.1 MiB [                    ] 1% 9.6s\u001b[0K\u001b[1G124.1 MiB [                    ] 1% 9.2s\u001b[0K\u001b[1G124.1 MiB [                    ] 1% 8.9s\u001b[0K\u001b[1G124.1 MiB [                    ] 2% 8.5s\u001b[0K\u001b[1G124.1 MiB [                    ] 2% 8.4s\u001b[0K\u001b[1G124.1 MiB [=                   ] 2% 8.3s\u001b[0K\u001b[1G124.1 MiB [=                   ] 2% 8.7s\u001b[0K\u001b[1G124.1 MiB [=                   ] 3% 8.5s\u001b[0K\u001b[1G124.1 MiB [=                   ] 3% 8.2s\u001b[0K\u001b[1G124.1 MiB [=                   ] 3% 7.9s\u001b[0K\u001b[1G124.1 MiB [=                   ] 3% 7.7s\u001b[0K\u001b[1G124.1 MiB [=                   ] 4% 7.5s\u001b[0K\u001b[1G124.1 MiB [=                   ] 4% 7.3s\u001b[0K\u001b[1G124.1 MiB [=                   ] 4% 7.0s\u001b[0K\u001b[1G124.1 MiB [=                   ] 5% 6.9s\u001b[0K\u001b[1G124.1 MiB [=                   ] 5% 7.0s\u001b[0K\u001b[1G124.1 MiB [=                   ] 6% 6.9s\u001b[0K\u001b[1G124.1 MiB [=                   ] 6% 6.8s\u001b[0K\u001b[1G124.1 MiB [=                   ] 6% 6.7s\u001b[0K\u001b[1G124.1 MiB [=                   ] 6% 6.6s\u001b[0K\u001b[1G124.1 MiB [=                   ] 7% 6.5s\u001b[0K\u001b[1G124.1 MiB [==                  ] 7% 6.5s\u001b[0K\u001b[1G124.1 MiB [==                  ] 7% 6.3s\u001b[0K\u001b[1G124.1 MiB [==                  ] 8% 6.3s\u001b[0K\u001b[1G124.1 MiB [==                  ] 8% 6.2s\u001b[0K\u001b[1G124.1 MiB [==                  ] 8% 6.3s\u001b[0K\u001b[1G124.1 MiB [==                  ] 9% 6.2s\u001b[0K\u001b[1G124.1 MiB [==                  ] 9% 6.3s\u001b[0K\u001b[1G124.1 MiB [==                  ] 9% 6.2s\u001b[0K\u001b[1G124.1 MiB [==                  ] 10% 6.0s\u001b[0K\u001b[1G124.1 MiB [==                  ] 10% 5.9s\u001b[0K\u001b[1G124.1 MiB [==                  ] 11% 5.9s\u001b[0K\u001b[1G124.1 MiB [==                  ] 11% 5.8s\u001b[0K\u001b[1G124.1 MiB [==                  ] 12% 5.8s\u001b[0K\u001b[1G124.1 MiB [===                 ] 12% 5.7s\u001b[0K\u001b[1G124.1 MiB [===                 ] 13% 5.7s\u001b[0K\u001b[1G124.1 MiB [===                 ] 13% 5.6s\u001b[0K\u001b[1G124.1 MiB [===                 ] 14% 5.5s\u001b[0K\u001b[1G124.1 MiB [===                 ] 14% 5.4s\u001b[0K\u001b[1G124.1 MiB [===                 ] 15% 5.4s\u001b[0K\u001b[1G124.1 MiB [===                 ] 15% 5.3s\u001b[0K\u001b[1G124.1 MiB [===                 ] 16% 5.3s\u001b[0K\u001b[1G124.1 MiB [===                 ] 16% 5.2s\u001b[0K\u001b[1G124.1 MiB [===                 ] 17% 5.2s\u001b[0K\u001b[1G124.1 MiB [====                ] 17% 5.1s\u001b[0K\u001b[1G124.1 MiB [====                ] 17% 6.6s\u001b[0K\u001b[1G124.1 MiB [====                ] 18% 6.5s\u001b[0K\u001b[1G124.1 MiB [====                ] 18% 6.4s\u001b[0K\u001b[1G124.1 MiB [====                ] 19% 6.3s\u001b[0K\u001b[1G124.1 MiB [====                ] 19% 6.2s\u001b[0K\u001b[1G124.1 MiB [====                ] 20% 6.2s\u001b[0K\u001b[1G124.1 MiB [====                ] 20% 6.1s\u001b[0K\u001b[1G124.1 MiB [====                ] 21% 6.0s\u001b[0K\u001b[1G124.1 MiB [====                ] 21% 5.9s\u001b[0K\u001b[1G124.1 MiB [====                ] 22% 5.8s\u001b[0K\u001b[1G124.1 MiB [=====               ] 22% 5.8s\u001b[0K\u001b[1G124.1 MiB [=====               ] 23% 5.7s\u001b[0K\u001b[1G124.1 MiB [=====               ] 23% 5.6s\u001b[0K\u001b[1G124.1 MiB [=====               ] 24% 5.6s\u001b[0K\u001b[1G124.1 MiB [=====               ] 24% 5.5s\u001b[0K\u001b[1G124.1 MiB [=====               ] 25% 5.5s\u001b[0K\u001b[1G124.1 MiB [=====               ] 25% 5.4s\u001b[0K\u001b[1G124.1 MiB [=====               ] 26% 5.4s\u001b[0K\u001b[1G124.1 MiB [=====               ] 26% 5.3s\u001b[0K\u001b[1G124.1 MiB [=====               ] 27% 5.3s\u001b[0K\u001b[1G124.1 MiB [=====               ] 27% 5.2s\u001b[0K\u001b[1G124.1 MiB [======              ] 27% 5.2s\u001b[0K\u001b[1G124.1 MiB [======              ] 28% 5.1s\u001b[0K\u001b[1G124.1 MiB [======              ] 29% 5.0s\u001b[0K\u001b[1G124.1 MiB [======              ] 30% 4.9s\u001b[0K\u001b[1G124.1 MiB [======              ] 30% 4.8s\u001b[0K\u001b[1G124.1 MiB [======              ] 31% 4.7s\u001b[0K\u001b[1G124.1 MiB [======              ] 32% 4.6s\u001b[0K\u001b[1G124.1 MiB [=======             ] 32% 4.5s\u001b[0K\u001b[1G124.1 MiB [=======             ] 33% 4.5s\u001b[0K\u001b[1G124.1 MiB [=======             ] 34% 4.4s\u001b[0K\u001b[1G124.1 MiB [=======             ] 34% 4.3s\u001b[0K\u001b[1G124.1 MiB [=======             ] 35% 4.3s\u001b[0K\u001b[1G124.1 MiB [=======             ] 35% 4.2s\u001b[0K\u001b[1G124.1 MiB [=======             ] 36% 4.2s\u001b[0K\u001b[1G124.1 MiB [=======             ] 36% 4.1s\u001b[0K\u001b[1G124.1 MiB [=======             ] 37% 4.1s\u001b[0K\u001b[1G124.1 MiB [========            ] 37% 4.0s\u001b[0K\u001b[1G124.1 MiB [========            ] 38% 4.0s\u001b[0K\u001b[1G124.1 MiB [========            ] 38% 3.9s\u001b[0K\u001b[1G124.1 MiB [========            ] 39% 3.9s\u001b[0K\u001b[1G124.1 MiB [========            ] 40% 3.8s\u001b[0K\u001b[1G124.1 MiB [========            ] 41% 3.7s\u001b[0K\u001b[1G124.1 MiB [========            ] 42% 3.6s\u001b[0K\u001b[1G124.1 MiB [=========           ] 42% 3.6s\u001b[0K\u001b[1G124.1 MiB [=========           ] 43% 3.6s\u001b[0K\u001b[1G124.1 MiB [=========           ] 43% 3.5s\u001b[0K\u001b[1G124.1 MiB [=========           ] 44% 3.5s\u001b[0K\u001b[1G124.1 MiB [=========           ] 44% 3.4s\u001b[0K\u001b[1G124.1 MiB [=========           ] 45% 3.4s\u001b[0K\u001b[1G124.1 MiB [=========           ] 45% 3.3s\u001b[0K\u001b[1G124.1 MiB [=========           ] 46% 3.3s\u001b[0K\u001b[1G124.1 MiB [=========           ] 47% 3.2s\u001b[0K\u001b[1G124.1 MiB [==========          ] 47% 3.2s\u001b[0K\u001b[1G124.1 MiB [==========          ] 48% 3.2s\u001b[0K\u001b[1G124.1 MiB [==========          ] 48% 3.1s\u001b[0K\u001b[1G124.1 MiB [==========          ] 49% 3.1s\u001b[0K\u001b[1G124.1 MiB [==========          ] 49% 3.0s\u001b[0K\u001b[1G124.1 MiB [==========          ] 50% 3.0s\u001b[0K\u001b[1G124.1 MiB [==========          ] 50% 3.1s\u001b[0K\u001b[1G124.1 MiB [==========          ] 50% 3.0s\u001b[0K\u001b[1G124.1 MiB [==========          ] 51% 3.0s\u001b[0K\u001b[1G124.1 MiB [==========          ] 52% 2.9s\u001b[0K\u001b[1G124.1 MiB [===========         ] 52% 2.9s\u001b[0K\u001b[1G124.1 MiB [===========         ] 53% 2.9s\u001b[0K\u001b[1G124.1 MiB [===========         ] 53% 2.8s\u001b[0K\u001b[1G124.1 MiB [===========         ] 54% 2.8s\u001b[0K\u001b[1G124.1 MiB [===========         ] 54% 2.7s\u001b[0K\u001b[1G124.1 MiB [===========         ] 55% 2.7s\u001b[0K\u001b[1G124.1 MiB [===========         ] 56% 2.7s\u001b[0K\u001b[1G124.1 MiB [===========         ] 56% 2.6s\u001b[0K\u001b[1G124.1 MiB [===========         ] 57% 2.6s\u001b[0K\u001b[1G124.1 MiB [============        ] 57% 2.6s\u001b[0K\u001b[1G124.1 MiB [============        ] 57% 2.7s\u001b[0K\u001b[1G124.1 MiB [============        ] 57% 2.8s\u001b[0K\u001b[1G124.1 MiB [============        ] 57% 2.9s\u001b[0K\u001b[1G124.1 MiB [============        ] 58% 2.9s\u001b[0K\u001b[1G124.1 MiB [============        ] 59% 2.8s\u001b[0K\u001b[1G124.1 MiB [============        ] 60% 2.7s\u001b[0K\u001b[1G124.1 MiB [============        ] 61% 2.7s\u001b[0K\u001b[1G124.1 MiB [============        ] 61% 2.6s\u001b[0K\u001b[1G124.1 MiB [============        ] 62% 2.6s\u001b[0K\u001b[1G124.1 MiB [=============       ] 62% 2.6s\u001b[0K\u001b[1G124.1 MiB [=============       ] 63% 2.5s\u001b[0K\u001b[1G124.1 MiB [=============       ] 64% 2.4s\u001b[0K\u001b[1G124.1 MiB [=============       ] 65% 2.4s\u001b[0K\u001b[1G124.1 MiB [=============       ] 65% 2.3s\u001b[0K\u001b[1G124.1 MiB [=============       ] 66% 2.3s\u001b[0K\u001b[1G124.1 MiB [=============       ] 66% 2.2s\u001b[0K\u001b[1G124.1 MiB [=============       ] 67% 2.2s\u001b[0K\u001b[1G124.1 MiB [==============      ] 67% 2.2s\u001b[0K\u001b[1G124.1 MiB [==============      ] 68% 2.1s\u001b[0K\u001b[1G124.1 MiB [==============      ] 69% 2.1s\u001b[0K\u001b[1G124.1 MiB [==============      ] 69% 2.0s\u001b[0K\u001b[1G124.1 MiB [==============      ] 70% 2.0s\u001b[0K\u001b[1G124.1 MiB [==============      ] 70% 1.9s\u001b[0K\u001b[1G124.1 MiB [==============      ] 71% 1.9s\u001b[0K\u001b[1G124.1 MiB [==============      ] 72% 1.8s\u001b[0K\u001b[1G124.1 MiB [===============     ] 72% 1.8s\u001b[0K\u001b[1G124.1 MiB [===============     ] 73% 1.8s\u001b[0K\u001b[1G124.1 MiB [===============     ] 73% 1.7s\u001b[0K\u001b[1G124.1 MiB [===============     ] 74% 1.7s\u001b[0K\u001b[1G124.1 MiB [===============     ] 75% 1.6s\u001b[0K\u001b[1G124.1 MiB [===============     ] 76% 1.6s\u001b[0K\u001b[1G124.1 MiB [===============     ] 76% 1.5s\u001b[0K\u001b[1G124.1 MiB [================    ] 77% 1.5s\u001b[0K\u001b[1G124.1 MiB [================    ] 77% 1.4s\u001b[0K\u001b[1G124.1 MiB [================    ] 78% 1.4s\u001b[0K\u001b[1G124.1 MiB [================    ] 79% 1.4s\u001b[0K\u001b[1G124.1 MiB [================    ] 79% 1.3s\u001b[0K\u001b[1G124.1 MiB [================    ] 80% 1.3s\u001b[0K\u001b[1G124.1 MiB [================    ] 80% 1.2s\u001b[0K\u001b[1G124.1 MiB [================    ] 81% 1.2s\u001b[0K\u001b[1G124.1 MiB [================    ] 82% 1.2s\u001b[0K\u001b[1G124.1 MiB [================    ] 82% 1.1s\u001b[0K\u001b[1G124.1 MiB [=================   ] 82% 1.1s\u001b[0K\u001b[1G124.1 MiB [=================   ] 83% 1.1s\u001b[0K\u001b[1G124.1 MiB [=================   ] 84% 1.0s\u001b[0K\u001b[1G124.1 MiB [=================   ] 85% 1.0s\u001b[0K\u001b[1G124.1 MiB [=================   ] 85% 0.9s\u001b[0K\u001b[1G124.1 MiB [=================   ] 86% 0.9s\u001b[0K\u001b[1G124.1 MiB [=================   ] 86% 0.8s\u001b[0K\u001b[1G124.1 MiB [=================   ] 87% 0.8s\u001b[0K\u001b[1G124.1 MiB [==================  ] 87% 0.8s\u001b[0K\u001b[1G124.1 MiB [==================  ] 88% 0.8s\u001b[0K\u001b[1G124.1 MiB [==================  ] 88% 0.7s\u001b[0K\u001b[1G124.1 MiB [==================  ] 89% 0.7s\u001b[0K\u001b[1G124.1 MiB [==================  ] 90% 0.6s\u001b[0K\u001b[1G124.1 MiB [==================  ] 91% 0.6s\u001b[0K\u001b[1G124.1 MiB [==================  ] 91% 0.5s\u001b[0K\u001b[1G124.1 MiB [==================  ] 92% 0.5s\u001b[0K\u001b[1G124.1 MiB [=================== ] 92% 0.5s\u001b[0K\u001b[1G124.1 MiB [=================== ] 93% 0.4s\u001b[0K\u001b[1G124.1 MiB [=================== ] 94% 0.4s\u001b[0K\u001b[1G124.1 MiB [=================== ] 94% 0.3s\u001b[0K\u001b[1G124.1 MiB [=================== ] 95% 0.3s\u001b[0K\u001b[1G124.1 MiB [=================== ] 96% 0.2s\u001b[0K\u001b[1G124.1 MiB [=================== ] 97% 0.2s\u001b[0K\u001b[1G124.1 MiB [====================] 97% 0.1s\u001b[0K\u001b[1G124.1 MiB [====================] 98% 0.1s\u001b[0K\u001b[1G124.1 MiB [====================] 99% 0.1s\u001b[0K\u001b[1G124.1 MiB [====================] 99% 0.0s\u001b[0K\u001b[1G124.1 MiB [====================] 100% 0.0s\u001b[0K\n",
      "Chromium 134.0.6998.35 (playwright build v1161) downloaded to /Users/sanjaikgv/Library/Caches/ms-playwright/chromium-1161\n",
      "Downloading Chromium Headless Shell 134.0.6998.35 (playwright build v1161)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/chromium/1161/chromium-headless-shell-mac-arm64.zip\u001b[22m\n",
      "\u001b[1G78.2 MiB [                    ] 0% 0.0s\u001b[0K\u001b[1G78.2 MiB [                    ] 0% 31.0s\u001b[0K\u001b[1G78.2 MiB [                    ] 0% 26.3s\u001b[0K\u001b[1G78.2 MiB [                    ] 0% 17.7s\u001b[0K\u001b[1G78.2 MiB [                    ] 0% 11.1s\u001b[0K\u001b[1G78.2 MiB [                    ] 1% 7.1s\u001b[0K\u001b[1G78.2 MiB [                    ] 1% 6.6s\u001b[0K\u001b[1G78.2 MiB [                    ] 2% 5.8s\u001b[0K\u001b[1G78.2 MiB [=                   ] 2% 6.0s\u001b[0K\u001b[1G78.2 MiB [=                   ] 3% 5.4s\u001b[0K\u001b[1G78.2 MiB [=                   ] 3% 5.1s\u001b[0K\u001b[1G78.2 MiB [=                   ] 4% 4.8s\u001b[0K\u001b[1G78.2 MiB [=                   ] 5% 4.6s\u001b[0K\u001b[1G78.2 MiB [=                   ] 5% 4.3s\u001b[0K\u001b[1G78.2 MiB [=                   ] 6% 4.3s\u001b[0K\u001b[1G78.2 MiB [=                   ] 6% 4.2s\u001b[0K\u001b[1G78.2 MiB [=                   ] 6% 4.1s\u001b[0K\u001b[1G78.2 MiB [=                   ] 7% 4.0s\u001b[0K\u001b[1G78.2 MiB [==                  ] 7% 4.0s\u001b[0K\u001b[1G78.2 MiB [==                  ] 8% 3.9s\u001b[0K\u001b[1G78.2 MiB [==                  ] 8% 3.8s\u001b[0K\u001b[1G78.2 MiB [==                  ] 9% 3.9s\u001b[0K\u001b[1G78.2 MiB [==                  ] 9% 3.8s\u001b[0K\u001b[1G78.2 MiB [==                  ] 10% 3.7s\u001b[0K\u001b[1G78.2 MiB [==                  ] 11% 3.6s\u001b[0K\u001b[1G78.2 MiB [==                  ] 11% 3.5s\u001b[0K\u001b[1G78.2 MiB [==                  ] 12% 3.5s\u001b[0K\u001b[1G78.2 MiB [===                 ] 12% 3.4s\u001b[0K\u001b[1G78.2 MiB [===                 ] 13% 3.4s\u001b[0K\u001b[1G78.2 MiB [===                 ] 14% 3.3s\u001b[0K\u001b[1G78.2 MiB [===                 ] 15% 3.3s\u001b[0K\u001b[1G78.2 MiB [===                 ] 15% 3.2s\u001b[0K\u001b[1G78.2 MiB [===                 ] 16% 3.2s\u001b[0K\u001b[1G78.2 MiB [===                 ] 17% 3.1s\u001b[0K\u001b[1G78.2 MiB [====                ] 17% 3.1s\u001b[0K\u001b[1G78.2 MiB [====                ] 18% 3.1s\u001b[0K\u001b[1G78.2 MiB [====                ] 18% 3.0s\u001b[0K\u001b[1G78.2 MiB [====                ] 19% 3.0s\u001b[0K\u001b[1G78.2 MiB [====                ] 20% 2.9s\u001b[0K\u001b[1G78.2 MiB [====                ] 21% 2.9s\u001b[0K\u001b[1G78.2 MiB [====                ] 22% 2.9s\u001b[0K\u001b[1G78.2 MiB [=====               ] 22% 2.8s\u001b[0K\u001b[1G78.2 MiB [=====               ] 23% 2.8s\u001b[0K\u001b[1G78.2 MiB [=====               ] 24% 2.8s\u001b[0K\u001b[1G78.2 MiB [=====               ] 24% 2.7s\u001b[0K\u001b[1G78.2 MiB [=====               ] 25% 2.7s\u001b[0K\u001b[1G78.2 MiB [=====               ] 26% 2.7s\u001b[0K\u001b[1G78.2 MiB [=====               ] 26% 2.6s\u001b[0K\u001b[1G78.2 MiB [=====               ] 27% 2.6s\u001b[0K\u001b[1G78.2 MiB [======              ] 28% 2.6s\u001b[0K\u001b[1G78.2 MiB [======              ] 28% 2.5s\u001b[0K\u001b[1G78.2 MiB [======              ] 29% 2.5s\u001b[0K\u001b[1G78.2 MiB [======              ] 30% 2.4s\u001b[0K\u001b[1G78.2 MiB [======              ] 31% 2.4s\u001b[0K\u001b[1G78.2 MiB [======              ] 32% 2.3s\u001b[0K\u001b[1G78.2 MiB [=======             ] 32% 2.3s\u001b[0K\u001b[1G78.2 MiB [=======             ] 33% 2.3s\u001b[0K\u001b[1G78.2 MiB [=======             ] 34% 2.3s\u001b[0K\u001b[1G78.2 MiB [=======             ] 35% 2.2s\u001b[0K\u001b[1G78.2 MiB [=======             ] 36% 2.2s\u001b[0K\u001b[1G78.2 MiB [=======             ] 37% 2.2s\u001b[0K\u001b[1G78.2 MiB [========            ] 37% 2.2s\u001b[0K\u001b[1G78.2 MiB [========            ] 38% 2.1s\u001b[0K\u001b[1G78.2 MiB [========            ] 39% 2.1s\u001b[0K\u001b[1G78.2 MiB [========            ] 40% 2.0s\u001b[0K\u001b[1G78.2 MiB [========            ] 41% 2.0s\u001b[0K\u001b[1G78.2 MiB [========            ] 42% 2.0s\u001b[0K\u001b[1G78.2 MiB [=========           ] 42% 2.0s\u001b[0K\u001b[1G78.2 MiB [=========           ] 43% 1.9s\u001b[0K\u001b[1G78.2 MiB [=========           ] 44% 1.9s\u001b[0K\u001b[1G78.2 MiB [=========           ] 45% 1.9s\u001b[0K\u001b[1G78.2 MiB [=========           ] 45% 1.8s\u001b[0K\u001b[1G78.2 MiB [=========           ] 46% 1.8s\u001b[0K\u001b[1G78.2 MiB [=========           ] 47% 1.8s\u001b[0K\u001b[1G78.2 MiB [==========          ] 47% 1.8s\u001b[0K\u001b[1G78.2 MiB [==========          ] 48% 1.7s\u001b[0K\u001b[1G78.2 MiB [==========          ] 49% 1.7s\u001b[0K\u001b[1G78.2 MiB [==========          ] 50% 1.7s\u001b[0K\u001b[1G78.2 MiB [==========          ] 51% 1.6s\u001b[0K\u001b[1G78.2 MiB [==========          ] 52% 1.6s\u001b[0K\u001b[1G78.2 MiB [===========         ] 52% 1.6s\u001b[0K\u001b[1G78.2 MiB [===========         ] 53% 1.5s\u001b[0K\u001b[1G78.2 MiB [===========         ] 54% 1.5s\u001b[0K\u001b[1G78.2 MiB [===========         ] 55% 1.5s\u001b[0K\u001b[1G78.2 MiB [===========         ] 56% 1.5s\u001b[0K\u001b[1G78.2 MiB [===========         ] 57% 1.5s\u001b[0K\u001b[1G78.2 MiB [============        ] 57% 1.4s\u001b[0K\u001b[1G78.2 MiB [============        ] 57% 1.5s\u001b[0K\u001b[1G78.2 MiB [============        ] 58% 1.5s\u001b[0K\u001b[1G78.2 MiB [============        ] 58% 1.4s\u001b[0K\u001b[1G78.2 MiB [============        ] 59% 1.4s\u001b[0K\u001b[1G78.2 MiB [============        ] 60% 1.4s\u001b[0K\u001b[1G78.2 MiB [============        ] 61% 1.3s\u001b[0K\u001b[1G78.2 MiB [============        ] 62% 1.3s\u001b[0K\u001b[1G78.2 MiB [=============       ] 62% 1.3s\u001b[0K\u001b[1G78.2 MiB [=============       ] 63% 1.3s\u001b[0K\u001b[1G78.2 MiB [=============       ] 64% 1.2s\u001b[0K\u001b[1G78.2 MiB [=============       ] 65% 1.2s\u001b[0K\u001b[1G78.2 MiB [=============       ] 66% 1.1s\u001b[0K\u001b[1G78.2 MiB [=============       ] 67% 1.1s\u001b[0K\u001b[1G78.2 MiB [==============      ] 67% 1.1s\u001b[0K\u001b[1G78.2 MiB [==============      ] 68% 1.1s\u001b[0K\u001b[1G78.2 MiB [==============      ] 69% 1.0s\u001b[0K\u001b[1G78.2 MiB [==============      ] 70% 1.0s\u001b[0K\u001b[1G78.2 MiB [==============      ] 71% 1.0s\u001b[0K\u001b[1G78.2 MiB [==============      ] 71% 0.9s\u001b[0K\u001b[1G78.2 MiB [==============      ] 72% 0.9s\u001b[0K\u001b[1G78.2 MiB [===============     ] 73% 0.9s\u001b[0K\u001b[1G78.2 MiB [===============     ] 74% 0.8s\u001b[0K\u001b[1G78.2 MiB [===============     ] 75% 0.8s\u001b[0K\u001b[1G78.2 MiB [===============     ] 76% 0.8s\u001b[0K\u001b[1G78.2 MiB [================    ] 77% 0.7s\u001b[0K\u001b[1G78.2 MiB [================    ] 78% 0.7s\u001b[0K\u001b[1G78.2 MiB [================    ] 79% 0.7s\u001b[0K\u001b[1G78.2 MiB [================    ] 80% 0.7s\u001b[0K\u001b[1G78.2 MiB [================    ] 80% 0.6s\u001b[0K\u001b[1G78.2 MiB [================    ] 81% 0.6s\u001b[0K\u001b[1G78.2 MiB [================    ] 82% 0.6s\u001b[0K\u001b[1G78.2 MiB [=================   ] 82% 0.6s\u001b[0K\u001b[1G78.2 MiB [=================   ] 83% 0.5s\u001b[0K\u001b[1G78.2 MiB [=================   ] 84% 0.5s\u001b[0K\u001b[1G78.2 MiB [=================   ] 85% 0.5s\u001b[0K\u001b[1G78.2 MiB [=================   ] 86% 0.4s\u001b[0K\u001b[1G78.2 MiB [=================   ] 87% 0.4s\u001b[0K\u001b[1G78.2 MiB [==================  ] 87% 0.4s\u001b[0K\u001b[1G78.2 MiB [==================  ] 88% 0.4s\u001b[0K\u001b[1G78.2 MiB [==================  ] 89% 0.3s\u001b[0K\u001b[1G78.2 MiB [==================  ] 90% 0.3s\u001b[0K\u001b[1G78.2 MiB [==================  ] 91% 0.3s\u001b[0K\u001b[1G78.2 MiB [==================  ] 92% 0.3s\u001b[0K\u001b[1G78.2 MiB [=================== ] 92% 0.2s\u001b[0K\u001b[1G78.2 MiB [=================== ] 93% 0.2s\u001b[0K\u001b[1G78.2 MiB [=================== ] 94% 0.2s\u001b[0K\u001b[1G78.2 MiB [=================== ] 95% 0.1s\u001b[0K\u001b[1G78.2 MiB [=================== ] 96% 0.1s\u001b[0K\u001b[1G78.2 MiB [=================== ] 97% 0.1s\u001b[0K\u001b[1G78.2 MiB [====================] 97% 0.1s\u001b[0K\u001b[1G78.2 MiB [====================] 98% 0.1s\u001b[0K\u001b[1G78.2 MiB [====================] 98% 0.0s\u001b[0K\u001b[1G78.2 MiB [====================] 99% 0.0s\u001b[0K\u001b[1G78.2 MiB [====================] 100% 0.0s\u001b[0K\n",
      "Chromium Headless Shell 134.0.6998.35 (playwright build v1161) downloaded to /Users/sanjaikgv/Library/Caches/ms-playwright/chromium_headless_shell-1161\n",
      "Downloading Firefox 135.0 (playwright build v1475)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/firefox/1475/firefox-mac-arm64.zip\u001b[22m\n",
      "\u001b[1G84.9 MiB [                    ] 0% 0.0s\u001b[0K\u001b[1G84.9 MiB [                    ] 0% 40.4s\u001b[0K\u001b[1G84.9 MiB [                    ] 0% 27.4s\u001b[0K\u001b[1G84.9 MiB [                    ] 0% 18.3s\u001b[0K\u001b[1G84.9 MiB [                    ] 0% 12.8s\u001b[0K\u001b[1G84.9 MiB [                    ] 1% 9.1s\u001b[0K\u001b[1G84.9 MiB [                    ] 1% 7.9s\u001b[0K\u001b[1G84.9 MiB [                    ] 1% 7.7s\u001b[0K\u001b[1G84.9 MiB [                    ] 2% 7.0s\u001b[0K\u001b[1G84.9 MiB [=                   ] 2% 6.5s\u001b[0K\u001b[1G84.9 MiB [=                   ] 2% 6.4s\u001b[0K\u001b[1G84.9 MiB [=                   ] 3% 6.2s\u001b[0K\u001b[1G84.9 MiB [=                   ] 4% 5.8s\u001b[0K\u001b[1G84.9 MiB [=                   ] 4% 5.4s\u001b[0K\u001b[1G84.9 MiB [=                   ] 4% 5.2s\u001b[0K\u001b[1G84.9 MiB [=                   ] 5% 5.2s\u001b[0K\u001b[1G84.9 MiB [=                   ] 5% 5.1s\u001b[0K\u001b[1G84.9 MiB [=                   ] 6% 4.9s\u001b[0K\u001b[1G84.9 MiB [=                   ] 7% 4.9s\u001b[0K\u001b[1G84.9 MiB [==                  ] 7% 4.8s\u001b[0K\u001b[1G84.9 MiB [==                  ] 8% 4.6s\u001b[0K\u001b[1G84.9 MiB [==                  ] 8% 4.5s\u001b[0K\u001b[1G84.9 MiB [==                  ] 9% 4.4s\u001b[0K\u001b[1G84.9 MiB [==                  ] 9% 4.3s\u001b[0K\u001b[1G84.9 MiB [==                  ] 10% 4.1s\u001b[0K\u001b[1G84.9 MiB [==                  ] 11% 4.0s\u001b[0K\u001b[1G84.9 MiB [==                  ] 12% 3.9s\u001b[0K\u001b[1G84.9 MiB [===                 ] 13% 3.8s\u001b[0K\u001b[1G84.9 MiB [===                 ] 13% 3.7s\u001b[0K\u001b[1G84.9 MiB [===                 ] 14% 3.7s\u001b[0K\u001b[1G84.9 MiB [===                 ] 14% 3.6s\u001b[0K\u001b[1G84.9 MiB [===                 ] 15% 3.6s\u001b[0K\u001b[1G84.9 MiB [===                 ] 16% 3.5s\u001b[0K\u001b[1G84.9 MiB [===                 ] 16% 3.4s\u001b[0K\u001b[1G84.9 MiB [===                 ] 17% 3.4s\u001b[0K\u001b[1G84.9 MiB [====                ] 17% 3.3s\u001b[0K\u001b[1G84.9 MiB [====                ] 18% 3.3s\u001b[0K\u001b[1G84.9 MiB [====                ] 19% 3.2s\u001b[0K\u001b[1G84.9 MiB [====                ] 20% 3.1s\u001b[0K\u001b[1G84.9 MiB [====                ] 21% 3.1s\u001b[0K\u001b[1G84.9 MiB [====                ] 21% 3.0s\u001b[0K\u001b[1G84.9 MiB [====                ] 22% 3.0s\u001b[0K\u001b[1G84.9 MiB [=====               ] 22% 2.9s\u001b[0K\u001b[1G84.9 MiB [=====               ] 23% 2.9s\u001b[0K\u001b[1G84.9 MiB [=====               ] 24% 2.9s\u001b[0K\u001b[1G84.9 MiB [=====               ] 24% 2.8s\u001b[0K\u001b[1G84.9 MiB [=====               ] 25% 2.8s\u001b[0K\u001b[1G84.9 MiB [=====               ] 26% 2.8s\u001b[0K\u001b[1G84.9 MiB [=====               ] 27% 2.8s\u001b[0K\u001b[1G84.9 MiB [======              ] 27% 2.8s\u001b[0K\u001b[1G84.9 MiB [======              ] 28% 2.7s\u001b[0K\u001b[1G84.9 MiB [======              ] 29% 2.7s\u001b[0K\u001b[1G84.9 MiB [======              ] 30% 2.6s\u001b[0K\u001b[1G84.9 MiB [======              ] 31% 2.6s\u001b[0K\u001b[1G84.9 MiB [======              ] 32% 2.5s\u001b[0K\u001b[1G84.9 MiB [=======             ] 32% 2.5s\u001b[0K\u001b[1G84.9 MiB [=======             ] 33% 2.5s\u001b[0K\u001b[1G84.9 MiB [=======             ] 34% 2.4s\u001b[0K\u001b[1G84.9 MiB [=======             ] 35% 2.4s\u001b[0K\u001b[1G84.9 MiB [=======             ] 36% 2.4s\u001b[0K\u001b[1G84.9 MiB [=======             ] 36% 2.3s\u001b[0K\u001b[1G84.9 MiB [=======             ] 37% 2.3s\u001b[0K\u001b[1G84.9 MiB [========            ] 37% 2.3s\u001b[0K\u001b[1G84.9 MiB [========            ] 38% 2.3s\u001b[0K\u001b[1G84.9 MiB [========            ] 38% 2.2s\u001b[0K\u001b[1G84.9 MiB [========            ] 39% 2.2s\u001b[0K\u001b[1G84.9 MiB [========            ] 40% 2.2s\u001b[0K\u001b[1G84.9 MiB [========            ] 41% 2.1s\u001b[0K\u001b[1G84.9 MiB [========            ] 42% 2.1s\u001b[0K\u001b[1G84.9 MiB [=========           ] 42% 2.1s\u001b[0K\u001b[1G84.9 MiB [=========           ] 43% 2.0s\u001b[0K\u001b[1G84.9 MiB [=========           ] 44% 2.0s\u001b[0K\u001b[1G84.9 MiB [=========           ] 45% 2.0s\u001b[0K\u001b[1G84.9 MiB [=========           ] 45% 1.9s\u001b[0K\u001b[1G84.9 MiB [=========           ] 46% 1.9s\u001b[0K\u001b[1G84.9 MiB [=========           ] 47% 1.9s\u001b[0K\u001b[1G84.9 MiB [==========          ] 47% 1.9s\u001b[0K\u001b[1G84.9 MiB [==========          ] 48% 1.9s\u001b[0K\u001b[1G84.9 MiB [==========          ] 49% 1.9s\u001b[0K\u001b[1G84.9 MiB [==========          ] 49% 1.8s\u001b[0K\u001b[1G84.9 MiB [==========          ] 50% 1.8s\u001b[0K\u001b[1G84.9 MiB [==========          ] 51% 1.8s\u001b[0K\u001b[1G84.9 MiB [==========          ] 52% 1.8s\u001b[0K\u001b[1G84.9 MiB [===========         ] 52% 1.8s\u001b[0K\u001b[1G84.9 MiB [===========         ] 53% 1.8s\u001b[0K\u001b[1G84.9 MiB [===========         ] 53% 1.7s\u001b[0K\u001b[1G84.9 MiB [===========         ] 53% 1.8s\u001b[0K\u001b[1G84.9 MiB [===========         ] 54% 1.8s\u001b[0K\u001b[1G84.9 MiB [===========         ] 55% 1.8s\u001b[0K\u001b[1G84.9 MiB [===========         ] 56% 1.8s\u001b[0K\u001b[1G84.9 MiB [===========         ] 57% 1.8s\u001b[0K\u001b[1G84.9 MiB [============        ] 57% 1.7s\u001b[0K\u001b[1G84.9 MiB [============        ] 57% 1.8s\u001b[0K\u001b[1G84.9 MiB [============        ] 58% 1.7s\u001b[0K\u001b[1G84.9 MiB [============        ] 58% 1.8s\u001b[0K\u001b[1G84.9 MiB [============        ] 59% 1.8s\u001b[0K\u001b[1G84.9 MiB [============        ] 59% 1.7s\u001b[0K\u001b[1G84.9 MiB [============        ] 60% 1.7s\u001b[0K\u001b[1G84.9 MiB [============        ] 61% 1.7s\u001b[0K\u001b[1G84.9 MiB [============        ] 61% 1.6s\u001b[0K\u001b[1G84.9 MiB [============        ] 62% 1.6s\u001b[0K\u001b[1G84.9 MiB [=============       ] 62% 1.6s\u001b[0K\u001b[1G84.9 MiB [=============       ] 63% 1.6s\u001b[0K\u001b[1G84.9 MiB [=============       ] 63% 1.5s\u001b[0K\u001b[1G84.9 MiB [=============       ] 64% 1.5s\u001b[0K\u001b[1G84.9 MiB [=============       ] 65% 1.5s\u001b[0K\u001b[1G84.9 MiB [=============       ] 65% 1.4s\u001b[0K\u001b[1G84.9 MiB [=============       ] 66% 1.4s\u001b[0K\u001b[1G84.9 MiB [=============       ] 67% 1.4s\u001b[0K\u001b[1G84.9 MiB [==============      ] 67% 1.3s\u001b[0K\u001b[1G84.9 MiB [==============      ] 68% 1.3s\u001b[0K\u001b[1G84.9 MiB [==============      ] 69% 1.3s\u001b[0K\u001b[1G84.9 MiB [==============      ] 70% 1.2s\u001b[0K\u001b[1G84.9 MiB [==============      ] 71% 1.2s\u001b[0K\u001b[1G84.9 MiB [==============      ] 72% 1.2s\u001b[0K\u001b[1G84.9 MiB [==============      ] 72% 1.1s\u001b[0K\u001b[1G84.9 MiB [===============     ] 72% 1.1s\u001b[0K\u001b[1G84.9 MiB [===============     ] 73% 1.1s\u001b[0K\u001b[1G84.9 MiB [===============     ] 74% 1.1s\u001b[0K\u001b[1G84.9 MiB [===============     ] 74% 1.0s\u001b[0K\u001b[1G84.9 MiB [===============     ] 75% 1.0s\u001b[0K\u001b[1G84.9 MiB [===============     ] 76% 1.0s\u001b[0K\u001b[1G84.9 MiB [===============     ] 77% 0.9s\u001b[0K\u001b[1G84.9 MiB [================    ] 77% 0.9s\u001b[0K\u001b[1G84.9 MiB [================    ] 78% 0.9s\u001b[0K\u001b[1G84.9 MiB [================    ] 79% 0.8s\u001b[0K\u001b[1G84.9 MiB [================    ] 80% 0.8s\u001b[0K\u001b[1G84.9 MiB [================    ] 81% 0.8s\u001b[0K\u001b[1G84.9 MiB [================    ] 81% 0.7s\u001b[0K\u001b[1G84.9 MiB [================    ] 82% 0.7s\u001b[0K\u001b[1G84.9 MiB [=================   ] 82% 0.7s\u001b[0K\u001b[1G84.9 MiB [=================   ] 83% 0.7s\u001b[0K\u001b[1G84.9 MiB [=================   ] 84% 0.6s\u001b[0K\u001b[1G84.9 MiB [=================   ] 85% 0.6s\u001b[0K\u001b[1G84.9 MiB [=================   ] 86% 0.6s\u001b[0K\u001b[1G84.9 MiB [=================   ] 86% 0.5s\u001b[0K\u001b[1G84.9 MiB [=================   ] 87% 0.5s\u001b[0K\u001b[1G84.9 MiB [==================  ] 87% 0.5s\u001b[0K\u001b[1G84.9 MiB [==================  ] 88% 0.5s\u001b[0K\u001b[1G84.9 MiB [==================  ] 88% 0.4s\u001b[0K\u001b[1G84.9 MiB [==================  ] 89% 0.4s\u001b[0K\u001b[1G84.9 MiB [==================  ] 90% 0.4s\u001b[0K\u001b[1G84.9 MiB [==================  ] 91% 0.4s\u001b[0K\u001b[1G84.9 MiB [==================  ] 91% 0.3s\u001b[0K\u001b[1G84.9 MiB [==================  ] 92% 0.3s\u001b[0K\u001b[1G84.9 MiB [=================== ] 92% 0.3s\u001b[0K\u001b[1G84.9 MiB [=================== ] 93% 0.3s\u001b[0K\u001b[1G84.9 MiB [=================== ] 93% 0.2s\u001b[0K\u001b[1G84.9 MiB [=================== ] 94% 0.2s\u001b[0K\u001b[1G84.9 MiB [=================== ] 95% 0.2s\u001b[0K\u001b[1G84.9 MiB [=================== ] 96% 0.1s\u001b[0K\u001b[1G84.9 MiB [=================== ] 97% 0.1s\u001b[0K\u001b[1G84.9 MiB [====================] 97% 0.1s\u001b[0K\u001b[1G84.9 MiB [====================] 98% 0.1s\u001b[0K\u001b[1G84.9 MiB [====================] 98% 0.0s\u001b[0K\u001b[1G84.9 MiB [====================] 99% 0.0s\u001b[0K\u001b[1G84.9 MiB [====================] 100% 0.0s\u001b[0K\n",
      "Firefox 135.0 (playwright build v1475) downloaded to /Users/sanjaikgv/Library/Caches/ms-playwright/firefox-1475\n",
      "Downloading Webkit 18.4 (playwright build v2140)\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/webkit/2140/webkit-mac-15-arm64.zip\u001b[22m\n",
      "\u001b[1G65.7 MiB [                    ] 0% 0.0s\u001b[0K\u001b[1G65.7 MiB [                    ] 0% 28.0s\u001b[0K\u001b[1G65.7 MiB [                    ] 0% 18.2s\u001b[0K\u001b[1G65.7 MiB [                    ] 0% 13.9s\u001b[0K\u001b[1G65.7 MiB [                    ] 0% 9.7s\u001b[0K\u001b[1G65.7 MiB [                    ] 1% 8.2s\u001b[0K\u001b[1G65.7 MiB [                    ] 1% 7.5s\u001b[0K\u001b[1G65.7 MiB [                    ] 2% 6.0s\u001b[0K\u001b[1G65.7 MiB [=                   ] 2% 5.8s\u001b[0K\u001b[1G65.7 MiB [=                   ] 3% 5.3s\u001b[0K\u001b[1G65.7 MiB [=                   ] 3% 4.9s\u001b[0K\u001b[1G65.7 MiB [=                   ] 4% 4.7s\u001b[0K\u001b[1G65.7 MiB [=                   ] 4% 4.5s\u001b[0K\u001b[1G65.7 MiB [=                   ] 5% 4.4s\u001b[0K\u001b[1G65.7 MiB [=                   ] 5% 4.2s\u001b[0K\u001b[1G65.7 MiB [=                   ] 6% 4.0s\u001b[0K\u001b[1G65.7 MiB [=                   ] 7% 3.8s\u001b[0K\u001b[1G65.7 MiB [==                  ] 8% 3.6s\u001b[0K\u001b[1G65.7 MiB [==                  ] 8% 3.4s\u001b[0K\u001b[1G65.7 MiB [==                  ] 9% 3.2s\u001b[0K\u001b[1G65.7 MiB [==                  ] 10% 3.1s\u001b[0K\u001b[1G65.7 MiB [==                  ] 11% 3.0s\u001b[0K\u001b[1G65.7 MiB [==                  ] 12% 3.0s\u001b[0K\u001b[1G65.7 MiB [===                 ] 12% 3.0s\u001b[0K\u001b[1G65.7 MiB [===                 ] 13% 2.9s\u001b[0K\u001b[1G65.7 MiB [===                 ] 14% 2.9s\u001b[0K\u001b[1G65.7 MiB [===                 ] 15% 2.9s\u001b[0K\u001b[1G65.7 MiB [===                 ] 15% 2.8s\u001b[0K\u001b[1G65.7 MiB [===                 ] 16% 2.8s\u001b[0K\u001b[1G65.7 MiB [===                 ] 17% 2.7s\u001b[0K\u001b[1G65.7 MiB [====                ] 17% 2.7s\u001b[0K\u001b[1G65.7 MiB [====                ] 18% 2.6s\u001b[0K\u001b[1G65.7 MiB [====                ] 19% 2.5s\u001b[0K\u001b[1G65.7 MiB [====                ] 20% 2.5s\u001b[0K\u001b[1G65.7 MiB [====                ] 21% 2.4s\u001b[0K\u001b[1G65.7 MiB [====                ] 22% 2.4s\u001b[0K\u001b[1G65.7 MiB [=====               ] 22% 2.4s\u001b[0K\u001b[1G65.7 MiB [=====               ] 23% 2.4s\u001b[0K\u001b[1G65.7 MiB [=====               ] 24% 2.4s\u001b[0K\u001b[1G65.7 MiB [=====               ] 24% 2.3s\u001b[0K\u001b[1G65.7 MiB [=====               ] 25% 2.3s\u001b[0K\u001b[1G65.7 MiB [=====               ] 26% 2.2s\u001b[0K\u001b[1G65.7 MiB [=====               ] 27% 2.2s\u001b[0K\u001b[1G65.7 MiB [======              ] 27% 2.2s\u001b[0K\u001b[1G65.7 MiB [======              ] 28% 2.2s\u001b[0K\u001b[1G65.7 MiB [======              ] 29% 2.2s\u001b[0K\u001b[1G65.7 MiB [======              ] 30% 2.1s\u001b[0K\u001b[1G65.7 MiB [======              ] 31% 2.1s\u001b[0K\u001b[1G65.7 MiB [======              ] 32% 2.0s\u001b[0K\u001b[1G65.7 MiB [=======             ] 33% 2.0s\u001b[0K\u001b[1G65.7 MiB [=======             ] 34% 2.0s\u001b[0K\u001b[1G65.7 MiB [=======             ] 34% 1.9s\u001b[0K\u001b[1G65.7 MiB [=======             ] 35% 1.9s\u001b[0K\u001b[1G65.7 MiB [=======             ] 36% 1.9s\u001b[0K\u001b[1G65.7 MiB [=======             ] 37% 1.9s\u001b[0K\u001b[1G65.7 MiB [========            ] 37% 1.8s\u001b[0K\u001b[1G65.7 MiB [========            ] 38% 1.8s\u001b[0K\u001b[1G65.7 MiB [========            ] 39% 1.8s\u001b[0K\u001b[1G65.7 MiB [========            ] 40% 1.8s\u001b[0K\u001b[1G65.7 MiB [========            ] 41% 1.8s\u001b[0K\u001b[1G65.7 MiB [========            ] 42% 1.7s\u001b[0K\u001b[1G65.7 MiB [=========           ] 42% 1.7s\u001b[0K\u001b[1G65.7 MiB [=========           ] 43% 1.7s\u001b[0K\u001b[1G65.7 MiB [=========           ] 44% 1.7s\u001b[0K\u001b[1G65.7 MiB [=========           ] 44% 1.6s\u001b[0K\u001b[1G65.7 MiB [=========           ] 45% 1.6s\u001b[0K\u001b[1G65.7 MiB [=========           ] 46% 1.6s\u001b[0K\u001b[1G65.7 MiB [=========           ] 47% 1.6s\u001b[0K\u001b[1G65.7 MiB [==========          ] 47% 1.6s\u001b[0K\u001b[1G65.7 MiB [==========          ] 48% 1.6s\u001b[0K\u001b[1G65.7 MiB [==========          ] 49% 1.6s\u001b[0K\u001b[1G65.7 MiB [==========          ] 50% 1.6s\u001b[0K\u001b[1G65.7 MiB [==========          ] 51% 1.6s\u001b[0K\u001b[1G65.7 MiB [==========          ] 52% 1.6s\u001b[0K\u001b[1G65.7 MiB [===========         ] 52% 1.6s\u001b[0K\u001b[1G65.7 MiB [===========         ] 53% 1.6s\u001b[0K\u001b[1G65.7 MiB [===========         ] 53% 1.5s\u001b[0K\u001b[1G65.7 MiB [===========         ] 54% 1.5s\u001b[0K\u001b[1G65.7 MiB [===========         ] 55% 1.5s\u001b[0K\u001b[1G65.7 MiB [===========         ] 56% 1.4s\u001b[0K\u001b[1G65.7 MiB [============        ] 57% 1.4s\u001b[0K\u001b[1G65.7 MiB [============        ] 58% 1.4s\u001b[0K\u001b[1G65.7 MiB [============        ] 58% 1.3s\u001b[0K\u001b[1G65.7 MiB [============        ] 59% 1.3s\u001b[0K\u001b[1G65.7 MiB [============        ] 60% 1.3s\u001b[0K\u001b[1G65.7 MiB [============        ] 61% 1.3s\u001b[0K\u001b[1G65.7 MiB [============        ] 61% 1.2s\u001b[0K\u001b[1G65.7 MiB [============        ] 62% 1.2s\u001b[0K\u001b[1G65.7 MiB [=============       ] 62% 1.2s\u001b[0K\u001b[1G65.7 MiB [=============       ] 63% 1.2s\u001b[0K\u001b[1G65.7 MiB [=============       ] 64% 1.1s\u001b[0K\u001b[1G65.7 MiB [=============       ] 65% 1.1s\u001b[0K\u001b[1G65.7 MiB [=============       ] 66% 1.1s\u001b[0K\u001b[1G65.7 MiB [=============       ] 67% 1.0s\u001b[0K\u001b[1G65.7 MiB [==============      ] 67% 1.0s\u001b[0K\u001b[1G65.7 MiB [==============      ] 68% 1.0s\u001b[0K\u001b[1G65.7 MiB [==============      ] 69% 1.0s\u001b[0K\u001b[1G65.7 MiB [==============      ] 70% 0.9s\u001b[0K\u001b[1G65.7 MiB [==============      ] 71% 0.9s\u001b[0K\u001b[1G65.7 MiB [==============      ] 72% 0.9s\u001b[0K\u001b[1G65.7 MiB [===============     ] 72% 0.9s\u001b[0K\u001b[1G65.7 MiB [===============     ] 73% 0.8s\u001b[0K\u001b[1G65.7 MiB [===============     ] 74% 0.8s\u001b[0K\u001b[1G65.7 MiB [===============     ] 75% 0.8s\u001b[0K\u001b[1G65.7 MiB [===============     ] 76% 0.8s\u001b[0K\u001b[1G65.7 MiB [===============     ] 76% 0.7s\u001b[0K\u001b[1G65.7 MiB [================    ] 77% 0.7s\u001b[0K\u001b[1G65.7 MiB [================    ] 78% 0.7s\u001b[0K\u001b[1G65.7 MiB [================    ] 79% 0.6s\u001b[0K\u001b[1G65.7 MiB [================    ] 80% 0.6s\u001b[0K\u001b[1G65.7 MiB [================    ] 81% 0.6s\u001b[0K\u001b[1G65.7 MiB [================    ] 82% 0.6s\u001b[0K\u001b[1G65.7 MiB [=================   ] 82% 0.5s\u001b[0K\u001b[1G65.7 MiB [=================   ] 83% 0.5s\u001b[0K\u001b[1G65.7 MiB [=================   ] 84% 0.5s\u001b[0K\u001b[1G65.7 MiB [=================   ] 85% 0.5s\u001b[0K\u001b[1G65.7 MiB [=================   ] 85% 0.4s\u001b[0K\u001b[1G65.7 MiB [=================   ] 86% 0.4s\u001b[0K\u001b[1G65.7 MiB [=================   ] 87% 0.4s\u001b[0K\u001b[1G65.7 MiB [==================  ] 87% 0.4s\u001b[0K\u001b[1G65.7 MiB [==================  ] 88% 0.4s\u001b[0K\u001b[1G65.7 MiB [==================  ] 88% 0.3s\u001b[0K\u001b[1G65.7 MiB [==================  ] 89% 0.3s\u001b[0K\u001b[1G65.7 MiB [==================  ] 90% 0.3s\u001b[0K\u001b[1G65.7 MiB [==================  ] 91% 0.3s\u001b[0K\u001b[1G65.7 MiB [==================  ] 91% 0.2s\u001b[0K\u001b[1G65.7 MiB [==================  ] 92% 0.2s\u001b[0K\u001b[1G65.7 MiB [=================== ] 93% 0.2s\u001b[0K\u001b[1G65.7 MiB [=================== ] 94% 0.2s\u001b[0K\u001b[1G65.7 MiB [=================== ] 95% 0.1s\u001b[0K\u001b[1G65.7 MiB [=================== ] 96% 0.1s\u001b[0K\u001b[1G65.7 MiB [=================== ] 97% 0.1s\u001b[0K\u001b[1G65.7 MiB [====================] 98% 0.1s\u001b[0K\u001b[1G65.7 MiB [====================] 99% 0.0s\u001b[0K\u001b[1G65.7 MiB [====================] 100% 0.0s\u001b[0K\n",
      "Webkit 18.4 (playwright build v2140) downloaded to /Users/sanjaikgv/Library/Caches/ms-playwright/webkit-2140\n",
      "Downloading FFMPEG playwright build v1011\u001b[2m from https://cdn.playwright.dev/dbazure/download/playwright/builds/ffmpeg/1011/ffmpeg-mac-arm64.zip\u001b[22m\n",
      "\u001b[1G1 MiB [                    ] 1% 0.0s\u001b[0K\u001b[1G1 MiB [=                   ] 4% 0.4s\u001b[0K\u001b[1G1 MiB [==                  ] 11% 0.3s\u001b[0K\u001b[1G1 MiB [=====               ] 25% 0.2s\u001b[0K\u001b[1G1 MiB [=======             ] 35% 0.1s\u001b[0K\u001b[1G1 MiB [==============      ] 71% 0.0s\u001b[0K\u001b[1G1 MiB [=================   ] 86% 0.0s\u001b[0K\u001b[1G1 MiB [====================] 100% 0.0s\u001b[0K\n",
      "FFMPEG playwright build v1011 downloaded to /Users/sanjaikgv/Library/Caches/ms-playwright/ffmpeg-1011\n"
     ]
    }
   ],
   "source": [
    "!pip install playwright\n",
    "!playwright install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0724c955",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import csv\n",
    "import difflib\n",
    "from urllib.parse import quote\n",
    "from PyPDF2 import PdfReader\n",
    "import io\n",
    "import requests\n",
    "from script_2 import extract_text_from_pdf, clean_text #script_2 is the code that parses the pdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "21edb3e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_url = \"https://www.aimodels.fyi\"\n",
    "papers_page = \"/papers?search=&selectedTimeRange=thisYear&page={}\"\n",
    "PDF_DIR = 'arxiv_pdfs_new'\n",
    "os.makedirs(PDF_DIR, exist_ok=True)\n",
    "paper_urls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "024181cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get paper URLs\n",
    "async def get_paper_urls():\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)  # Set to True to run headlessly\n",
    "        page = await browser.new_page()\n",
    "\n",
    "        # Iterate through pages (adjust as needed)\n",
    "        for page_num in range(1, 70):\n",
    "            url = base_url + papers_page.format(page_num)\n",
    "            await page.goto(url)\n",
    "            await page.wait_for_timeout(5000)  # Wait for the page to load\n",
    "\n",
    "            # Get page content and pass to BeautifulSoup for parsing\n",
    "            page_content = await page.content()\n",
    "            soup = BeautifulSoup(page_content, 'html.parser')\n",
    "\n",
    "            # Find all paper links\n",
    "            for link in soup.find_all('a', href=True):\n",
    "                href = link['href']\n",
    "                if href.startswith('/papers/arxiv/'):\n",
    "                    full_url = base_url + href\n",
    "                    if full_url not in paper_urls:\n",
    "                        paper_urls.append(full_url)\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "    print(f\"Found {len(paper_urls)} paper URLs.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6cfa86b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract summary and heading from each paper page\n",
    "async def extract_summary_and_pdf(paper_url):\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "\n",
    "        await page.goto(paper_url)\n",
    "        await page.wait_for_timeout(3000)\n",
    "\n",
    "        # Get page content and pass to BeautifulSoup for parsing\n",
    "        page_content = await page.content()\n",
    "        soup = BeautifulSoup(page_content, 'html.parser')\n",
    "\n",
    "        # Extract the paper's heading\n",
    "        head_div = soup.find('div', class_='css-b1ilzc')\n",
    "        heading = head_div.find('h1').get_text(\" \", strip=True)\n",
    "\n",
    "        # Extract the summary (or article body)\n",
    "        summary_div = soup.find('div', class_='css-79elbk')\n",
    "\n",
    "        if not summary_div:\n",
    "            return 'No summary found.', heading, paper_url\n",
    "\n",
    "        # Replace all <h2> and <p> tags with plain text in a continuous format\n",
    "        parts = []\n",
    "        for element in summary_div.find_all(['h2', 'p', 'li']):\n",
    "            if element.name == 'h2':\n",
    "                text = element.get_text(\" \", strip=True)\n",
    "                text = '**' + text + '**'\n",
    "                parts.append(text)\n",
    "            else:\n",
    "                text = element.get_text(\" \", strip=True)\n",
    "                parts.append(text)\n",
    "\n",
    "        summary = ' '.join(parts)  # Join all parts with a space\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "    return summary, heading, paper_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "f6621875",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to find arxiv_id by title (using fuzzy matching)\n",
    "def find_arxiv_id_by_title(title, similarity_threshold=0.8):\n",
    "    ARXIV_API_URL = \"http://export.arxiv.org/api/query?search_query=ti:\\\"{}\\\"&max_results=1\"\n",
    "    query_url = ARXIV_API_URL.format(quote(title))\n",
    "    response = requests.get(query_url)\n",
    "    if response.status_code != 200:\n",
    "        print(f\"Failed to search arXiv for: {title}\")\n",
    "        return None\n",
    "\n",
    "    try:\n",
    "        import xml.etree.ElementTree as ET\n",
    "        root = ET.fromstring(response.text)\n",
    "        entries = root.findall(\"{http://www.w3.org/2005/Atom}entry\")\n",
    "        best_match = None\n",
    "        best_score = 0\n",
    "\n",
    "        for entry in entries:\n",
    "            arxiv_title = entry.find(\"{http://www.w3.org/2005/Atom}title\").text.strip()\n",
    "            score = difflib.SequenceMatcher(None, title.strip().lower(), arxiv_title.lower()).ratio()\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_match = entry\n",
    "\n",
    "        if best_match and best_score >= similarity_threshold:\n",
    "            arxiv_id_url = best_match.find(\"{http://www.w3.org/2005/Atom}id\").text\n",
    "            arxiv_id = arxiv_id_url.split('/abs/')[-1]\n",
    "            print(f\"Fuzzy match found (score={best_score:.2f}): {arxiv_id}\")\n",
    "            return arxiv_id\n",
    "        else:\n",
    "            print(f\"No good match found for: {title} (best score: {best_score:.2f})\")\n",
    "            return None\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error parsing arXiv response for title '{title}': {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6ded09c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download PDF\n",
    "def download_pdf(heading, arxiv_id):\n",
    "    pdf_url = f\"https://arxiv.org/pdf/{arxiv_id}.pdf\"\n",
    "    response = requests.get(pdf_url)\n",
    "    if response.status_code == 200:\n",
    "        file_path = os.path.join(PDF_DIR, f\"{heading}.pdf\")\n",
    "        print(f\"Downloading {file_path}\")\n",
    "        with open(file_path, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        return file_path, io.BytesIO(response.content)\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1680e718",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to count the number of pages in a PDF\n",
    "def count_pdf_pages(pdf_stream):\n",
    "    try:\n",
    "        reader = PdfReader(pdf_stream)\n",
    "        print(f\"Found {len(reader.pages)} pages\")\n",
    "        return len(reader.pages)\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52242d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to orchestrate everything\n",
    "async def main():\n",
    "    await get_paper_urls()  # Get all paper URLs\n",
    "    print(f\"Found {len(paper_urls)} paper URLs.\")\n",
    "\n",
    "    # Open CSV file for writing\n",
    "    with open('papers_summary.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        fieldnames = ['link', 'heading', 'arxiv_id', 'file_path', 'page_count', 'pdf_text', 'summary']\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "        writer.writeheader()\n",
    "\n",
    "        # Process each paper URL\n",
    "        for paper_url in paper_urls:\n",
    "            summary, heading, paper_url = await extract_summary_and_pdf(paper_url)\n",
    "            print(f\"Processing {heading}...\")\n",
    "\n",
    "            arxiv_id = find_arxiv_id_by_title(heading)\n",
    "            file_path, pdf = download_pdf(heading, arxiv_id)\n",
    "\n",
    "            num_pages = 0\n",
    "            if pdf:\n",
    "                num_pages = count_pdf_pages(pdf)\n",
    "                print(f\"{num_pages} pages\")\n",
    "\n",
    "            cleaned_pdf_text = \"\"\n",
    "            if file_path and os.path.exists(file_path) and num_pages < 50:\n",
    "                print(f\"Extracting and cleaning PDF: {file_path}\")\n",
    "                raw_text = extract_text_from_pdf(file_path)\n",
    "                if raw_text:\n",
    "                    cleaned_pdf_text = clean_text(raw_text)\n",
    "                else:\n",
    "                    print(f\"Could not extract text from PDF: {file_path}\")\n",
    "\n",
    "            # Write data to CSV\n",
    "            writer.writerow({\n",
    "                'link': paper_url,  \n",
    "                'heading': heading,\n",
    "                'arxiv_id': arxiv_id,\n",
    "                'file_path': file_path,\n",
    "                'page_count': num_pages,\n",
    "                'pdf_text': cleaned_pdf_text,\n",
    "                'summary': summary\n",
    "            })\n",
    "            print(f\"Entry completed for {heading}\")\n",
    "            print(\"---------------------------------------------------------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "4f8e0bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing Words or Vision: Do Vision-Language Models Have Blind Faith in Text?...\n",
      "No good match found for: Words or Vision: Do Vision-Language Models Have Blind Faith in Text? (best score: 0.00)\n",
      "Entry completed for Words or Vision: Do Vision-Language Models Have Blind Faith in Text?\n",
      "---------------------------------------------------------------------------------------\n",
      "Processing MedSAM2: Segment Anything in 3D Medical Images and Videos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fb/g1_y4h_17xj3txvtss0ghkth0000gn/T/ipykernel_46069/3411432412.py:24: DeprecationWarning: Testing an element's truth value will always return True in future versions.  Use specific 'len(elem)' or 'elem is not None' test instead.\n",
      "  if best_match and best_score >= similarity_threshold:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fuzzy match found (score=1.00): 2504.03600v1\n",
      "Downloading arxiv_pdfs_new/MedSAM2: Segment Anything in 3D Medical Images and Videos.pdf\n",
      "Found 13 pages\n",
      "13 pages\n",
      "Extracting and cleaning PDF: arxiv_pdfs_new/MedSAM2: Segment Anything in 3D Medical Images and Videos.pdf\n",
      "Initial text length: 65087 characters\n",
      "Removing contributors to reduce text length (65087 characters)\n",
      "Removing references to reduce text length (38077 characters)\n",
      "Removing appendix to reduce text length (22681 characters)\n",
      "Removing acknowledgments to reduce text length (22681 characters)\n",
      "Removing citations to reduce text length (22681 characters)\n",
      "Removing emails to reduce text length (22556 characters)\n",
      "Removing page_numbers to reduce text length (22556 characters)\n",
      "Still over 30k after removing sections, truncating (22533 characters)\n",
      "Final text length: 15993 characters\n",
      "Entry completed for MedSAM2: Segment Anything in 3D Medical Images and Videos\n",
      "---------------------------------------------------------------------------------------\n",
      "Processing StyleCrafter: Enhancing Stylized Text-to-Video Generation with Style Adapter...\n",
      "No good match found for: StyleCrafter: Enhancing Stylized Text-to-Video Generation with Style Adapter (best score: 0.00)\n",
      "Entry completed for StyleCrafter: Enhancing Stylized Text-to-Video Generation with Style Adapter\n",
      "---------------------------------------------------------------------------------------\n",
      "Processing Benchmarking Vision, Language, & Action Models on Robotic Learning Tasks...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fb/g1_y4h_17xj3txvtss0ghkth0000gn/T/ipykernel_46069/3411432412.py:24: DeprecationWarning: Testing an element's truth value will always return True in future versions.  Use specific 'len(elem)' or 'elem is not None' test instead.\n",
      "  if best_match and best_score >= similarity_threshold:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fuzzy match found (score=1.00): 2411.05821v2\n",
      "Downloading arxiv_pdfs_new/Benchmarking Vision, Language, & Action Models on Robotic Learning Tasks.pdf\n",
      "Found 19 pages\n",
      "19 pages\n",
      "Extracting and cleaning PDF: arxiv_pdfs_new/Benchmarking Vision, Language, & Action Models on Robotic Learning Tasks.pdf\n",
      "Initial text length: 69218 characters\n",
      "Removing contributors to reduce text length (69218 characters)\n",
      "Cleaned text too short (1912 chars), reverting to truncated original\n",
      "Final text length: 15998 characters\n",
      "Entry completed for Benchmarking Vision, Language, & Action Models on Robotic Learning Tasks\n",
      "---------------------------------------------------------------------------------------\n",
      "Processing The Model Openness Framework: Promoting Completeness and Openness for Reproducibility, Transparency, and Usability in Artificial Intelligence...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fb/g1_y4h_17xj3txvtss0ghkth0000gn/T/ipykernel_46069/3411432412.py:24: DeprecationWarning: Testing an element's truth value will always return True in future versions.  Use specific 'len(elem)' or 'elem is not None' test instead.\n",
      "  if best_match and best_score >= similarity_threshold:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fuzzy match found (score=0.99): 2403.13784v6\n",
      "Downloading arxiv_pdfs_new/The Model Openness Framework: Promoting Completeness and Openness for Reproducibility, Transparency, and Usability in Artificial Intelligence.pdf\n",
      "Found 28 pages\n",
      "28 pages\n",
      "Extracting and cleaning PDF: arxiv_pdfs_new/The Model Openness Framework: Promoting Completeness and Openness for Reproducibility, Transparency, and Usability in Artificial Intelligence.pdf\n",
      "Initial text length: 99465 characters\n",
      "Removing contributors to reduce text length (99465 characters)\n",
      "Removing references to reduce text length (32120 characters)\n",
      "Cleaned text too short (7886 chars), reverting to truncated original\n",
      "Final text length: 15998 characters\n",
      "Entry completed for The Model Openness Framework: Promoting Completeness and Openness for Reproducibility, Transparency, and Usability in Artificial Intelligence\n",
      "---------------------------------------------------------------------------------------\n",
      "Processing RLEF: Grounding Code LLMs in Execution Feedback with Reinforcement Learning...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fb/g1_y4h_17xj3txvtss0ghkth0000gn/T/ipykernel_46069/3411432412.py:24: DeprecationWarning: Testing an element's truth value will always return True in future versions.  Use specific 'len(elem)' or 'elem is not None' test instead.\n",
      "  if best_match and best_score >= similarity_threshold:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fuzzy match found (score=0.99): 2410.02089v2\n",
      "Downloading arxiv_pdfs_new/RLEF: Grounding Code LLMs in Execution Feedback with Reinforcement Learning.pdf\n",
      "Found 23 pages\n",
      "23 pages\n",
      "Extracting and cleaning PDF: arxiv_pdfs_new/RLEF: Grounding Code LLMs in Execution Feedback with Reinforcement Learning.pdf\n",
      "Initial text length: 73241 characters\n",
      "Removing contributors to reduce text length (73241 characters)\n",
      "Cleaned text too short (48 chars), reverting to truncated original\n",
      "Final text length: 15976 characters\n",
      "Entry completed for RLEF: Grounding Code LLMs in Execution Feedback with Reinforcement Learning\n",
      "---------------------------------------------------------------------------------------\n",
      "Processing Learning to Move Like Professional Counter-Strike Players...\n",
      "No good match found for: Learning to Move Like Professional Counter-Strike Players (best score: 0.00)\n",
      "Entry completed for Learning to Move Like Professional Counter-Strike Players\n",
      "---------------------------------------------------------------------------------------\n",
      "Processing MoCha: Towards Movie-Grade Talking Character Synthesis...\n",
      "No good match found for: MoCha: Towards Movie-Grade Talking Character Synthesis (best score: 0.00)\n",
      "Entry completed for MoCha: Towards Movie-Grade Talking Character Synthesis\n",
      "---------------------------------------------------------------------------------------\n",
      "Processing Point Policy: Unifying Observations and Actions with Key Points for Robot Manipulation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fb/g1_y4h_17xj3txvtss0ghkth0000gn/T/ipykernel_46069/3411432412.py:24: DeprecationWarning: Testing an element's truth value will always return True in future versions.  Use specific 'len(elem)' or 'elem is not None' test instead.\n",
      "  if best_match and best_score >= similarity_threshold:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fuzzy match found (score=0.99): 2502.20391v1\n",
      "Downloading arxiv_pdfs_new/Point Policy: Unifying Observations and Actions with Key Points for Robot Manipulation.pdf\n",
      "Found 16 pages\n",
      "16 pages\n",
      "Extracting and cleaning PDF: arxiv_pdfs_new/Point Policy: Unifying Observations and Actions with Key Points for Robot Manipulation.pdf\n",
      "Initial text length: 64064 characters\n",
      "Removing contributors to reduce text length (64064 characters)\n",
      "Cleaned text too short (690 chars), reverting to truncated original\n",
      "Final text length: 15998 characters\n",
      "Entry completed for Point Policy: Unifying Observations and Actions with Key Points for Robot Manipulation\n",
      "---------------------------------------------------------------------------------------\n",
      "Processing WebAssembly enables low latency interoperable augmented and virtual reality software...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fb/g1_y4h_17xj3txvtss0ghkth0000gn/T/ipykernel_46069/3411432412.py:24: DeprecationWarning: Testing an element's truth value will always return True in future versions.  Use specific 'len(elem)' or 'elem is not None' test instead.\n",
      "  if best_match and best_score >= similarity_threshold:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fuzzy match found (score=0.99): 2110.07128v2\n",
      "Downloading arxiv_pdfs_new/WebAssembly enables low latency interoperable augmented and virtual reality software.pdf\n",
      "Found 11 pages\n",
      "11 pages\n",
      "Extracting and cleaning PDF: arxiv_pdfs_new/WebAssembly enables low latency interoperable augmented and virtual reality software.pdf\n",
      "Initial text length: 36301 characters\n",
      "Removing contributors to reduce text length (36301 characters)\n",
      "Final text length: 14709 characters\n",
      "Entry completed for WebAssembly enables low latency interoperable augmented and virtual reality software\n",
      "---------------------------------------------------------------------------------------\n",
      "Processing Exploring GPU-to-GPU Communication: Insights into Supercomputer Interconnects...\n",
      "No good match found for: Exploring GPU-to-GPU Communication: Insights into Supercomputer Interconnects (best score: 0.00)\n",
      "Entry completed for Exploring GPU-to-GPU Communication: Insights into Supercomputer Interconnects\n",
      "---------------------------------------------------------------------------------------\n",
      "Processing Generative Agent Simulations of 1,000 People...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fb/g1_y4h_17xj3txvtss0ghkth0000gn/T/ipykernel_46069/3411432412.py:24: DeprecationWarning: Testing an element's truth value will always return True in future versions.  Use specific 'len(elem)' or 'elem is not None' test instead.\n",
      "  if best_match and best_score >= similarity_threshold:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fuzzy match found (score=1.00): 2411.10109v1\n",
      "Downloading arxiv_pdfs_new/Generative Agent Simulations of 1,000 People.pdf\n",
      "Found 65 pages\n",
      "65 pages\n",
      "Entry completed for Generative Agent Simulations of 1,000 People\n",
      "---------------------------------------------------------------------------------------\n",
      "Processing MambaByte: Token-free Selective State Space Model...\n",
      "No good match found for: MambaByte: Token-free Selective State Space Model (best score: 0.00)\n",
      "Entry completed for MambaByte: Token-free Selective State Space Model\n",
      "---------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Run the script using asyncio\n",
    "await main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b2e5c008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.aimodels.fyi/papers/arxiv/difuzcam-replacing-camera-lens-mask-diffusion-model',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/deepseek-r1-incentivizing-reasoning-capability-llms-via',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/elements-differentiable-programming',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/chemputer-chemputation-universal-chemical-compound-synthesis-machine',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/xlstmtime-long-term-time-series-forecasting-xlstm',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/q-sparse-all-large-language-models-can',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/distilling-system-2-into-system-1',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/automated-design-agentic-systems',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/learning-to-learn-at-test-time-rnns',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/meta-rewarding-language-models-self-improving-alignment',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/to-code-or-not-to-code-exploring',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/llm-pruning-distillation-practice-minitron-approach',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/differential-transformer',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/were-rnns-all-we-needed',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/fft-strikes-back-efficient-alternative-to-self',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/surveilling-masses-wi-fi-based-positioning-systems',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/wildgaussians-3d-gaussian-splatting-wild',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/toolsandbox-stateful-conversational-interactive-evaluation-benchmark-llm',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/imagen-3',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/deep-tempest-using-deep-learning-to-eavesdrop',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/phi-3-technical-report-highly-capable-language',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/timegpt-1',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/limo-less-is-more-reasoning',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/ladder-self-improving-llms-through-recursive-problem',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/alice-wonderland-simple-tasks-showing-complete-reasoning',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/mind-your-step-by-step-chain-thought',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/foundation-model-earth-system',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/large-language-monkeys-scaling-inference-compute-repeated',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/mixture-nested-experts-adaptive-processing-visual-tokens',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/tree-attention-topology-aware-decoding-long-context',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/jpeg-lm-llms-as-image-generators-canonical',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/beyond-better-planning-transformers-via-search-dynamics',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/mixture-million-experts',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/structuredrag-json-response-formatting-large-language-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/better-faster-large-language-models-via-multi',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/chameleon-mixed-modal-early-fusion-foundation-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/sigma-gpts-new-approach-to-autoregressive-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/paligemma-versatile-3b-vlm-transfer',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/more-agents-is-all-you-need',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/abundance-katherines-game-theory-baby-naming',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/training-large-language-models-to-reason-continuous',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/llms-will-always-hallucinate-we-need-to',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/cognitive-behaviors-that-enable-self-improving-reasoners',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/planting-undetectable-backdoors-machine-learning-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/accuracy-is-not-all-you-need',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/does-reasoning-emerge-examining-probabilities-causation-large',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/hardware-acceleration-llms-comprehensive-survey-comparison',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/transformer-layers-as-painters',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/bytes-are-all-you-need-transformers-operating',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/chain-thought-empowers-transformers-to-solve-inherently',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/gsm-symbolic-understanding-limitations-mathematical-reasoning-large',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/tpi-llm-serving-70b-scale-llms-efficiently',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/collaborative-text-editing-eg-walker-better-faster',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/bluesky-at-protocol-usable-decentralized-social-media',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/grokked-transformers-are-implicit-reasoners-mechanistic-journey',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/llms-cannot-find-reasoning-errors-but-can',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/from-pixels-to-planning-scale-free-active',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/thermodynamic-linear-algebra',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/you-need-to-pay-better-attention-rethinking',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/training-language-models-to-self-correct-via',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/towards-system-2-reasoning-llms-learning-how',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/toponets-high-performing-vision-language-models-brain',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/neural-network-diffusion',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/improving-retrieval-augmented-language-model-self-reasoning',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/tutorial-diffusion-models-imaging-vision',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/is-artificial-consciousness-achievable-lessons-from-human',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/refusal-language-models-is-mediated-by-single',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/reasoning-large-language-models-geometric-perspective',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/transformers-can-do-arithmetic-right-embeddings',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/chronos-learning-language-time-series',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/scaling-up-test-time-compute-latent-reasoning',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/scalable-matmul-free-language-modeling',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/self-retrieval-end-to-end-information-retrieval',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/thermodynamic-natural-gradient-descent',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/inference-time-scaling-generalist-reward-modeling',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/liveportrait-efficient-portrait-animation-stitching-retargeting-control',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/spreadsheetllm-encoding-spreadsheets-large-language-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/amortized-planning-large-scale-transformers-case-study',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/openhands-open-platform-ai-software-developers-as',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/xlstm-extended-long-short-term-memory',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/all-optical-general-purpose-cpu-optical-computer',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/deeprag-thinking-to-retrieval-step-by-step',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/drag-your-gan-interactive-point-based-manipulation',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/ai-problem-knowledge-collapse',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/matryoshka-adaptor-unsupervised-supervised-tuning-smaller-embedding',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/no-zero-shot-without-exponential-data-pretraining',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/lora-efficient-low-rank-adaptation-large-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/magicoder-empowering-code-generation-oss-instruct',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/fishing-magikarp-automatically-detecting-under-trained-tokens',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/can-transformer-represent-kalman-filter',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/mm1-methods-analysis-insights-from-multimodal-llm',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/llava-cot-let-vision-language-models-reason',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/lora-learns-less-forgets-less',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/llms-can-teach-themselves-to-better-predict',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/tokenformer-rethinking-transformer-scaling-tokenized-model-parameters',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/expect-unexpected-failsafe-long-context-qa-finance',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/efficient-reasoning-hidden-thinking',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/poisoning-web-scale-training-datasets-is-practical',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/training-language-models-to-generate-text-citations',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/openelm-efficient-language-model-family-open-training',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/delving-into-chatgpt-usage-academic-writing-through',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/spotting-llms-binoculars-zero-shot-detection-machine',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/evaluating-world-model-implicit-generative-model',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/block-diffusion-interpolating-between-autoregressive-diffusion-language',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/ultimate-guide-to-fine-tuning-llms-from',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/dynamic-memory-compression-retrofitting-llms-accelerated-inference',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/multi-token-attention',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/think-before-you-speak-training-language-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/lossless-compression-vector-ids-approximate-nearest-neighbor',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/generative-multimodal-models-are-context-learners',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/artprompt-ascii-art-based-jailbreak-attacks-against',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/tinyllama-open-source-small-language-model',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/simplifying-transformer-blocks',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/benchmarking-vision-language-models-optical-character-recognition',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/computational-life-how-well-formed-self-replicating',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/sft-memorizes-rl-generalizes-comparative-study-foundation',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/large-language-models-are-zero-shot-time',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/realm-reference-resolution-as-language-modeling',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/robust-autonomy-emerges-from-self-play',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/beyond-black-box-statistical-model-llm-reasoning',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/lossless-acceleration-large-language-model-via-adaptive',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/llms-know-more-than-they-show-intrinsic',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/removing-reflections-from-raw-photos',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/auto-differentiating-any-llm-workflow-farewell-to',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/mamba-linear-time-sequence-modeling-selective-state',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/beyond-memorization-violating-privacy-via-inference-large',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/layer-condensed-kv-cache-efficient-inference-large',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/from-llm-to-nmt-advancing-low-resource',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/qwen2-technical-report',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/chinchilla-scaling-replication-attempt',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/how-far-are-we-from-intelligent-visual',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/geometry-categorical-hierarchical-concepts-large-language-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/can-large-language-models-understand-symbolic-graphics',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/large-models-what-mistaking-engineering-achievements-human',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/from-words-to-numbers-your-large-language',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/siglip-2-multilingual-vision-language-encoders-improved',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/optpde-discovering-novel-integrable-systems-via-ai',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/player-driven-emergence-llm-driven-game-narrative',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/large-language-models-think-too-fast-to',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/tokenisation-is-np-complete',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/grokfast-accelerated-grokking-by-amplifying-slow-gradients',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/smoldocling-ultra-compact-vision-language-model-end',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/foundations-large-language-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/guardreasoner-towards-reasoning-based-llm-safeguards',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/enhancing-large-language-model-self-controlled-memory',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/turbotls-tls-connection-establishment-1-less-round',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/offline-reinforcement-learning-llm-multi-step-reasoning',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/swe-lancer-can-frontier-llms-earn-dollar1',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/lm2-large-memory-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/beyond-quacking-deep-integration-language-models-rag',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/language-models-are-super-mario-absorbing-abilities',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/teams-llm-agents-can-exploit-zero-day',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/skywork-r1v-pioneering-multimodal-reasoning-chain-thought',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/efficient-llm-inference-solution-intel-gpu',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/qwen25-technical-report',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/phd-knowledge-not-required-reasoning-challenge-large',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/from-artificial-needles-to-real-haystacks-improving',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/accessing-gpt-4-level-mathematical-olympiad-solutions',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/gpudrive-data-driven-multi-agent-driving-simulation',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/aria-open-multimodal-native-mixture-experts-model',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/clustering-behavior-sliding-windows',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/beyond-gradient-averaging-parallel-optimization-improved-robustness',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/porting-hpc-applications-to-amd-instinctdollartexttmdollar-mi300a',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/shocking-amount-web-is-machine-translated-insights',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/chain-thought-reasoning-without-prompting',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/auto-regressive-next-token-predictors-are-universal',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/large-language-models-pass-turing-test',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/standalone-16-bit-training-missing-study-hardware',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/what-if-we-recaption-billions-web-images',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/can-language-models-serve-as-text-based',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/citydreamer-compositional-generative-model-unbounded-3d-cities',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/no-more-adam-learning-rate-scaling-at',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/process-reinforcement-through-implicit-rewards',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/large-language-models-can-strategically-deceive-their',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/its-your-turn-novel-channel-contention-mechanism',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/grokking-at-edge-linear-separability',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/levels-agi-operationalizing-progress-path-to-agi',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/antidistillation-sampling',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/gradual-disempowerment-systemic-existential-risks-from-incremental',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/simulacra-as-conscious-exotica',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/ghost-20-generative-high-fidelity-one-shot',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/thousands-ai-authors-future-ai',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/sakuga-42m-dataset-scaling-up-cartoon-research',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/identifying-factors-contributing-to-bad-days-software',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/sequencematch-imitation-learning-autoregressive-sequence-modelling-backtracking',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/mathematical-perspective-transformers',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/momaland-set-benchmarks-multi-objective-multi-agent',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/brainformers-trading-simplicity-efficiency',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/godel-agent-self-referential-agent-framework-recursive',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/spark-tts-efficient-llm-based-text-to',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/language-agent-tree-search-unifies-reasoning-acting',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/analyze-feature-flow-to-enhance-interpretation-steering',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/mindeye2-shared-subject-models-enable-fmri-to',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/interpretable-graph-neural-networks-tabular-data',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/extending-llama-3s-context-ten-fold-overnight',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/building-large-japanese-web-corpus-large-language',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/eliza-reinterpreted-worlds-first-chatbot-was-not',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/jamba-hybrid-transformer-mamba-language-model',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/vision-mamba-efficient-visual-representation-learning-bidirectional',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/from-zero-to-hero-harnessing-transformers-biomedical',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/xval-continuous-numerical-tokenization-scientific-language-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/relational-graph-convolutional-networks-sentiment-analysis',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/gdpr-is-it-worth-it-perceptions-workers',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/matryoshka-diffusion-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/colorbench-can-vlms-see-understand-colorful-world',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/value-based-deep-rl-scales-predictably',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/maps-multi-agent-framework-based-big-seven',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/neural-symbolic-recursive-machine-systematic-generalization',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/bridging-empirical-theoretical-gap-neural-network-formal',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/best-n-jailbreaking',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/agentops-enabling-observability-llm-agents',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/defending-llms-against-jailbreaking-attacks-via-backtranslation',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/skip-hash-fast-ordered-map-via-software',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/position-aiml-influencers-have-place-academic-process',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/how-numerical-precision-affects-mathematical-reasoning-capabilities',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/parametric-matrix-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/meissonic-revitalizing-masked-generative-transformers-efficient-high',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/operationalizing-threat-model-red-teaming-large-language',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/stop-overthinking-survey-efficient-reasoning-large-language',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/gold-medalist-performance-solving-olympiad-geometry-alphageometry2',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/janus-pro-unified-multimodal-understanding-generation-data',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/automated-capability-discovery-via-model-self-exploration',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/trustworthiness-generative-foundation-models-guideline-assessment-perspective',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/r2-t2-re-routing-test-time-multimodal',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/many-shot-context-learning',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/long-context-compression-activation-beacon',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/npga-neural-parametric-gaussian-avatars',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/how-well-do-llms-generate-code-different',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/packing-input-frame-context-next-frame-prediction',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/illusion-state-state-space-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/does-gpt-4-pass-turing-test',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/can-llms-generate-novel-research-ideas-large',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/fine-tuning-quantization-llms-navigating-unintended-outcomes',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/chromadistill-colorizing-monochrome-radiance-fields-knowledge-distillation',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/atla-selene-mini-general-purpose-evaluation-model',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/neobert-next-generation-bert',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/smaller-weaker-yet-better-training-llm-reasoners',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/to-believe-or-not-to-believe-your',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/sapiens-foundation-human-vision-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/position-its-time-to-act-risk-efficient',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/moment-family-open-time-series-foundation-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/samurai-adapting-segment-anything-model-zero-shot',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/eliza-reanimated-worlds-first-chatbot-restored-worlds',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/prover-verifier-games-improve-legibility-llm-outputs',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/brain-inspired-efficient-pruning-exploiting-criticality-spiking',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/process-based-self-rewarding-language-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/rethinking-large-scale-dataset-compression-shifting-focus',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/evidence-interrelated-cognitive-like-capabilities-large-language',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/self-taught-optimizer-stop-recursively-self-improving',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/can-agents-spontaneously-form-society-introducing-novel',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/improving-text-embeddings-large-language-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/bayesian-regression-markets',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/replacing-judges-juries-evaluating-llm-generations-panel',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/retrieval-augmented-large-language-models-financial-time',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/creation-mmbench-assessing-context-aware-creative-intelligence',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/unicorn-text-only-data-synthesis-vision-language',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/recurrentgemma-moving-past-transformers-efficient-open-language',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/llmd-large-language-model-interpreting-longitudinal-medical',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/transformerfam-feedback-attention-is-working-memory',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/diloco-distributed-low-communication-training-language-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/conceptattention-diffusion-transformers-learn-highly-interpretable-features',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/unlocking-state-tracking-linear-rnns-through-negative',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/nerf-supervised-feature-point-detection-description',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/manipulating-large-language-models-to-increase-product',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/fantasytalking-realistic-talking-portrait-generation-via-coherent',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/cookingsense-culinary-knowledgebase-multidisciplinary-assertions',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/transparent-image-layer-diffusion-using-latent-transparency',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/powerinfer-2-fast-large-language-model-inference',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/empirical-study-mamba-based-language-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/impact-element-ordering-lm-agent-performance',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/diffmoe-dynamic-token-selection-scalable-diffusion-transformers',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/towards-physically-plausible-video-generation-via-vlm',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/lumina-image-20-unified-efficient-image-generative',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/lessons-from-trenches-reproducible-evaluation-language-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/s-lora-serving-thousands-concurrent-lora-adapters',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/watermark-large-language-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/monster-monash-scalable-time-series-evaluation-repository',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/diffrhythm-blazingly-fast-embarrassingly-simple-end-to',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/you-do-not-fully-utilize-transformers-representation',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/relay-mining-incentivizing-full-non-validating-nodes',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/transformers-are-multi-state-rnns',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/step-video-t2v-technical-report-practice-challenges',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/self-play-fine-tuning-converts-weak-language',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/remamba-equip-mamba-effective-long-sequence-modeling',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/opendiloco-open-source-framework-globally-distributed-low',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/multimodal-chain-thought-reasoning-language-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/saferag-benchmarking-security-retrieval-augmented-generation-large',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/lingoly-too-disentangling-memorisation-from-reasoning-linguistic',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/leave-no-context-behind-efficient-infinite-context',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/bot-or-human-detecting-chatgpt-imposters-single',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/transformers-utilization-chart-understanding-review-recent-advances',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/rstar-math-small-llms-can-master-math',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/dora-weight-decomposed-low-rank-adaptation',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/sdfs-from-unoriented-point-clouds-using-neural',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/fully-autonomous-ai-agents-should-not-be',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/recursive-introspection-teaching-language-model-agents-how',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/quantization-hurts-reasoning-empirical-study-quantized-reasoning',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/multimodal-representation-alignment-image-generation-text-image',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/your-transformer-is-secretly-linear',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/does-fine-tuning-llms-new-knowledge-encourage',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/biology-inspired-joint-distribution-neurons-based-hierarchical',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/landscape-thoughts-visualizing-reasoning-process-large-language',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/diffsplat-repurposing-image-diffusion-models-scalable-gaussian',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/using-large-language-models-hyperparameter-optimization',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/kitab-bench-comprehensive-multi-domain-benchmark-arabic',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/platonic-representation-hypothesis',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/q-improving-multi-step-reasoning-llms-deliberative',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/experimental-demonstration-magnetic-tunnel-junction-based-computational',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/m1-towards-scalable-test-time-compute-mamba',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/omnimmi-comprehensive-multi-modal-interaction-benchmark-streaming',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/gemstones-model-suite-multi-faceted-scaling-laws',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/omnipaint-mastering-object-oriented-editing-via-disentangled',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/vision-autonomic-computing-can-llms-make-it',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/ragcache-efficient-knowledge-caching-retrieval-augmented-generation',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/personhood-credentials-artificial-intelligence-value-privacy-preserving',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/internvl3-exploring-advanced-training-test-time-recipes',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/longwriter-v-enabling-ultra-long-high-fidelity',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/melting-point-mobile-evaluation-language-transformers',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/towards-trustworthy-gui-agents-survey',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/lasp-2-rethinking-sequence-parallelism-linear-attention',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/we-have-package-you-comprehensive-analysis-package',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/toto-time-series-optimized-transformer-observability',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/unconditional-priors-matter-improving-conditional-generation-fine',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/all-memory-stochastic-computing-using-reram',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/llasa-scaling-train-time-inference-time-compute',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/small-models-struggle-to-learn-from-strong',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/physbench-benchmarking-enhancing-vision-language-models-physical',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/llm2vec-large-language-models-are-secretly-powerful',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/can-base-chatgpt-be-used-forecasting-without',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/addition-is-all-you-need-energy-efficient',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/optimizing-decomposition-optimal-claim-verification',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/when-llm-is-apprehensive-about-its-answers',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/gendop-auto-regressive-camera-trajectory-generation-as',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/bizgen-advancing-article-level-visual-text-rendering',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/dmm-building-versatile-image-generation-model-via',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/concept-lancet-image-editing-compositional-representation-transplant',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/sayself-teaching-llms-to-express-confidence-self',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/ditto-tts-efficient-scalable-zero-shot-text',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/kan-kolmogorov-arnold-networks',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/satori-reinforcement-learning-chain-action-thought-enhances',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/free4d-tuning-free-4d-scene-generation-spatial',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/typhoon-t1-open-thai-reasoning-model',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/dextrack-towards-generalizable-neural-tracking-control-dexterous',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/wavecoder-widespread-versatile-enhancement-code-large-language',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/alphamath-almost-zero-process-supervision-without-process',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/long-form-factuality-large-language-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/mlrc-bench-can-language-agents-solve-machine',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/multimodal-mamba-decoder-only-multimodal-state-space',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/perceptually-accurate-3d-talking-head-generation-new',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/segagent-exploring-pixel-understanding-capabilities-mllms-by',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/transfer-between-modalities-metaqueries',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/did-we-miss-p-cap-partial-progress',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/nnn-next-generation-neural-networks-marketing-mix',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/semviqa-semantic-question-answering-system-vietnamese-information',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/beyond-rag-task-aware-kv-cache-compression',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/motionlab-unified-human-motion-generation-editing-via',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/mobius-text-to-seamless-looping-video-generation',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/can-we-trust-ai-benchmarks-interdisciplinary-review',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/deep-learning-multi-label-learning-comprehensive-survey',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/uncovering-mesa-optimization-algorithms-transformers',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/webrl-training-llm-web-agents-via-self',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/mme-unify-comprehensive-benchmark-unified-multimodal-understanding',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/datadecide-how-to-predict-best-pretraining-data',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/fmgs-foundation-model-embedded-3d-gaussian-splatting',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/michelangelo-long-context-evaluations-beyond-haystacks-via',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/model-supply-chain-poisoning-backdooring-pre-trained',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/procedural-knowledge-pretraining-drives-reasoning-large-language',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/goedel-prover-frontier-model-open-source-automated',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/token-assorted-mixing-latent-text-tokens-improved',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/syncity-training-free-generation-3d-worlds',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/hypencoder-hypernetworks-information-retrieval',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/llave-large-language-vision-embedding-models-hardness',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/efficient-personalization-quantized-diffusion-model-without-backpropagation',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/long-context-tuning-video-generation',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/implicit-reasoning-transformers-is-reasoning-through-shortcuts',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/distill-any-depth-distillation-creates-stronger-monocular',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/ai-consciousness-is-inevitable-theoretical-computer-science',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/poa-pre-training-once-models-all-sizes',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/adapting-automatic-speech-recognition-accented-air-traffic',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/late-chunking-contextual-chunk-embeddings-using-long',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/socioverse-world-model-social-simulation-powered-by',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/enhancing-abnormality-grounding-vision-language-models-knowledge',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/imagerag-dynamic-image-retrieval-reference-guided-image',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/rank1-test-time-compute-reranking-information-retrieval',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/ignore-kl-penalty-boosting-exploration-critical-tokens',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/agent-r-training-language-model-agents-to',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/we-cant-understand-ai-using-our-existing',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/llm-flash-efficient-large-language-model-inference',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/mathreader-text-to-speech-mathematical-documents',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/iterative-reasoning-preference-optimization',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/physics-informed-shadowgraph-network-end-to-end',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/lightweight-safety-classification-using-pruned-language-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/yolov12-attention-centric-real-time-object-detectors',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/fresca-unveiling-scaling-space-diffusion-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/wildgs-slam-monocular-gaussian-splatting-slam-dynamic',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/fine-tuning-small-language-models-domain-specific',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/cake-circuit-aware-editing-enables-generalizable-knowledge',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/lq-lora-low-rank-plus-quantized-matrix',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/mooncake-kvcache-centric-disaggregated-architecture-llm-serving',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/people-who-frequently-use-chatgpt-writing-tasks',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/blockwise-self-supervised-learning-at-scale',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/against-achilles-heel-survey-red-teaming-generative',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/native-sparse-attention-hardware-aligned-natively-trainable',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/motionstreamer-streaming-motion-generation-via-diffusion-based',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/codesync-synchronizing-large-language-models-dynamic-code',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/teacher-hacking-language-model-distillation',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/efficient-hybrid-language-model-compression-through-group',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/eclair-extracting-content-layout-integrated-reading-order',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/iformer-integrating-convnet-transformer-mobile-application',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/why-reasoning-matters-survey-advancements-multimodal-reasoning',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/sim-to-real-reinforcement-learning-vision-based',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/droidspeak-kv-cache-sharing-cross-llm-communication',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/reasoning-models-can-be-effective-without-thinking',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/presumed-cultural-identity-how-names-shape-llm',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/nasu-novel-actuating-screw-unit-origami-inspired',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/competitive-programming-large-reasoning-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/rethinking-mixture-agents-is-mixing-different-large',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/visuothink-empowering-lvlm-reasoning-multimodal-tree-search',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/concept-steerers-leveraging-k-sparse-autoencoders-controllable',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/lean-mean-decoupled-value-policy-optimization-global',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/bigobench-can-llms-generate-code-controlled-time',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/are-you-getting-what-you-pay-auditing',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/bolt-bootstrap-long-chain-thought-language-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/step-differences-instructional-video',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/cfg-manifold-constrained-classifier-free-guidance-diffusion',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/concurrent-data-structures-made-easy-extended-version',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/minicpm-unveiling-potential-small-language-models-scalable',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/can-lvlms-obtain-drivers-license-benchmark-towards',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/sorft-issue-resolving-subtask-oriented-reinforced-fine',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/skyladder-better-faster-pretraining-via-context-window',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/adaptive-layer-skipping-pre-trained-llms',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/hybrimoe-hybrid-cpu-gpu-scheduling-cache-management',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/spot-fake-large-multimodal-model-based-synthetic',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/remasking-discrete-diffusion-models-inference-time-scaling',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/retrieval-models-arent-tool-savvy-benchmarking-tool',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/physicsgen-can-generative-models-learn-from-images',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/lex-art-rethinking-text-generation-via-scalable',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/benchmax-comprehensive-multilingual-evaluation-suite-large-language',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/mcts-rag-enhancing-retrieval-augmented-generation-monte',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/language-models-learn-to-mislead-humans-via',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/arrows-time-large-language-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/order-doesnt-matter-but-reasoning-does-training',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/footstep-recognition-as-people-identification-systematic-literature',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/pump-dumps-bitcoin-era-real-time-detection',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/scaling-synthetic-data-creation-1000000000-personas',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/mooseagent-llm-based-multi-agent-framework-automating',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/agents-thinking-fast-slow-talker-reasoner-architecture',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/how-far-are-we-from-agi',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/opencharacter-training-customizable-role-playing-llms-large',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/teaching-language-models-to-critique-via-reinforcement',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/readerlm-v2-small-language-model-html-to',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/cha-maevit-unifying-channel-aware-masked-autoencoders',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/apigen-mt-agentic-pipeline-multi-turn-data',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/q-filters-leveraging-qk-geometry-efficient-kv',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/forgetting-transformer-softmax-attention-forget-gate',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/using-mechanistic-interpretability-to-craft-adversarial-attacks',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/ditailistener-controllable-high-fidelity-listener-video-generation',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/slamming-training-speech-language-model-one-gpu',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/psychosocial-impacts-generative-ai-harms',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/evolving-deeper-llm-thinking',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/llamav-o1-rethinking-step-by-step-visual',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/open-source-assessments-ai-capabilities-proliferation-ai',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/mixture-mamba-enhancing-multi-modal-state-space',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/benchmarking-ai-models-software-engineering-review-search',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/attention-iou-examining-biases-celeba-using-attention',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/where-do-large-vision-language-models-look',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/do-large-language-model-benchmarks-test-reliability',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/piece-it-together-part-based-concepting-ip',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/linguistic-generalizability-test-time-scaling-mathematical-reasoning',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/pvchat-personalized-video-chat-one-shot-learning',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/minorbench-hand-built-benchmark-content-based-risks',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/animate-anyone-2-high-fidelity-character-image',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/cuckoo-ie-free-rider-hatched-by-massive',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/qe4pe-word-level-quality-estimation-human-post',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/lucie-7b-llm-lucie-training-dataset-open',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/reinforcement-learning-reasoning-small-llms-what-works',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/reinforcement-learning-long-horizon-interactive-llm-agents',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/demystifying-long-chain-thought-reasoning-llms',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/turbo-sparse-achieving-llm-sota-performance-minimal',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/improving-consistency-large-language-models-through-chain',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/switchhead-accelerating-transformers-mixture-experts-attention',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/dawn-designing-distributed-agents-worldwide-network',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/special-characters-attack-toward-scalable-training-data',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/zerobench-impossible-visual-benchmark-contemporary-large-multimodal',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/llm-neo-parameter-efficient-knowledge-distillation-large',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/memory-layers-at-scale',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/microarchitectural-comparison-core-modeling-state-art-cpus',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/agent-hospital-simulacrum-hospital-evolvable-medical-agents',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/dynvfx-augmenting-real-videos-dynamic-content',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/multimodal-rewardbench-holistic-evaluation-reward-models-vision',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/image-as-imu-estimating-camera-motion-from',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/that-chip-has-sailed-critique-unfounded-skepticism',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/relation-specific-neurons-large-language-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/plutus-benchmarking-large-language-models-low-resource',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/gptree-towards-explainable-decision-making-via-llm',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/precise-legal-sentence-boundary-detection-retrieval-at',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/maskdollar2dollardit-dual-mask-based-diffusion-transformer-multi',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/egolife-towards-egocentric-life-assistant',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/can-community-notes-replace-professional-fact-checkers',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/word-form-matters-llms-semantic-reconstruction-under',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/feasible-learning',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/ads-edit-multimodal-knowledge-editing-dataset-autonomous',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/training-llms-over-neurally-compressed-text',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/cdsd-chinese-dysarthria-speech-database',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/vl-rethinker-incentivizing-self-reflection-vision-language',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/backpropagation-through-space-time-brain',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/bringing-auto-tuning-to-hip-analysis-tuning',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/finding-sweet-spot-preference-data-construction-scaling',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/quantization-openais-whisper-models-comparative-analysis',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/3d-scene-understanding-through-local-random-access',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/rethinking-reflection-pre-training',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/linear-representation-hypothesis-geometry-large-language-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/genxd-generating-any-3d-4d-scenes',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/legalbench-rag-benchmark-retrieval-augmented-generation-legal',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/population-parameter-averaging-papa',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/privacy-vs-profit-impact-googles-manifest-version',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/awq-activation-aware-weight-quantization-llm-compression',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/deliberation-latent-space-via-differentiable-cache-augmentation',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/magicid-hybrid-preference-optimization-id-consistent-dynamic',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/llm-based-user-profile-management-recommender-system',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/do-construction-distributions-shape-formal-language-learning',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/flame-federated-learning-benchmark-robotic-manipulation',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/vargpt-v11-improve-visual-autoregressive-large-unified',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/pretraining-language-models-diachronic-linguistic-change-discovery',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/alias-free-latent-diffusion-modelsimproving-fractional-shift',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/see-saw-modality-balance-see-gradient-sew',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/poseless-depth-free-vision-to-joint-control',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/diffclip-differential-attention-meets-clip',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/gliner-biomed-suite-efficient-models-open-biomedical',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/rsq-learning-from-important-tokens-leads-to',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/clipper-compression-enables-long-context-synthetic-data',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/plainqafact-automatic-factuality-evaluation-metric-biomedical-plain',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/scholawrite-dataset-end-to-end-scholarly-writing',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/enhancing-cognition-explainability-multimodal-foundation-models-self',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/histoires-morales-french-dataset-assessing-moral-alignment',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/exploring-word-sense-disambiguation-capabilities-large-language',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/omnimamba-efficient-unified-multimodal-understanding-generation-via',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/primus-pioneering-collection-open-source-datasets-cybersecurity',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/is-gpt-4-conscious',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/assemblage-automatic-binary-dataset-construction-machine-learning',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/spikingnerf-making-bio-inspired-neural-networks-see',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/exploring-hidden-reasoning-process-large-language-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/can-large-vision-language-models-read-maps',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/careful-examination-large-language-model-performance-grade',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/collapse-or-thrive-perils-promises-synthetic-data',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/towards-optimal-multi-draft-speculative-decoding',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/capacity-aware-inference-mitigating-straggler-effect-mixture',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/open-recipe-adapting-language-specific-llms-to',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/collapse-dense-retrievers-short-early-literal-biases',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/latent-radiance-fields-3d-aware-2d-representations',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/designing-conditional-prior-distribution-flow-based-generative',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/mme5-improving-multimodal-multilingual-embeddings-via-high',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/stable-virtual-camera-generative-view-synthesis-diffusion',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/m-prometheus-suite-open-multilingual-llm-judges',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/generator-long-context-generative-genomic-foundation-model',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/triposg-high-fidelity-3d-shape-synthesis-using',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/mediator-memory-efficient-llm-merging-less-parameter',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/sightation-counts-leveraging-sighted-user-feedback-building',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/vidorag-visual-document-retrieval-augmented-generation-via',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/speak-easy-eliciting-harmful-jailbreaks-from-llms',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/self-supervised-multimodal-deep-learning-approach-to',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/ditar-diffusion-transformer-autoregressive-modeling-speech-generation',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/postersum-multimodal-benchmark-scientific-poster-summarization',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/sample-scrutinize-scale-effective-inference-time-search',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/muon-is-scalable-llm-training',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/probabilistic-inference-approach-to-inference-time-scaling',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/self-taught-agentic-long-context-understanding',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/trading-inference-time-compute-adversarial-robustness',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/federated-sketching-lora-device-collaborative-fine-tuning',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/from-flatland-to-space-teaching-vision-language',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/thermodynamic-bayesian-inference',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/colpali-efficient-document-retrieval-vision-language-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/open-endedness-is-essential-artificial-superhuman-intelligence',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/harvard-undergraduate-survey-generative-ai',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/hunyuan-large-open-source-moe-model-52',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/slim-attention-cut-your-context-memory-half',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/interpreting-clip-sparse-linear-concept-embeddings-splice',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/how-to-avoid-machine-learning-pitfalls-guide',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/direct-preference-optimization-your-language-model-is',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/can-go-ais-be-adversarially-robust',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/superhuman-performance-large-language-model-reasoning-tasks',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/measuring-ai-ability-to-complete-long-tasks',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/causal-reasoning-large-language-models-opening-new',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/textgrad-automatic-differentiation-via-text',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/proof-or-bluff-evaluating-llms-2025-usa',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/large-language-models-are-unreliable-cyber-threat',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/fin-r1-large-language-model-financial-reasoning',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/qwen25-omni-technical-report',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/large-language-model-brained-gui-agents-survey',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/creativity-has-left-chat-price-debiasing-language',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/why-is-anything-conscious',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/graph-convolutional-branch-bound',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/contrastive-learning-mixture-experts-enables-precise-vector',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/galore-memory-efficient-llm-training-by-gradient',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/reinforcement-learning-overview',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/reversion-diffusion-based-relation-inversion-from-images',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/hype-sustainability-price-bigger-is-better-paradigm',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/thinking-tokens-language-modeling',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/super-weight-large-language-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/simultaneous-many-row-activation-off-shelf-dram',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/scaling-laws-vocabulary-larger-models-deserve-larger',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/neuromorphic-programming-emerging-directions-brain-inspired-hardware',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/questionable-practices-machine-learning',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/emo-emote-portrait-alive-generating-expressive-portrait',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/halo-hardware-aware-quantization-low-critical-path',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/lemma-learning-from-errors-mathematical-advancement-llms',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/instruct-clip-improving-instruction-guided-image-editing',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/xvlm2vec-adapting-lvlm-based-embedding-models-to',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/coralscapes-dataset-semantic-scene-understanding-coral-reefs',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/self-supervised-learning-motion-concepts-by-optimizing',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/visualwebinstruct-scaling-up-multimodal-instruction-data-through',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/multilingual-machine-translation-open-large-language-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/efficient-reinforcement-finetuning-via-adaptive-curriculum-learning',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/swiltra-bench-swiss-legal-translation-benchmark',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/unified-video-action-model',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/from-tower-to-spire-adding-speech-modality',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/direct-discriminative-optimization-your-likelihood-based-visual',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/can-llms-predict-citation-intent-experimental-analysis',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/3d-rex-causal-explanations-3d-neuroimaging-classification',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/diverse-controllable-diffusion-policy-signal-temporal-logic',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/improve-llm-as-judge-ability-as-general',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/graph-learning-across-data-silos',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/moc-mixtures-text-chunking-learners-retrieval-augmented',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/toward-dynamic-comfort-model-human-building-interaction',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/predicting-cognitive-decline-multimodal-ai-approach-to',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/selfcite-self-supervised-alignment-context-attribution-large',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/utilizing-3d-fast-spin-echo-anatomical-imaging',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/pave-patching-adapting-video-large-language-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/ukbob-one-billion-mri-labeled-masks-generalizable',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/renderbox-expressive-performance-rendering-text-control',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/compact-language-models-via-pruning-knowledge-distillation',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/achieving-human-level-competitive-robot-table-tennis',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/matryoshka-quantization',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/thoughts-are-all-over-place-underthinking-o1',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/why-are-web-ai-agents-more-vulnerable',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/video-r1-reinforcing-video-reasoning-mllms',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/when-your-ais-deceive-you-challenges-partial',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/breaking-recaptchav2',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/llm4ed-large-language-models-automatic-equation-discovery',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/chatbcg-can-ai-read-your-slide-deck',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/language-models-show-human-like-content-effects',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/interpretable-mesomorphic-networks-tabular-data',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/vision-lstm-xlstm-as-generic-vision-backbone',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/squeezellm-dense-sparse-quantization',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/easy-problems-that-llms-get-wrong',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/occult-evaluating-large-language-models-offensive-cyber',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/dollarepsilondollar-vae-denoising-as-visual-decoding',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/pattern-matching-ai-compilers-its-formalization-extended',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/gemini-15-unlocking-multimodal-understanding-across-millions',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/ffn-fusion-rethinking-sequential-computation-large-language',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/fractal-patterns-may-illuminate-success-next-token',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/large-language-models-as-markov-chains',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/frontier-models-are-capable-context-scheming',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/flashattention-napkin-diagrammatic-approach-to-deep-learning',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/samba-simple-hybrid-state-space-models-efficient',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/outlier-weighed-layerwise-sparsity-owl-missing-secret',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/multiple-dimensions-spuriousness-machine-learning',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/is-model-collapse-inevitable-breaking-curse-recursion',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/complexity-as-design-material',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/belebele-benchmark-parallel-reading-comprehension-dataset-122',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/s1-simple-test-time-scaling',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/could-large-language-model-be-conscious',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/towards-time-series-reasoning-llms',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/generative-ai-beyond-llms-system-implications-multi',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/beyond-scale-diversity-coefficient-as-data-quality',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/instructedit-instruction-based-knowledge-editing-large-language',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/evolution-knightian-blindspot-machine-learning',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/large-language-models-data-annotation-synthesis-survey',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/do-language-models-plan-ahead-future-tokens',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/memory-consciousness-large-language-model',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/demystifying-clip-data',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/syzygy-dual-code-test-c-to-safe',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/egonormia-benchmarking-physical-social-norm-understanding',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/densing-law-llms',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/hyperfusion-hypernetwork-approach-to-multimodal-integration-tabular',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/how-stroop-effect-arises-from-optimal-response',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/cosmos-transfer1-conditional-world-generation-adaptive-multimodal',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/fillerbuster-multi-view-scene-completion-casual-captures',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/focalcodec-low-bitrate-speech-coding-via-focal',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/spf-portrait-towards-pure-portrait-customization-semantic',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/worldsense-evaluating-real-world-omnimodal-understanding-multimodal',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/sshelf-single-shot-hierarchical-extrapolation-latent-features',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/nitibench-comprehensive-studies-llm-frameworks-capabilities-thai',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/neuron-platonic-intrinsic-representation-from-dynamics-using',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/ola-pushing-frontiers-omni-modal-language-model',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/comp-continual-multimodal-pre-training-vision-foundation',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/great-models-think-alike-this-undermines-ai',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/imageinwords-unlocking-hyper-detailed-image-descriptions',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/vocalcrypt-novel-active-defense-against-deepfake-voice',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/dc-vsr-spatially-temporally-consistent-video-super',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/unleashing-vecset-diffusion-model-fast-shape-generation',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/large-language-model-agent-survey-methodology-applications',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/supergpqa-scaling-llm-evaluation-across-285-graduate',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/ddt-decoupled-diffusion-transformer',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/automated-muscle-fat-segmentation-computed-tomography-comprehensive',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/scaling-analysis-interleaved-speech-text-language-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/enhancing-multilingual-llm-pretraining-model-based-data',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/learned-bayesian-cramer-rao-bound-unknown-measurement',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/score-mixture-training-training-one-step-generative',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/dermasynth-rich-synthetic-image-text-pairs-using',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/filter-like-you-test-data-driven-data',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/region-adaptive-sampling-diffusion-transformers',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/protogcd-unified-unbiased-prototype-learning-generalized-category',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/using-gradient-lagrangian-function-to-compute-efficient',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/ui-r1-enhancing-action-prediction-gui-agents',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/tinyllava-video-simple-framework-small-scale-large',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/eagle-2-building-post-training-data-strategies',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/dropletvideo-dataset-approach-to-explore-integral-spatio',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/instance-segmentation-scene-sketches-using-natural-image',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/towards-visual-text-grounding-multimodal-large-language',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/escaping-platos-cave-towards-alignment-3d-text',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/life-gom-generalizable-human-rendering-learned-iterative',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/livs-pluralistic-alignment-dataset-inclusive-public-spaces',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/words-or-vision-do-vision-language-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/medsam2-segment-anything-3d-medical-images-videos',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/stylecrafter-enhancing-stylized-text-to-video-generation',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/benchmarking-vision-language-action-models-robotic-learning',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/model-openness-framework-promoting-completeness-openness-reproducibility',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/rlef-grounding-code-llms-execution-feedback-reinforcement',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/learning-to-move-like-professional-counter-strike',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/mocha-towards-movie-grade-talking-character-synthesis',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/point-policy-unifying-observations-actions-key-points',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/webassembly-enables-low-latency-interoperable-augmented-virtual',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/exploring-gpu-to-gpu-communication-insights-into',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/generative-agent-simulations-1000-people',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/mambabyte-token-free-selective-state-space-model']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2d21fcd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(61,\n",
       "  'https://www.aimodels.fyi/papers/arxiv/toponets-high-performing-vision-language-models-brain'),\n",
       " (91,\n",
       "  'https://www.aimodels.fyi/papers/arxiv/llava-cot-let-vision-language-models-reason'),\n",
       " (113,\n",
       "  'https://www.aimodels.fyi/papers/arxiv/benchmarking-vision-language-models-optical-character-recognition'),\n",
       " (135,\n",
       "  'https://www.aimodels.fyi/papers/arxiv/siglip-2-multilingual-vision-language-encoders-improved'),\n",
       " (141,\n",
       "  'https://www.aimodels.fyi/papers/arxiv/smoldocling-ultra-compact-vision-language-model-end'),\n",
       " (255,\n",
       "  'https://www.aimodels.fyi/papers/arxiv/unicorn-text-only-data-synthesis-vision-language'),\n",
       " (326,\n",
       "  'https://www.aimodels.fyi/papers/arxiv/physbench-benchmarking-enhancing-vision-language-models-physical'),\n",
       " (381,\n",
       "  'https://www.aimodels.fyi/papers/arxiv/enhancing-abnormality-grounding-vision-language-models-knowledge'),\n",
       " (465,\n",
       "  'https://www.aimodels.fyi/papers/arxiv/where-do-large-vision-language-models-look'),\n",
       " (504,\n",
       "  'https://www.aimodels.fyi/papers/arxiv/vl-rethinker-incentivizing-self-reflection-vision-language'),\n",
       " (542,\n",
       "  'https://www.aimodels.fyi/papers/arxiv/can-large-vision-language-models-read-maps'),\n",
       " (569,\n",
       "  'https://www.aimodels.fyi/papers/arxiv/from-flatland-to-space-teaching-vision-language'),\n",
       " (571,\n",
       "  'https://www.aimodels.fyi/papers/arxiv/colpali-efficient-document-retrieval-vision-language-models'),\n",
       " (713,\n",
       "  'https://www.aimodels.fyi/papers/arxiv/words-or-vision-do-vision-language-models'),\n",
       " (716,\n",
       "  'https://www.aimodels.fyi/papers/arxiv/benchmarking-vision-language-action-models-robotic-learning')]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(i, url) for i, url in enumerate(paper_urls) if 'vision-language' in url.lower()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "6ccbd511",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://www.aimodels.fyi/papers/arxiv/position-aiml-influencers-have-place-academic-process',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/how-numerical-precision-affects-mathematical-reasoning-capabilities',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/parametric-matrix-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/meissonic-revitalizing-masked-generative-transformers-efficient-high',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/operationalizing-threat-model-red-teaming-large-language',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/stop-overthinking-survey-efficient-reasoning-large-language',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/gold-medalist-performance-solving-olympiad-geometry-alphageometry2',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/janus-pro-unified-multimodal-understanding-generation-data',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/automated-capability-discovery-via-model-self-exploration',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/trustworthiness-generative-foundation-models-guideline-assessment-perspective',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/r2-t2-re-routing-test-time-multimodal',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/many-shot-context-learning',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/long-context-compression-activation-beacon',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/npga-neural-parametric-gaussian-avatars',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/how-well-do-llms-generate-code-different',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/packing-input-frame-context-next-frame-prediction',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/illusion-state-state-space-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/does-gpt-4-pass-turing-test',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/can-llms-generate-novel-research-ideas-large',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/fine-tuning-quantization-llms-navigating-unintended-outcomes',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/chromadistill-colorizing-monochrome-radiance-fields-knowledge-distillation',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/atla-selene-mini-general-purpose-evaluation-model',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/neobert-next-generation-bert',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/smaller-weaker-yet-better-training-llm-reasoners',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/to-believe-or-not-to-believe-your',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/sapiens-foundation-human-vision-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/position-its-time-to-act-risk-efficient',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/moment-family-open-time-series-foundation-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/samurai-adapting-segment-anything-model-zero-shot',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/eliza-reanimated-worlds-first-chatbot-restored-worlds',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/prover-verifier-games-improve-legibility-llm-outputs',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/brain-inspired-efficient-pruning-exploiting-criticality-spiking',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/process-based-self-rewarding-language-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/rethinking-large-scale-dataset-compression-shifting-focus',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/evidence-interrelated-cognitive-like-capabilities-large-language',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/self-taught-optimizer-stop-recursively-self-improving',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/can-agents-spontaneously-form-society-introducing-novel',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/improving-text-embeddings-large-language-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/bayesian-regression-markets',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/replacing-judges-juries-evaluating-llm-generations-panel',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/retrieval-augmented-large-language-models-financial-time',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/creation-mmbench-assessing-context-aware-creative-intelligence',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/unicorn-text-only-data-synthesis-vision-language',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/recurrentgemma-moving-past-transformers-efficient-open-language',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/llmd-large-language-model-interpreting-longitudinal-medical',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/transformerfam-feedback-attention-is-working-memory',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/diloco-distributed-low-communication-training-language-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/conceptattention-diffusion-transformers-learn-highly-interpretable-features',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/unlocking-state-tracking-linear-rnns-through-negative',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/nerf-supervised-feature-point-detection-description',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/manipulating-large-language-models-to-increase-product',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/fantasytalking-realistic-talking-portrait-generation-via-coherent',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/cookingsense-culinary-knowledgebase-multidisciplinary-assertions',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/transparent-image-layer-diffusion-using-latent-transparency',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/powerinfer-2-fast-large-language-model-inference',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/empirical-study-mamba-based-language-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/impact-element-ordering-lm-agent-performance',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/diffmoe-dynamic-token-selection-scalable-diffusion-transformers',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/towards-physically-plausible-video-generation-via-vlm',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/lumina-image-20-unified-efficient-image-generative',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/lessons-from-trenches-reproducible-evaluation-language-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/s-lora-serving-thousands-concurrent-lora-adapters',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/watermark-large-language-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/monster-monash-scalable-time-series-evaluation-repository',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/diffrhythm-blazingly-fast-embarrassingly-simple-end-to',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/you-do-not-fully-utilize-transformers-representation',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/relay-mining-incentivizing-full-non-validating-nodes',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/transformers-are-multi-state-rnns',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/step-video-t2v-technical-report-practice-challenges',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/self-play-fine-tuning-converts-weak-language',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/remamba-equip-mamba-effective-long-sequence-modeling',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/opendiloco-open-source-framework-globally-distributed-low',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/multimodal-chain-thought-reasoning-language-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/saferag-benchmarking-security-retrieval-augmented-generation-large',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/lingoly-too-disentangling-memorisation-from-reasoning-linguistic',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/leave-no-context-behind-efficient-infinite-context',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/bot-or-human-detecting-chatgpt-imposters-single',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/transformers-utilization-chart-understanding-review-recent-advances',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/rstar-math-small-llms-can-master-math',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/dora-weight-decomposed-low-rank-adaptation',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/sdfs-from-unoriented-point-clouds-using-neural',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/fully-autonomous-ai-agents-should-not-be',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/recursive-introspection-teaching-language-model-agents-how',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/quantization-hurts-reasoning-empirical-study-quantized-reasoning',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/multimodal-representation-alignment-image-generation-text-image',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/your-transformer-is-secretly-linear',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/does-fine-tuning-llms-new-knowledge-encourage',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/biology-inspired-joint-distribution-neurons-based-hierarchical',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/landscape-thoughts-visualizing-reasoning-process-large-language',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/diffsplat-repurposing-image-diffusion-models-scalable-gaussian',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/using-large-language-models-hyperparameter-optimization',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/kitab-bench-comprehensive-multi-domain-benchmark-arabic',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/platonic-representation-hypothesis',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/q-improving-multi-step-reasoning-llms-deliberative',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/experimental-demonstration-magnetic-tunnel-junction-based-computational',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/m1-towards-scalable-test-time-compute-mamba',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/omnimmi-comprehensive-multi-modal-interaction-benchmark-streaming',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/gemstones-model-suite-multi-faceted-scaling-laws',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/omnipaint-mastering-object-oriented-editing-via-disentangled',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/vision-autonomic-computing-can-llms-make-it',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/ragcache-efficient-knowledge-caching-retrieval-augmented-generation',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/personhood-credentials-artificial-intelligence-value-privacy-preserving',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/internvl3-exploring-advanced-training-test-time-recipes',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/longwriter-v-enabling-ultra-long-high-fidelity',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/melting-point-mobile-evaluation-language-transformers',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/towards-trustworthy-gui-agents-survey',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/lasp-2-rethinking-sequence-parallelism-linear-attention',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/we-have-package-you-comprehensive-analysis-package',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/toto-time-series-optimized-transformer-observability',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/unconditional-priors-matter-improving-conditional-generation-fine',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/all-memory-stochastic-computing-using-reram',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/llasa-scaling-train-time-inference-time-compute',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/small-models-struggle-to-learn-from-strong',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/physbench-benchmarking-enhancing-vision-language-models-physical',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/llm2vec-large-language-models-are-secretly-powerful',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/can-base-chatgpt-be-used-forecasting-without',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/addition-is-all-you-need-energy-efficient',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/optimizing-decomposition-optimal-claim-verification',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/when-llm-is-apprehensive-about-its-answers',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/gendop-auto-regressive-camera-trajectory-generation-as',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/bizgen-advancing-article-level-visual-text-rendering',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/dmm-building-versatile-image-generation-model-via',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/concept-lancet-image-editing-compositional-representation-transplant',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/sayself-teaching-llms-to-express-confidence-self',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/ditto-tts-efficient-scalable-zero-shot-text',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/kan-kolmogorov-arnold-networks',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/satori-reinforcement-learning-chain-action-thought-enhances',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/free4d-tuning-free-4d-scene-generation-spatial',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/typhoon-t1-open-thai-reasoning-model',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/dextrack-towards-generalizable-neural-tracking-control-dexterous',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/wavecoder-widespread-versatile-enhancement-code-large-language',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/alphamath-almost-zero-process-supervision-without-process',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/long-form-factuality-large-language-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/mlrc-bench-can-language-agents-solve-machine',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/multimodal-mamba-decoder-only-multimodal-state-space',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/perceptually-accurate-3d-talking-head-generation-new',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/segagent-exploring-pixel-understanding-capabilities-mllms-by',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/transfer-between-modalities-metaqueries',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/did-we-miss-p-cap-partial-progress',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/nnn-next-generation-neural-networks-marketing-mix',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/semviqa-semantic-question-answering-system-vietnamese-information',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/beyond-rag-task-aware-kv-cache-compression',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/motionlab-unified-human-motion-generation-editing-via',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/mobius-text-to-seamless-looping-video-generation',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/can-we-trust-ai-benchmarks-interdisciplinary-review',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/deep-learning-multi-label-learning-comprehensive-survey',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/uncovering-mesa-optimization-algorithms-transformers',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/webrl-training-llm-web-agents-via-self',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/mme-unify-comprehensive-benchmark-unified-multimodal-understanding',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/datadecide-how-to-predict-best-pretraining-data',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/fmgs-foundation-model-embedded-3d-gaussian-splatting',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/michelangelo-long-context-evaluations-beyond-haystacks-via',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/model-supply-chain-poisoning-backdooring-pre-trained',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/procedural-knowledge-pretraining-drives-reasoning-large-language',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/goedel-prover-frontier-model-open-source-automated',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/token-assorted-mixing-latent-text-tokens-improved',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/syncity-training-free-generation-3d-worlds',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/hypencoder-hypernetworks-information-retrieval',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/llave-large-language-vision-embedding-models-hardness',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/efficient-personalization-quantized-diffusion-model-without-backpropagation',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/long-context-tuning-video-generation',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/implicit-reasoning-transformers-is-reasoning-through-shortcuts',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/distill-any-depth-distillation-creates-stronger-monocular',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/ai-consciousness-is-inevitable-theoretical-computer-science',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/poa-pre-training-once-models-all-sizes',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/adapting-automatic-speech-recognition-accented-air-traffic',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/late-chunking-contextual-chunk-embeddings-using-long',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/socioverse-world-model-social-simulation-powered-by',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/enhancing-abnormality-grounding-vision-language-models-knowledge',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/imagerag-dynamic-image-retrieval-reference-guided-image',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/rank1-test-time-compute-reranking-information-retrieval',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/ignore-kl-penalty-boosting-exploration-critical-tokens',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/agent-r-training-language-model-agents-to',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/we-cant-understand-ai-using-our-existing',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/llm-flash-efficient-large-language-model-inference',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/mathreader-text-to-speech-mathematical-documents',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/iterative-reasoning-preference-optimization',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/physics-informed-shadowgraph-network-end-to-end',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/lightweight-safety-classification-using-pruned-language-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/yolov12-attention-centric-real-time-object-detectors',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/fresca-unveiling-scaling-space-diffusion-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/wildgs-slam-monocular-gaussian-splatting-slam-dynamic',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/fine-tuning-small-language-models-domain-specific',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/cake-circuit-aware-editing-enables-generalizable-knowledge',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/lq-lora-low-rank-plus-quantized-matrix',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/mooncake-kvcache-centric-disaggregated-architecture-llm-serving',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/people-who-frequently-use-chatgpt-writing-tasks',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/blockwise-self-supervised-learning-at-scale',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/against-achilles-heel-survey-red-teaming-generative',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/native-sparse-attention-hardware-aligned-natively-trainable',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/motionstreamer-streaming-motion-generation-via-diffusion-based',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/codesync-synchronizing-large-language-models-dynamic-code',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/teacher-hacking-language-model-distillation',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/efficient-hybrid-language-model-compression-through-group',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/eclair-extracting-content-layout-integrated-reading-order',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/iformer-integrating-convnet-transformer-mobile-application',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/why-reasoning-matters-survey-advancements-multimodal-reasoning',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/sim-to-real-reinforcement-learning-vision-based',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/droidspeak-kv-cache-sharing-cross-llm-communication',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/reasoning-models-can-be-effective-without-thinking',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/presumed-cultural-identity-how-names-shape-llm',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/nasu-novel-actuating-screw-unit-origami-inspired',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/competitive-programming-large-reasoning-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/rethinking-mixture-agents-is-mixing-different-large',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/visuothink-empowering-lvlm-reasoning-multimodal-tree-search',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/concept-steerers-leveraging-k-sparse-autoencoders-controllable',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/lean-mean-decoupled-value-policy-optimization-global',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/bigobench-can-llms-generate-code-controlled-time',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/are-you-getting-what-you-pay-auditing',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/bolt-bootstrap-long-chain-thought-language-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/step-differences-instructional-video',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/cfg-manifold-constrained-classifier-free-guidance-diffusion',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/concurrent-data-structures-made-easy-extended-version',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/minicpm-unveiling-potential-small-language-models-scalable',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/can-lvlms-obtain-drivers-license-benchmark-towards',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/sorft-issue-resolving-subtask-oriented-reinforced-fine',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/skyladder-better-faster-pretraining-via-context-window',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/adaptive-layer-skipping-pre-trained-llms',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/hybrimoe-hybrid-cpu-gpu-scheduling-cache-management',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/spot-fake-large-multimodal-model-based-synthetic',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/remasking-discrete-diffusion-models-inference-time-scaling',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/retrieval-models-arent-tool-savvy-benchmarking-tool',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/physicsgen-can-generative-models-learn-from-images',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/lex-art-rethinking-text-generation-via-scalable',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/benchmax-comprehensive-multilingual-evaluation-suite-large-language',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/mcts-rag-enhancing-retrieval-augmented-generation-monte',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/language-models-learn-to-mislead-humans-via',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/arrows-time-large-language-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/order-doesnt-matter-but-reasoning-does-training',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/footstep-recognition-as-people-identification-systematic-literature',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/pump-dumps-bitcoin-era-real-time-detection',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/scaling-synthetic-data-creation-1000000000-personas',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/mooseagent-llm-based-multi-agent-framework-automating',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/agents-thinking-fast-slow-talker-reasoner-architecture',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/how-far-are-we-from-agi',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/opencharacter-training-customizable-role-playing-llms-large',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/teaching-language-models-to-critique-via-reinforcement',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/readerlm-v2-small-language-model-html-to',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/cha-maevit-unifying-channel-aware-masked-autoencoders',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/apigen-mt-agentic-pipeline-multi-turn-data',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/q-filters-leveraging-qk-geometry-efficient-kv',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/forgetting-transformer-softmax-attention-forget-gate',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/using-mechanistic-interpretability-to-craft-adversarial-attacks',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/ditailistener-controllable-high-fidelity-listener-video-generation',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/slamming-training-speech-language-model-one-gpu',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/psychosocial-impacts-generative-ai-harms',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/evolving-deeper-llm-thinking',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/llamav-o1-rethinking-step-by-step-visual',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/open-source-assessments-ai-capabilities-proliferation-ai',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/mixture-mamba-enhancing-multi-modal-state-space',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/benchmarking-ai-models-software-engineering-review-search',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/attention-iou-examining-biases-celeba-using-attention',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/where-do-large-vision-language-models-look',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/do-large-language-model-benchmarks-test-reliability',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/piece-it-together-part-based-concepting-ip',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/linguistic-generalizability-test-time-scaling-mathematical-reasoning',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/pvchat-personalized-video-chat-one-shot-learning',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/minorbench-hand-built-benchmark-content-based-risks',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/animate-anyone-2-high-fidelity-character-image',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/cuckoo-ie-free-rider-hatched-by-massive',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/qe4pe-word-level-quality-estimation-human-post',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/lucie-7b-llm-lucie-training-dataset-open',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/reinforcement-learning-reasoning-small-llms-what-works',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/reinforcement-learning-long-horizon-interactive-llm-agents',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/demystifying-long-chain-thought-reasoning-llms',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/turbo-sparse-achieving-llm-sota-performance-minimal',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/improving-consistency-large-language-models-through-chain',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/switchhead-accelerating-transformers-mixture-experts-attention',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/dawn-designing-distributed-agents-worldwide-network',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/special-characters-attack-toward-scalable-training-data',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/zerobench-impossible-visual-benchmark-contemporary-large-multimodal',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/llm-neo-parameter-efficient-knowledge-distillation-large',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/memory-layers-at-scale',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/microarchitectural-comparison-core-modeling-state-art-cpus',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/agent-hospital-simulacrum-hospital-evolvable-medical-agents',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/dynvfx-augmenting-real-videos-dynamic-content',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/multimodal-rewardbench-holistic-evaluation-reward-models-vision',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/image-as-imu-estimating-camera-motion-from',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/that-chip-has-sailed-critique-unfounded-skepticism',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/relation-specific-neurons-large-language-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/plutus-benchmarking-large-language-models-low-resource',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/gptree-towards-explainable-decision-making-via-llm',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/precise-legal-sentence-boundary-detection-retrieval-at',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/maskdollar2dollardit-dual-mask-based-diffusion-transformer-multi',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/egolife-towards-egocentric-life-assistant',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/can-community-notes-replace-professional-fact-checkers',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/word-form-matters-llms-semantic-reconstruction-under',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/feasible-learning',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/ads-edit-multimodal-knowledge-editing-dataset-autonomous',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/training-llms-over-neurally-compressed-text',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/cdsd-chinese-dysarthria-speech-database',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/vl-rethinker-incentivizing-self-reflection-vision-language',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/backpropagation-through-space-time-brain',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/bringing-auto-tuning-to-hip-analysis-tuning',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/finding-sweet-spot-preference-data-construction-scaling',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/quantization-openais-whisper-models-comparative-analysis',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/3d-scene-understanding-through-local-random-access',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/rethinking-reflection-pre-training',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/linear-representation-hypothesis-geometry-large-language-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/genxd-generating-any-3d-4d-scenes',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/legalbench-rag-benchmark-retrieval-augmented-generation-legal',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/population-parameter-averaging-papa',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/privacy-vs-profit-impact-googles-manifest-version',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/awq-activation-aware-weight-quantization-llm-compression',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/deliberation-latent-space-via-differentiable-cache-augmentation',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/magicid-hybrid-preference-optimization-id-consistent-dynamic',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/llm-based-user-profile-management-recommender-system',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/do-construction-distributions-shape-formal-language-learning',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/flame-federated-learning-benchmark-robotic-manipulation',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/vargpt-v11-improve-visual-autoregressive-large-unified',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/pretraining-language-models-diachronic-linguistic-change-discovery',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/alias-free-latent-diffusion-modelsimproving-fractional-shift',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/see-saw-modality-balance-see-gradient-sew',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/poseless-depth-free-vision-to-joint-control',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/diffclip-differential-attention-meets-clip',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/gliner-biomed-suite-efficient-models-open-biomedical',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/rsq-learning-from-important-tokens-leads-to',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/clipper-compression-enables-long-context-synthetic-data',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/plainqafact-automatic-factuality-evaluation-metric-biomedical-plain',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/scholawrite-dataset-end-to-end-scholarly-writing',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/enhancing-cognition-explainability-multimodal-foundation-models-self',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/histoires-morales-french-dataset-assessing-moral-alignment',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/exploring-word-sense-disambiguation-capabilities-large-language',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/omnimamba-efficient-unified-multimodal-understanding-generation-via',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/primus-pioneering-collection-open-source-datasets-cybersecurity',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/is-gpt-4-conscious',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/assemblage-automatic-binary-dataset-construction-machine-learning',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/spikingnerf-making-bio-inspired-neural-networks-see',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/exploring-hidden-reasoning-process-large-language-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/can-large-vision-language-models-read-maps',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/careful-examination-large-language-model-performance-grade',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/collapse-or-thrive-perils-promises-synthetic-data',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/towards-optimal-multi-draft-speculative-decoding',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/capacity-aware-inference-mitigating-straggler-effect-mixture',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/open-recipe-adapting-language-specific-llms-to',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/collapse-dense-retrievers-short-early-literal-biases',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/latent-radiance-fields-3d-aware-2d-representations',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/designing-conditional-prior-distribution-flow-based-generative',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/mme5-improving-multimodal-multilingual-embeddings-via-high',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/stable-virtual-camera-generative-view-synthesis-diffusion',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/m-prometheus-suite-open-multilingual-llm-judges',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/generator-long-context-generative-genomic-foundation-model',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/triposg-high-fidelity-3d-shape-synthesis-using',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/mediator-memory-efficient-llm-merging-less-parameter',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/sightation-counts-leveraging-sighted-user-feedback-building',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/vidorag-visual-document-retrieval-augmented-generation-via',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/speak-easy-eliciting-harmful-jailbreaks-from-llms',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/self-supervised-multimodal-deep-learning-approach-to',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/ditar-diffusion-transformer-autoregressive-modeling-speech-generation',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/postersum-multimodal-benchmark-scientific-poster-summarization',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/sample-scrutinize-scale-effective-inference-time-search',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/muon-is-scalable-llm-training',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/probabilistic-inference-approach-to-inference-time-scaling',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/self-taught-agentic-long-context-understanding',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/trading-inference-time-compute-adversarial-robustness',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/federated-sketching-lora-device-collaborative-fine-tuning',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/from-flatland-to-space-teaching-vision-language',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/thermodynamic-bayesian-inference',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/colpali-efficient-document-retrieval-vision-language-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/open-endedness-is-essential-artificial-superhuman-intelligence',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/harvard-undergraduate-survey-generative-ai',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/hunyuan-large-open-source-moe-model-52',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/slim-attention-cut-your-context-memory-half',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/interpreting-clip-sparse-linear-concept-embeddings-splice',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/how-to-avoid-machine-learning-pitfalls-guide',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/direct-preference-optimization-your-language-model-is',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/can-go-ais-be-adversarially-robust',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/superhuman-performance-large-language-model-reasoning-tasks',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/measuring-ai-ability-to-complete-long-tasks',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/causal-reasoning-large-language-models-opening-new',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/textgrad-automatic-differentiation-via-text',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/proof-or-bluff-evaluating-llms-2025-usa',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/large-language-models-are-unreliable-cyber-threat',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/fin-r1-large-language-model-financial-reasoning',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/qwen25-omni-technical-report',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/large-language-model-brained-gui-agents-survey',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/creativity-has-left-chat-price-debiasing-language',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/why-is-anything-conscious',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/graph-convolutional-branch-bound',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/contrastive-learning-mixture-experts-enables-precise-vector',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/galore-memory-efficient-llm-training-by-gradient',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/reinforcement-learning-overview',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/reversion-diffusion-based-relation-inversion-from-images',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/hype-sustainability-price-bigger-is-better-paradigm',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/thinking-tokens-language-modeling',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/super-weight-large-language-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/simultaneous-many-row-activation-off-shelf-dram',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/scaling-laws-vocabulary-larger-models-deserve-larger',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/neuromorphic-programming-emerging-directions-brain-inspired-hardware',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/questionable-practices-machine-learning',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/emo-emote-portrait-alive-generating-expressive-portrait',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/halo-hardware-aware-quantization-low-critical-path',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/lemma-learning-from-errors-mathematical-advancement-llms',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/instruct-clip-improving-instruction-guided-image-editing',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/xvlm2vec-adapting-lvlm-based-embedding-models-to',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/coralscapes-dataset-semantic-scene-understanding-coral-reefs',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/self-supervised-learning-motion-concepts-by-optimizing',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/visualwebinstruct-scaling-up-multimodal-instruction-data-through',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/multilingual-machine-translation-open-large-language-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/efficient-reinforcement-finetuning-via-adaptive-curriculum-learning',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/swiltra-bench-swiss-legal-translation-benchmark',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/unified-video-action-model',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/from-tower-to-spire-adding-speech-modality',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/direct-discriminative-optimization-your-likelihood-based-visual',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/can-llms-predict-citation-intent-experimental-analysis',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/3d-rex-causal-explanations-3d-neuroimaging-classification',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/diverse-controllable-diffusion-policy-signal-temporal-logic',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/improve-llm-as-judge-ability-as-general',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/graph-learning-across-data-silos',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/moc-mixtures-text-chunking-learners-retrieval-augmented',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/toward-dynamic-comfort-model-human-building-interaction',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/predicting-cognitive-decline-multimodal-ai-approach-to',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/selfcite-self-supervised-alignment-context-attribution-large',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/utilizing-3d-fast-spin-echo-anatomical-imaging',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/pave-patching-adapting-video-large-language-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/ukbob-one-billion-mri-labeled-masks-generalizable',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/renderbox-expressive-performance-rendering-text-control',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/compact-language-models-via-pruning-knowledge-distillation',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/achieving-human-level-competitive-robot-table-tennis',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/matryoshka-quantization',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/thoughts-are-all-over-place-underthinking-o1',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/why-are-web-ai-agents-more-vulnerable',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/video-r1-reinforcing-video-reasoning-mllms',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/when-your-ais-deceive-you-challenges-partial',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/breaking-recaptchav2',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/llm4ed-large-language-models-automatic-equation-discovery',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/chatbcg-can-ai-read-your-slide-deck',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/language-models-show-human-like-content-effects',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/interpretable-mesomorphic-networks-tabular-data',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/vision-lstm-xlstm-as-generic-vision-backbone',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/squeezellm-dense-sparse-quantization',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/easy-problems-that-llms-get-wrong',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/occult-evaluating-large-language-models-offensive-cyber',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/dollarepsilondollar-vae-denoising-as-visual-decoding',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/pattern-matching-ai-compilers-its-formalization-extended',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/gemini-15-unlocking-multimodal-understanding-across-millions',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/ffn-fusion-rethinking-sequential-computation-large-language',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/fractal-patterns-may-illuminate-success-next-token',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/large-language-models-as-markov-chains',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/frontier-models-are-capable-context-scheming',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/flashattention-napkin-diagrammatic-approach-to-deep-learning',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/samba-simple-hybrid-state-space-models-efficient',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/outlier-weighed-layerwise-sparsity-owl-missing-secret',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/multiple-dimensions-spuriousness-machine-learning',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/is-model-collapse-inevitable-breaking-curse-recursion',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/complexity-as-design-material',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/belebele-benchmark-parallel-reading-comprehension-dataset-122',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/s1-simple-test-time-scaling',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/could-large-language-model-be-conscious',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/towards-time-series-reasoning-llms',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/generative-ai-beyond-llms-system-implications-multi',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/beyond-scale-diversity-coefficient-as-data-quality',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/instructedit-instruction-based-knowledge-editing-large-language',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/evolution-knightian-blindspot-machine-learning',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/large-language-models-data-annotation-synthesis-survey',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/do-language-models-plan-ahead-future-tokens',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/memory-consciousness-large-language-model',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/demystifying-clip-data',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/syzygy-dual-code-test-c-to-safe',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/egonormia-benchmarking-physical-social-norm-understanding',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/densing-law-llms',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/hyperfusion-hypernetwork-approach-to-multimodal-integration-tabular',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/how-stroop-effect-arises-from-optimal-response',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/cosmos-transfer1-conditional-world-generation-adaptive-multimodal',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/fillerbuster-multi-view-scene-completion-casual-captures',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/focalcodec-low-bitrate-speech-coding-via-focal',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/spf-portrait-towards-pure-portrait-customization-semantic',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/worldsense-evaluating-real-world-omnimodal-understanding-multimodal',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/sshelf-single-shot-hierarchical-extrapolation-latent-features',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/nitibench-comprehensive-studies-llm-frameworks-capabilities-thai',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/neuron-platonic-intrinsic-representation-from-dynamics-using',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/ola-pushing-frontiers-omni-modal-language-model',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/comp-continual-multimodal-pre-training-vision-foundation',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/great-models-think-alike-this-undermines-ai',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/imageinwords-unlocking-hyper-detailed-image-descriptions',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/vocalcrypt-novel-active-defense-against-deepfake-voice',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/dc-vsr-spatially-temporally-consistent-video-super',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/unleashing-vecset-diffusion-model-fast-shape-generation',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/large-language-model-agent-survey-methodology-applications',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/supergpqa-scaling-llm-evaluation-across-285-graduate',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/ddt-decoupled-diffusion-transformer',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/automated-muscle-fat-segmentation-computed-tomography-comprehensive',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/scaling-analysis-interleaved-speech-text-language-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/enhancing-multilingual-llm-pretraining-model-based-data',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/learned-bayesian-cramer-rao-bound-unknown-measurement',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/score-mixture-training-training-one-step-generative',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/dermasynth-rich-synthetic-image-text-pairs-using',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/filter-like-you-test-data-driven-data',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/region-adaptive-sampling-diffusion-transformers',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/protogcd-unified-unbiased-prototype-learning-generalized-category',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/using-gradient-lagrangian-function-to-compute-efficient',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/ui-r1-enhancing-action-prediction-gui-agents',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/tinyllava-video-simple-framework-small-scale-large',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/eagle-2-building-post-training-data-strategies',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/dropletvideo-dataset-approach-to-explore-integral-spatio',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/instance-segmentation-scene-sketches-using-natural-image',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/towards-visual-text-grounding-multimodal-large-language',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/escaping-platos-cave-towards-alignment-3d-text',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/life-gom-generalizable-human-rendering-learned-iterative',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/livs-pluralistic-alignment-dataset-inclusive-public-spaces',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/words-or-vision-do-vision-language-models',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/medsam2-segment-anything-3d-medical-images-videos',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/stylecrafter-enhancing-stylized-text-to-video-generation',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/benchmarking-vision-language-action-models-robotic-learning',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/model-openness-framework-promoting-completeness-openness-reproducibility',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/rlef-grounding-code-llms-execution-feedback-reinforcement',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/learning-to-move-like-professional-counter-strike',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/mocha-towards-movie-grade-talking-character-synthesis',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/point-policy-unifying-observations-actions-key-points',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/webassembly-enables-low-latency-interoperable-augmented-virtual',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/exploring-gpu-to-gpu-communication-insights-into',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/generative-agent-simulations-1000-people',\n",
       " 'https://www.aimodels.fyi/papers/arxiv/mambabyte-token-free-selective-state-space-model']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper_urls[213:]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
